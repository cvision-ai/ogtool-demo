{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = TavilyClient(os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.crawl(\n",
    "    url=\"https://interviewing.io/blog\",\n",
    "    instructions=\"Get all blog posts\", \n",
    "    exclude_paths=[\"/category/.*\",\"/page/.*\"],\n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.json', 'w') as f:\n",
    "    json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/blog_posts/blog_posts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains an image with relative path\n",
    "                    if '![' in paragraph and '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to relative image paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY GUIDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found interview process links:\n",
      "https://interviewing.io/guides/hiring-process/google\n",
      "https://interviewing.io/guides/hiring-process/meta-facebook\n",
      "https://interviewing.io/guides/hiring-process/amazon\n",
      "https://interviewing.io/guides/hiring-process/microsoft\n",
      "https://interviewing.io/guides/hiring-process/netflix\n",
      "https://interviewing.io/guides/hiring-process/apple\n",
      "https://interviewing.io/affirm-interview-questions\n",
      "https://interviewing.io/airbnb-interview-questions\n",
      "https://interviewing.io/anduril-interview-questions\n",
      "https://interviewing.io/anthropic-interview-questions\n",
      "https://interviewing.io/atlassian-interview-questions\n",
      "https://interviewing.io/block-interview-questions\n",
      "https://interviewing.io/bloomberg-interview-questions\n",
      "https://interviewing.io/capital-one-interview-questions\n",
      "https://interviewing.io/coinbase-interview-questions\n",
      "https://interviewing.io/databricks-interview-questions\n",
      "https://interviewing.io/datadog-interview-questions\n",
      "https://interviewing.io/doordash-interview-questions\n",
      "https://interviewing.io/figma-interview-questions\n",
      "https://interviewing.io/fireeye-interview-questions\n",
      "https://interviewing.io/grammarly-interview-questions\n",
      "https://interviewing.io/hubspot-interview-questions\n",
      "https://interviewing.io/instacart-interview-questions\n",
      "https://interviewing.io/interviewingio-interview-questions\n",
      "https://interviewing.io/jane-street-interview-questions\n",
      "https://interviewing.io/jpmorgan-interview-questions\n",
      "https://interviewing.io/linkedin-interview-questions\n",
      "https://interviewing.io/mathworks-interview-questions\n",
      "https://interviewing.io/morgan-stanley-interview-questions\n",
      "https://interviewing.io/nvidia-interview-questions\n",
      "https://interviewing.io/openai-interview-questions\n",
      "https://interviewing.io/palantir-interview-questions\n",
      "https://interviewing.io/pivotal-labs-interview-questions\n",
      "https://interviewing.io/rippling-interview-questions\n",
      "https://interviewing.io/robinhood-interview-questions\n",
      "https://interviewing.io/roblox-interview-questions\n",
      "https://interviewing.io/salesforce-interview-questions\n",
      "https://interviewing.io/samsung-interview-questions\n",
      "https://interviewing.io/shopify-interview-questions\n",
      "https://interviewing.io/slack-interview-questions\n",
      "https://interviewing.io/snap-interview-questions\n",
      "https://interviewing.io/snowflake-interview-questions\n",
      "https://interviewing.io/spacex-interview-questions\n",
      "https://interviewing.io/spotify-interview-questions\n",
      "https://interviewing.io/stripe-interview-questions\n",
      "https://interviewing.io/tiktok-interview-questions\n",
      "https://interviewing.io/uber-interview-questions\n",
      "https://interviewing.io/vmware-interview-questions\n",
      "https://interviewing.io/walmart-interview-questions\n",
      "https://interviewing.io/wurlinc-interview-questions\n"
     ]
    }
   ],
   "source": [
    "url = \"https://interviewing.io/topics#companies\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all links containing \"Interview process & questions\"\n",
    "interview_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    if \"Interview process & questions\" in link.text:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            # Prepend domain if href is relative path\n",
    "            if href.startswith('/'):\n",
    "                href = f\"https://interviewing.io{href}\"\n",
    "            # Remove hash and everything after\n",
    "            href = href.split('#')[0]\n",
    "            interview_links.append(href)\n",
    "\n",
    "print(\"Found interview process links:\")\n",
    "for link in interview_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []\n",
    "for i in range(0, len(interview_links), 20):\n",
    "    batch = interview_links[i:i+20]\n",
    "    response = client.extract(\n",
    "        urls=batch,\n",
    "        extract_depth=\"advanced\",\n",
    "        include_images=True\n",
    "    )\n",
    "    all_responses.extend(response.get('results', []))\n",
    "\n",
    "final_response = {'results': all_responses}\n",
    "\n",
    "with open('exports/company_guides/company_guides.json', 'w') as f:\n",
    "    json.dump(final_response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/company_guides/company_guides.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/company_guides/company_guides.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains any relative links or images\n",
    "                    if '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to all relative paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

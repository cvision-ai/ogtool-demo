{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = TavilyClient(os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.crawl(\n",
    "    url=\"https://interviewing.io/blog\",\n",
    "    instructions=\"Get all blog posts\", \n",
    "    exclude_paths=[\"/category/.*\",\"/page/.*\"],\n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.json', 'w') as f:\n",
    "    json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/blog_posts/blog_posts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains an image with relative path\n",
    "                    if '![' in paragraph and '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to relative image paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.crawl(\n",
    "    url=\"https://interviewing.io/topics#companies\",\n",
    "    instructions=\"Get all interview resources by company\", \n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "with open('exports/company_guides/company_guides.json', 'w') as f:\n",
    "    json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

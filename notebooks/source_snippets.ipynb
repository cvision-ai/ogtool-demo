{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import base64\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from mistralai import Mistral, DocumentURLChunk\n",
    "from mistralai.models import OCRResponse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = TavilyClient(os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.crawl(\n",
    "    url=\"https://interviewing.io/blog\",\n",
    "    instructions=\"Get all blog posts\", \n",
    "    exclude_paths=[\"/category/.*\",\"/page/.*\"],\n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.json', 'w') as f:\n",
    "    json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN BLOG POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/blog_posts/blog_posts.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/blog_posts/blog_posts.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains an image with relative path\n",
    "                    if '![' in paragraph and '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to relative image paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY GUIDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found interview process links:\n",
      "https://interviewing.io/guides/hiring-process/google\n",
      "https://interviewing.io/guides/hiring-process/meta-facebook\n",
      "https://interviewing.io/guides/hiring-process/amazon\n",
      "https://interviewing.io/guides/hiring-process/microsoft\n",
      "https://interviewing.io/guides/hiring-process/netflix\n",
      "https://interviewing.io/guides/hiring-process/apple\n",
      "https://interviewing.io/affirm-interview-questions\n",
      "https://interviewing.io/airbnb-interview-questions\n",
      "https://interviewing.io/anduril-interview-questions\n",
      "https://interviewing.io/anthropic-interview-questions\n",
      "https://interviewing.io/atlassian-interview-questions\n",
      "https://interviewing.io/block-interview-questions\n",
      "https://interviewing.io/bloomberg-interview-questions\n",
      "https://interviewing.io/capital-one-interview-questions\n",
      "https://interviewing.io/coinbase-interview-questions\n",
      "https://interviewing.io/databricks-interview-questions\n",
      "https://interviewing.io/datadog-interview-questions\n",
      "https://interviewing.io/doordash-interview-questions\n",
      "https://interviewing.io/figma-interview-questions\n",
      "https://interviewing.io/fireeye-interview-questions\n",
      "https://interviewing.io/grammarly-interview-questions\n",
      "https://interviewing.io/hubspot-interview-questions\n",
      "https://interviewing.io/instacart-interview-questions\n",
      "https://interviewing.io/interviewingio-interview-questions\n",
      "https://interviewing.io/jane-street-interview-questions\n",
      "https://interviewing.io/jpmorgan-interview-questions\n",
      "https://interviewing.io/linkedin-interview-questions\n",
      "https://interviewing.io/mathworks-interview-questions\n",
      "https://interviewing.io/morgan-stanley-interview-questions\n",
      "https://interviewing.io/nvidia-interview-questions\n",
      "https://interviewing.io/openai-interview-questions\n",
      "https://interviewing.io/palantir-interview-questions\n",
      "https://interviewing.io/pivotal-labs-interview-questions\n",
      "https://interviewing.io/rippling-interview-questions\n",
      "https://interviewing.io/robinhood-interview-questions\n",
      "https://interviewing.io/roblox-interview-questions\n",
      "https://interviewing.io/salesforce-interview-questions\n",
      "https://interviewing.io/samsung-interview-questions\n",
      "https://interviewing.io/shopify-interview-questions\n",
      "https://interviewing.io/slack-interview-questions\n",
      "https://interviewing.io/snap-interview-questions\n",
      "https://interviewing.io/snowflake-interview-questions\n",
      "https://interviewing.io/spacex-interview-questions\n",
      "https://interviewing.io/spotify-interview-questions\n",
      "https://interviewing.io/stripe-interview-questions\n",
      "https://interviewing.io/tiktok-interview-questions\n",
      "https://interviewing.io/uber-interview-questions\n",
      "https://interviewing.io/vmware-interview-questions\n",
      "https://interviewing.io/walmart-interview-questions\n",
      "https://interviewing.io/wurlinc-interview-questions\n"
     ]
    }
   ],
   "source": [
    "url = \"https://interviewing.io/topics#companies\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all links containing \"Interview process & questions\"\n",
    "interview_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    if \"Interview process & questions\" in link.text:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            # Prepend domain if href is relative path\n",
    "            if href.startswith('/'):\n",
    "                href = f\"https://interviewing.io{href}\"\n",
    "            # Remove hash and everything after\n",
    "            href = href.split('#')[0]\n",
    "            interview_links.append(href)\n",
    "\n",
    "print(\"Found interview process links:\")\n",
    "for link in interview_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []\n",
    "for i in range(0, len(interview_links), 20):\n",
    "    batch = interview_links[i:i+20]\n",
    "    response = client.extract(\n",
    "        urls=batch,\n",
    "        extract_depth=\"advanced\",\n",
    "        include_images=True\n",
    "    )\n",
    "    all_responses.extend(response.get('results', []))\n",
    "\n",
    "final_response = {'results': all_responses}\n",
    "\n",
    "with open('exports/company_guides/company_guides.json', 'w') as f:\n",
    "    json.dump(final_response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/company_guides/company_guides.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/company_guides/company_guides.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains any relative links or images\n",
    "                    if '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to all relative paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://interviewing.io/guides/amazon-leadership-principles\n",
      "https://interviewing.io/guides/system-design-interview\n",
      "https://interviewing.io/guides/hiring-process\n"
     ]
    }
   ],
   "source": [
    "url = \"https://interviewing.io/learn#interview-guides\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Target class from the question\n",
    "target_class = \"col-span-2 mb-14 mt-6 grid grid-cols-1 gap-2 sm:grid-cols-2 sm:gap-4 lg:grid-cols-2 lg:gap-6\"\n",
    "\n",
    "# Find the target div\n",
    "target_div = soup.find(\"div\", class_=target_class)\n",
    "\n",
    "# Extract all hrefs within that div\n",
    "hrefs = []\n",
    "if target_div:\n",
    "    for a_tag in target_div.find_all(\"a\", href=True):\n",
    "        href = a_tag[\"href\"]\n",
    "        if href.startswith('/'):\n",
    "            href = 'https://interviewing.io' + href\n",
    "        hrefs.append(href)\n",
    "\n",
    "# Display the results\n",
    "for link in hrefs:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []\n",
    "for i in range(0, len(hrefs), 20):\n",
    "    batch = hrefs[i:i+20]\n",
    "    response = client.extract(\n",
    "        urls=batch,\n",
    "        extract_depth=\"advanced\",\n",
    "        include_images=True\n",
    "    )\n",
    "    all_responses.extend(response.get('results', []))\n",
    "\n",
    "final_response = {'results': all_responses}\n",
    "\n",
    "with open('exports/interview_guides/interview_guides.json', 'w') as f:\n",
    "    json.dump(final_response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/interview_guides/interview_guides.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/interview_guides/interview_guides.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains any relative links or images\n",
    "                    if '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to all relative paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://interviewing.io/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.crawl(\n",
    "    url=\"https://nilmamano.com/blog/category/dsa\",\n",
    "    instructions=\"Get all blog posts\",\n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "with open('exports/nilmamano/nilmamano.json', 'w') as f:\n",
    "    json.dump(response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exports/nilmamano/nilmamano.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('exports/nilmamano/nilmamano.md', 'w') as f:\n",
    "    seen_paragraphs = set()\n",
    "    \n",
    "    # First pass - collect all cleaned paragraphs\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                seen_paragraphs.add(cleaned)\n",
    "    \n",
    "    # Second pass - only write paragraphs that appear once\n",
    "    paragraph_counts = {}\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            if cleaned:\n",
    "                paragraph_counts[cleaned] = paragraph_counts.get(cleaned, 0) + 1\n",
    "    \n",
    "    # Write content\n",
    "    for result in data.get('results', []):\n",
    "        content = result.get('raw_content')\n",
    "        url = result.get('url', '')\n",
    "            \n",
    "        # Split content into paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        paragraphs = [p.replace('```', '') for p in paragraphs]\n",
    "        \n",
    "        # Write unique paragraphs\n",
    "        first_h1 = True\n",
    "        first_paragraph = True\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove all whitespace before comparing\n",
    "            cleaned = ''.join(paragraph.split())\n",
    "            # Add check that cleaned exists in paragraph_counts before accessing\n",
    "            if cleaned and cleaned in paragraph_counts and paragraph_counts[cleaned] == 1:\n",
    "                # Check if this is an H1 heading (starts with single #)\n",
    "                if first_paragraph:\n",
    "                    first_paragraph = False\n",
    "                    continue\n",
    "                if paragraph.strip().startswith('# ') and first_h1:\n",
    "                    title = paragraph.strip().replace('# ', '')\n",
    "                    # Skip writing the title line since it will be included in the header\n",
    "                    f.write(f'# [{title}]({url})\\n\\n')\n",
    "                    first_h1 = False\n",
    "                else:\n",
    "                    # Check if paragraph contains any relative links or images\n",
    "                    if '](/' in paragraph:\n",
    "                        # Add interviewing.io domain to all relative paths\n",
    "                        paragraph = paragraph.replace('](/', '](https://nilmamano.com/')\n",
    "                    f.write(paragraph.strip() + '\\n\\n')\n",
    "                \n",
    "        f.write('\\n')  # Add spacing between articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "# Path configuration\n",
    "INPUT_DIR = Path(\"exports/chapters\")   # Folder where the user places the PDFs to be processed\n",
    "DONE_DIR = Path(\"exports/chapters\")            # Folder where processed PDFs will be moved\n",
    "OUTPUT_ROOT_DIR = Path(\"exports/chapters\")    # Root folder for conversion results\n",
    "\n",
    "# Ensure directories exist\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "DONE_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_ROOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    This converts base64 encoded images directly in the markdown...\n",
    "    And replaces them with links to external images, so the markdown is more readable and organized.\n",
    "    \"\"\"\n",
    "    for img_name, base64_str in images_dict.items():\n",
    "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
    "    return markdown_str\n",
    "\n",
    "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
    "    \"\"\"\n",
    "    Part of the response from the Mistral API, which is an OCRResponse object...\n",
    "    And returns a single string with the combined markdown of all the pages of the PDF.\n",
    "    \"\"\"\n",
    "    markdowns: list[str] = []\n",
    "    for page in ocr_response.pages:\n",
    "        image_data = {}\n",
    "        for img in page.images:\n",
    "            image_data[img.id] = img.image_base64\n",
    "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
    "\n",
    "    return \"\\n\\n\".join(markdowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path: Path):\n",
    "    # Process all PDFs in INPUT_DIR\n",
    "    # - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "    #   and it could cause errors by exceeding the limit.\n",
    "\n",
    "    # PDF base name\n",
    "    pdf_base = pdf_path.stem\n",
    "    print(f\"Processing {pdf_path.name} ...\")\n",
    "    \n",
    "    # Output folders\n",
    "    output_dir = OUTPUT_ROOT_DIR / pdf_base\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    images_dir = output_dir / \"images\"\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # PDF -> OCR\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_bytes = f.read()\n",
    "        \n",
    "    uploaded_file = client.files.upload(\n",
    "        file={\n",
    "            \"file_name\": pdf_path.name,\n",
    "            \"content\": pdf_bytes,\n",
    "        },\n",
    "        purpose=\"ocr\"\n",
    "    )\n",
    "    \n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
    "    \n",
    "    ocr_response = client.ocr.process(\n",
    "        document=DocumentURLChunk(document_url=signed_url.url),\n",
    "        model=\"mistral-ocr-latest\",\n",
    "        include_image_base64=True\n",
    "    )\n",
    "    \n",
    "    # Save OCR in JSON \n",
    "    # (in case something fails it could be reused, but it is not used in the rest of the code)\n",
    "    ocr_json_path = output_dir / \"ocr_response.json\"\n",
    "    with open(ocr_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(ocr_response.model_dump(), json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"OCR response saved in {ocr_json_path}\")\n",
    "    \n",
    "    # OCR -> Markdown prepared for Obsidian\n",
    "    # - That is, from base64 encoded images, it converts them to links to \n",
    "    #   external images and generates the images as such, in a subfolder.\n",
    "    \n",
    "    global_counter = 1\n",
    "    updated_markdown_pages = []\n",
    "    \n",
    "    for page in ocr_response.pages:\n",
    "        updated_markdown = page.markdown\n",
    "        for image_obj in page.images:\n",
    "            \n",
    "            # base64 to image\n",
    "            base64_str = image_obj.image_base64\n",
    "            if base64_str.startswith(\"data:\"):\n",
    "                base64_str = base64_str.split(\",\", 1)[1]\n",
    "            image_bytes = base64.b64decode(base64_str)\n",
    "            \n",
    "            # image extensions\n",
    "            ext = Path(image_obj.id).suffix if Path(image_obj.id).suffix else \".png\"\n",
    "            new_image_name = f\"{pdf_base}_img_{global_counter}{ext}\"\n",
    "            global_counter += 1\n",
    "            \n",
    "            # save in subfolder\n",
    "            image_output_path = images_dir / new_image_name\n",
    "            with open(image_output_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            \n",
    "            # Update markdown with wikilink: ![[nombre_imagen]]\n",
    "            updated_markdown = updated_markdown.replace(\n",
    "                f\"![{image_obj.id}]({image_obj.id})\",\n",
    "                f\"![[{new_image_name}]]\"\n",
    "            )\n",
    "        updated_markdown_pages.append(updated_markdown)\n",
    "    \n",
    "    final_markdown = \"\\n\\n\".join(updated_markdown_pages)\n",
    "    output_markdown_path = output_dir / \"output.md\"\n",
    "    with open(output_markdown_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(final_markdown)\n",
    "    print(f\"Markdown generated in {output_markdown_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Sneak Peek BCTCI - First 7 Chapters - What's Broken About Coding Interviews, What Recruiters Won't Tell You, How to Get In the Door, and more.pdf ...\n",
      "OCR response saved in exports/chapters/Sneak Peek BCTCI - First 7 Chapters - What's Broken About Coding Interviews, What Recruiters Won't Tell You, How to Get In the Door, and more/ocr_response.json\n",
      "Markdown generated in exports/chapters/Sneak Peek BCTCI - First 7 Chapters - What's Broken About Coding Interviews, What Recruiters Won't Tell You, How to Get In the Door, and more/output.md\n",
      "Sneak Peek BCTCI - First 7 Chapters - What's Broken About Coding Interviews, What Recruiters Won't Tell You, How to Get In the Door, and more.pdf moved to exports/chapters\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in INPUT_DIR\n",
    "# - Important to be careful with the number of PDFs, as the Mistral API has a usage limit\n",
    "#   and it could cause errors by exceeding the limit.\n",
    "\n",
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))      # Get all PDFs in pdfs_to_process. So make sure to place the PDFs there.\n",
    "if not pdf_files:\n",
    "    response = input(\"No PDFs to process. Pick them manually? y/n: \")\n",
    "    if response.lower() == \"y\":\n",
    "        pdf_files = [Path(input(\"Enter the path to the PDF: \"))]\n",
    "    else:\n",
    "        print(\"Exiting...\")\n",
    "        exit()\n",
    "    \n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        process_pdf(pdf_file)\n",
    "        shutil.move(str(pdf_file), DONE_DIR)\n",
    "        print(f\"{pdf_file.name} moved to {DONE_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

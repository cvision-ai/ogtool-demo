# [We ran the numbers, and there really is a pipeline problem in eng hiring.](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring)

By Aline Lerner | Published: December 2, 2019; Last updated: May 1, 2023

If you say the words “there’s a pipeline problem” to explain why we’ve failed to make meaningful progress toward gender parity in software engineering, you probably won’t make many friends (or many hires). The pipeline problem argument goes something like this: *“There aren’t enough qualified women out there, so it’s not our fault if we don’t hire them.”*

Many people don’t like this reductive line of thinking because it ignores the growing body of research that points to unwelcoming environments that drive underrepresented talent out of tech: STEM in early education being unfriendly to children from underrepresented backgrounds, lack of a level playing field and unequal access to quality STEM education (see [this study](https://www.kaporcenter.org/wp-content/uploads/2019/06/Computer-Science-in-California-Schools.pdf) on how few of California’s schools offer AP Computer Science for instance), hostile university culture in male-dominated CS programs, biased hiring practices, and ultimately non-inclusive work environments that force women and people of color to leave tech at disproportionately high rates.[1](#user-content-fn-1)

However, because systemic issues can be hard to fix (they can take years, concerted efforts across many big organizations, and even huge socioeconomic shifts), the argument against the pipeline problem tends to get reduced to *“No, the candidates are there. We just need to fix the bias in our process.”*

**This kind of reductive thinking is also not great. For years, companies have been [pumping money and resources into things like unconscious bias training](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity) (which has been [shown not to work](https://leakytechpipeline.com/wp-content/themes/kapor/pdf/KC18001_report_v6.pdf)), anonymizing resumes, and all sorts of other initiatives, and the numbers have barely moved.** It’s no wonder tech eventually succumbs to a [“diversity fatigue”](https://www.latimes.com/business/technology/la-fi-tn-diversity-fatigue-20180604-story.html) that comes from trying to make changes and not seeing results.

**We ran the numbers and learned that there really IS a pipeline problem in hiring — there really aren’t enough women to meet demand… if we keep hiring the way we’re hiring. Namely, if we keep over-indexing on CS degrees from top schools, and even if we remove unconscious bias from the process entirely, we will not get to gender parity.** And yes, there is a way to surface strong candidates without relying on proxies like a college degree. We’ll talk about that toward the end.

Our findings ARE NOT meant to diminish the systemic issues that make engineering feel unwelcome to underrepresented talent, nor to diminish our ability to work together as an industry to effect change — to enact policy changes like access to CS in public schools, for instance. Our findings ARE meant to empower those individuals already working very hard to make hiring better who find themselves frustrated because, despite their efforts, the numbers aren’t moving. To those people, we say, please don’t lose sight of the systemic problems, but in the short term, there are things you can do that will yield results. We hope that, over time, by addressing both systemic pipeline issues and biases, we will get to critical mass of women from all backgrounds in engineering positions, and that these women, in turn, will do a lot to change the pipeline problem by providing role models and advocates and by changing the culture within companies.

Lastly, an important disclaimer before we proceed. In this post, we chose to focus on gender (and not on race). This decision was mostly due to the dearth of publicly available data around race and intersectionality in CS programs/bootcamps/MOOCs.[2](#user-content-fn-2) While this analysis does not examine race and intersectionality, it is important to note that we recognize: 1) Not all women have the same experience in their engineering journey, and 2) tech’s disparities by gender is no more important than the lack of representation of people of color in engineering. We will revisit these subjects in a future post.

## The percentage of women engineers is low and likely worse than reported

It’s very hard to have a cogent public conversation about diversity when there is no standardization of what statistic means what. As this post is about engineering specifically, we needed to find a way to get at how many women engineers are actually in the market and work around two big limitations in how FAAMNG (Facebook, Amazon, Apple, Microsoft, Google, and Netflix) report their diversity numbers.

The first limitation is that FAAMNG’s numbers are *global*. Why does this matter? It turns out that other countries, especially those where big companies have their offshore development offices, tend to have a higher percentage of female developers.[3](#user-content-fn-3) In India, for instance, [about 35% of developers are women](https://www.computerweekly.com/news/252437742/Why-does-India-have-a-higher-percentage-of-women-in-tech-than-the-UK); in the U.S., it’s 16%. Why are these numbers reported globally? The cynic in me says that it’s likely because the U.S. numbers, on their own, are pretty dismal, and these companies know it.[4](#user-content-fn-4) To account for this limitation and get at the U.S. estimate, we did some napkin math and conservatively cut each company’s female headcount by 20%.

The second limitation is that reported numbers are based on “technical roles,” which Facebook at least defines very broadly: “A position that requires specialization and knowledge needed to accomplish mathematical, engineering, or scientific related duties.” I expect the other giants use something similar. What are the implications of this fairly broad definition? Given that product management and UX design count as technical roles, we did some more napkin math and removed ~20% to correct for PMs and designers.[5](#user-content-fn-5)

With these limitations in mind, below is a graph comparing the makeup of the U.S. population to its representation in tech at FAAMNG companies where said data was available, as well as an estimate of women in engineering specifically.

![Chart comparing the % of women working at FAANGs to the general US population](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F41a7b_theres_still_a_lot_of_work_to_do_if_we_want_to_reach_gender_parity_in_engineering_2_7a234c8e90.webp&w=1920&q=75 "% women at FAANGs versus the general population")

There's still a lot of work to do if we want to reach gender parity in engineering

If we want to reach gender parity in engineering, especially when we correct for women in the U.S. (and whether they’re actually software engineers), you can see that we have a long way to go.

### Is it a pipeline problem?

So, are there just not enough qualified women in the hiring pool? It turns out that we’re actually hiring women at pretty much the same rate that women are graduating with CS degrees from four-year universities — out of the 71,420 students who graduated with a CS degree in 2017, 13,654, or ~20%, were women.[6](#user-content-fn-6) So maybe we just need more women to get CS degrees?

Top tech companies and their non-profit arms have been using their diversity and inclusion budgets to [bolster education initiatives](https://www.google.org/billion-commitment-to-create-more-opportunity/), in the hopes that this will help them improve gender diversity in hiring. Diversity initiatives started taking off in earnest in 2014, and in 4 years, enrollment in CS programs grew by about 60%. It’s not anywhere near enough to get to gender parity.

And even if we could meaningfully increase the number of women enrolling in CS programs overall, top companies have historically tended to favor candidates from elite universities (based on some targeted LinkedIn Recruiter searches, 60% of software engineers at FAANG hold a degree from a top 20 school). You can see enrollment rates of women in 3 representative top computer science programs below. Note that while the numbers are mostly going up, growth is linear and not very fast.

![Chart showing increase in women enrolled in undergraduate computer science](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F26c4f_growth_in_womens_undergraduate_computer_science_enrollment_at_top_schools_1_8914d7d98a.webp&w=1920&q=75 "Growth in women's undergraduate computer science enrollment at top schools")

Sources: [UC-Berkeley](https://pages.github.berkeley.edu/OPA/our-berkeley/student-headcount-by-major.html), MIT [1](http://web.mit.edu/fnl/volume/305/bucciarelli.html) and [2](https://registrar.mit.edu/statistics-reports/enrollment-statistics-year)), and [Stanford](https://studentservices.stanford.edu/more-resources/office-university-registrar/everyone/enrollment-statistics) enrollment data

To see if it’s possible to reach gender parity if we remove unconscious bias but keep hiring primarily from top schools, let’s build a model. For the purposes of this model let’s focus solely on new jobs — if companies want to meet their diversity goals, at a minimum they need to achieve parity on any new jobs they’ve created. Based on the US BLS’s projections, the number of software engineering jobs is estimated to increase by 20% by 2028 (or about 1.8% annually). Today, the BLS estimates there are about 4 million computer-related jobs. This projects to about 70,000 new jobs created this year, increasing to 85,000 new jobs created in 2028.

If the goal is to hit gender parity in the workforce, our goal should be to have 50% of these new seats filled by women.

To see if this is possible, let’s project the growth of the incoming job pool over the same timeframe. Based on NCES’ 2017 survey, computer science graduates have grown annually anywhere between 7% and 11% this decade. Let’s optimistically assume this annual growth rate persists at 10%. Let’s also assume that the  
percentage of graduates who are women remains at 20%, which has been true for the last 15 years. But, there are some gotchas.

First, there’s no guarantee that the seats earmarked for women actually get filled by women, particularly in a world where male CS graduates will continue to outnumber females 4-to-1. Not all of these jobs will be entry-level, so some portion of these jobs will be pulling from an already female-constrained pool of senior candidates. Finally, there’s no guarantee that traditional 4-year colleges will be able to support the projected influx of computer science candidates, particularly from the top-tier universities that companies usually prefer. Below, we graph the net new seats we’d need to fill if women held half of software engineering jobs (blue line) vs. how many women are actually available to hire if we keep focusing largely on educational pedigree in our recruiting efforts (red line). **As you can see, it’s not possible to hit our goals, whether or not we’re biased against women at any point in the hiring process.**[7](#user-content-fn-7)

![Chart showing a gap in the labor pool](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fa35da_there_arent_enough_women_in_the_labor_pool_to_get_to_gender_parity_based_on_how_we_hire_today_d19d624d91.webp&w=1920&q=75 "There aren't enough women in the labor pool to get to gender parity (based on how we hire today)")

## So if the pipeline is at least partially to blame, what can we do?

You saw above that enrollment in undergraduate computer science programs among women is growing linearly. Rising tuition costs coupled with 4-year universities’ inability to keep up with demand for computer science education have forced growing numbers of people to go outside the system to learn to code.

Below is a graph of the portion of developers who have a bachelor’s degree in computer science and the portion of developers who are at least partially self-taught, according to the Stack Overflow Developer Surveys from 2015 to 2019. As you can see, in 2015, the numbers were pretty close, and then, with the emergence of MOOCs, there was a serious spurt, with more likely to come.

![Chart showing education sources: BS in CS vs at least partially self-taught](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1086f_education_sources_for_software_engineers_3_6b49673d18.webp&w=1920&q=75 "Education sources for software engineers")

Sources: Stack Overflow Developer Surveys for [2015](https://stackoverflow.com/research/developer-survey-2015), [2016](https://stackoverflow.com/research/developer-survey-2016) and [2019](https://insights.stackoverflow.com/survey/2019#education).

The rate of change in alternative, more affordable education is rapidly outpacing university enrollment. Unlike enrollment in traditional four-year schools, enrollment in MOOCs and bootcamps is growing exponentially.

In 2015 alone, [over 35 million people have signed up for at least one MOOC course](https://www.classcentral.com/report/moocs-2015-stats/), and in 2018 MOOCs collectively had over 100M students. Of course, many people treat MOOCs as a supplement to their existing educational efforts or career rather than relying on MOOCs entirely to learn to code. This is something we factored into our model.

Despite their price tag (most charge on the order of $10-20K), bootcamps seem like a rational choice when compared to the price of top colleges. Since 2013, [bootcamp enrollment has grown 9X](https://www.coursereport.com/reports/2018-coding-bootcamp-market-size-research), with a total of 20,316 grads in 2018. Though these numbers represent enrollment across all genders[8](#user-content-fn-8) and the raw number of grads lags behind CS programs (for now), below you can see that the portion of women graduating from bootcamps is also on the rise and that graduation from online programs has actually reached gender parity (as compared to 20% in traditional CS programs).

![Infographic showing increase in women graduating from bootcamps](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffemale_grads_over_time_858b8b46ad.webp&w=1080&q=75 "Female Bootcamp Grads since 2011")

![Chart showing 39% of graduates from in-person bootcamps VS. 49.5% of graduates from online bootcamps are women](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fonline_vs_in_person_ce041533ef.webp&w=1920&q=75 "Gender Parity Online vs In-Person")

Of course, one may rightfully question the quality of grads from alternative education programs. We factored in [bootcamp placement rates](https://www.switchup.org/rankings/coding-bootcamp-survey) in building our updated model below.

Outside of alternative education programs, the most obvious thing we can do to increase the supply of qualified women engineers is to expand our pipeline to include strong engineers who don’t hail from top schools or top companies.

In previous posts, we looked at the relationship between interview performance and traditional credentialing and found that [participation in MOOCs mattered almost twice as much](https://interviewing.io/blog/lessons-from-3000-technical-interviews) for interview performance than whether the candidate had worked at a top company. And top school was least predictive of performance and [sometimes not at all](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt). And some of my earlier research indicates that the most predictive attribute of a resume is the number of typos and grammatical errors (more is bad), rather than top school or top company. In this particular study, experience at a top company mattered a little, and a [degree from a top school didn’t matter at all](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/).

But, even if lower-tier schools and alternative programs have their fair share of talent, how do we surface the most qualified candidates? After all, employers have historically leaned so hard on 4-year degrees from top schools because they’re a decent-seeming proxy. Is there a better way?

## But culling non-traditional talent is hard… that’s why we rely on pedigree and can’t change how we hire!

Of course, this post lives on our blog, so I’ll take a moment to plug what we do. In a world where there’s a growing credentialing gap and where it’s really hard to figure out how to separate a mediocre non-traditional candidate from a stellar one, we can help. interviewing.io helps companies find and hire engineers based on ability, not pedigree. We give out free mock interviews to engineers, and we use the data from these interviews to identify top performers, independently of how they look on paper. Those top performers then get to interview anonymously with employers on our platform (we’ve hired for Lyft, Uber, Dropbox, Quora, and many other great, high-bar companies). And this system works. Not only are our candidates’ conversion rates 3X the industry standard (about 70% of our candidates ace their phone screens, as compared to 20-25% in a typical, high-performing funnel), **about 40% of the hires made by top companies on our platform have come from non-traditional backgrounds. Because of our completely anonymous, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when an engineer unmasks at the end of a successful interview, the company in question realizes that the student who just aced their phone screen was one whose resume was sitting at the bottom of the pile all along** (we recently had someone get hired after having been rejected by that same company 3 times based on his resume!).

Frankly, think of how much [time and money you’re wasting competing for only a small pool of superficially qualified candidates](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should) when you could be hiring overlooked talent that’s *actually* qualified. Your CFO will be happier, and so will your engineers. Look, whether you use us or something else, there’s a slew of tech-enabled solutions that are redefining credentialing in engineering, from asynchronous coding assessments like CodeSignal or HackerRank to solutions that vet candidates before sending them to you, like Triplebyte, to solutions that help you vet your inbound candidate pool, like Karat.

And using these new tools isn’t just paying lip service to a long-suffering D&I initiative. It gets you the candidates that everyone in the world isn’t chasing without compromising on quality, helps you make more hires faster, and just makes hiring fairer across the board. And, yes, it will also help you meet your diversity goals. Here’s another model.

## How does changing your hiring practices improve the pipeline?

Above, you saw our take on status quo supply and demand of women engineers — basically how many engineers are available to hire using today’s practices versus how many we’d need to actually reach gender parity. Now, let’s see what it looks like when we include candidates without a traditional pedigree (yellow line).

![Chart showing reduction in gender parity](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fd27ee_gender_parity_is_within_reach_if_we_change_how_we_hire_ed3fb681a7.webp&w=1920&q=75 "Gender parity is within reach if we change how we hire")

As you can see, broadening your pipeline isn’t a magic pill, and as long as demand for software engineers continues to grow, it’s still going to be really hard, systemic changes to our society notwithstanding. If we do make these changes, however, the tech industry as a whole can accelerate its path toward gender parity and potentially get there within a decade.

### What about specific companies? An interactive visualization.

So far we’ve talked about trends in the industry as a whole. But, how do these insights affect individual employers? Below is an interactive model where you visualize when Google, Facebook, or your company (where you can plug in your hiring numbers) will be able to hit their goals based on current hiring practices versus the more inclusive ones we advocate in this post. Unlike the industry as a whole, built into this visualization is the idea of candidate reach, as well as hire rates — one company can’t source and hire ALL the women (as much as they might want to). Of course, the stronger your brand, the higher your response rates will be.

We made some assumptions about response rates to sourcing outreach for both Google and Facebook. Specifically, we guessed a 60%-70% response rate for these giants based on the strength of their brand and their army of sourcers — when those companies reach out and tenaciously follow up, you’ll probably respond eventually.[9](#user-content-fn-9) We also made some assumptions about their hire rates (5-10% of interviewed candidates). You can see both sets of assumptions below. And you can see that even with all the changes we propose, in our model, Google and Facebook will *still* not get to gender parity!

We also included a tab called “Your company” where you can play around with the sliders and see how long it would take your company to get to gender parity/whether it’s possible. There, we made much more conservative assumptions about response rates!

Goal:

By Year 2025

50% Women

05,00010,00015,00005,00010,00015,00005,00010,00015,00005,00010,00015,00005,00010,00015,000Demand (# women needed in the pool being sourced from)New women in pool (traditional + bootcamp)New women in pool (mostly top schools)

Facebook

Google

Your company

My eng team grows this percent every year

At my company there are this many engineers

Hire rate (percent)

Sourcing response rate (percent)

As you can see, for the giants, getting to gender parity is a tall order even with broadening your pipeline to include non-traditional candidates. And while it may be easier for smaller companies to get there without making drastic changes, when you’re small is exactly the right time to get fairer hiring into your DNA. It’s much harder to turn the ship around later on.

Regardless of whether you’re a giant or a small company, as long as hiring practices largely limits itself to top schools, the status quo will continue to be fundamentally inefficient, unmeritocratic, and elitist, and any hope of reaching gender parity will be impossible. Look, there are no easy fixes or band-aids when it comes to diversifying your workforce. Rather than continuing to look for hidden sources of women engineers (I promise, we’re not all hiding somewhere, just slightly out of reach) or trying to hire all the women from top schools, the data clearly shows that the only path forward is to improve hiring for everyone by going beyond top schools and hiring the best people for the job based on their ability, not how they look on paper.

I was recently in a pitch meeting where I got asked what interviewing.io’s mission is. I said that it’s to make hiring more efficient. The investors in the room were a bit surprised by this and asked, given that I care about hiring being fair, why that’s not the mission. First off, “fair” is hard to define and open to all manners of interpretation, whereas in an efficient hiring market, a qualified candidate, by definition, gets the job, with the least amount of pain and missteps. In other words, meritocracy is a logical consequence of efficiency. Secondly, and even more importantly, while I firmly believe that most people at companies want to do “the right thing”, it’s much easier to actually do the right thing in a big organization when it’s also cheaper, better, and faster.

All that’s to say that there are no shortcuts, and the most honorable (and most viable) path forward is to make hiring better for everyone and then hit your diversity goals in the process (or at least get closer to them). Software engineering is supposed to be this microcosm of the American dream — anyone can put in the work, learn to code, and level up, right? Until we own our very conscious biases about pedigree and change how we hire, that dream is a hollow pipe.

To assess whether there exists a pipeline problem, we need to estimate the number of job openings that exist, as well as the number of recent female job market entrants that could feasibly fill those roles. If a pipeline problem does exist, the number of job openings would be greater than the number of female entrants.

For this analysis, we focused on new jobs created over the next 10 years and ignored openings from existing jobs due to attrition. Unfortunately, engineering do  
es have a significantly higher attrition rate for women than other industries, so likely the numbers are worse than they appear in our models.9

That said, if a company wants to meet its diversity goals, it seems reasonable to expect them to do so with jobs that don’t yet exist, rather than on existing jobs whose pool of candidates we know are dominated by men.

### Demand: Projected new jobs created

Tech industry net new jobs created = (# tech industry jobs prior year) x (annual growth rate)

- Tech jobs (2018): 4 million ([Bureau of Labor Statistics](https://www.bls.gov/oes/current/oes150000.htm))
- 12% growth 2018-2028, or 1.2% annual growth ([Bureau of Labor Statistics](https://www.bls.gov/ooh/computer-and-information-technology/home.htm))

### Supply: Projected new women in job pool from top tier universities

New women in job pool = (# CS graduates prior year) x (annual CS graduate growth rate) x (% CS graduates that are women) \* (% CS graduates from top tier schools)

- Current # CS graduates: 70,000 ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))
- CS graduate growth rate: 10% ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))
- % women: 20% ([National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_325.35.asp?current=yes))
- % of all CS graduates from top tier schools: 25%

### Supply: Projected new women in job pool beyond top tier universities

This represents female bootcamp graduates plus female CS graduates not from top schools.

New women in job pool from beyond top schools =  
(# bootcamp graduates prior year) x (% bootcamp graduates that are women) x (% annual bootcamp graduate growth)  
+ (# CS graduates prior year) x (annual CS graduate growth rate) x (% women in CS grads) x (% CS graduates not from top tier schools)

Bootcamp assumptions:

- Current # bootcamp graduates: 20,000 ([Course Report](https://www.coursereport.com/reports/coding-bootcamp-market-size-research-2019))
- % bootcamp graduates that are women: 40% ([Switchup](https://www.switchup.org/rankings/coding-bootcamp-survey#student-demographics))
- Bootcamp graduates growth rate: 10% ([Course Report](https://www.coursereport.com/reports/coding-bootcamp-market-size-research-2019))
- Placement rate: 50% ([Switchup](https://www.switchup.org/rankings/coding-bootcamp-survey#employment-outcomes))
- % CS graduates not from top tier schools: 75% (see assumption from “Supply: Projected new women in job pool from top tier universities”)

Assumptions for CS graduates beyond top tier universities are the same as those found under “Supply: Projected new women in job pool from top tier universities”, but taking the remaining 75% of CS graduates excluded there.

### Company-specific Demand: Projected number of candidates needed to source for job openings

Number of women to source = (# Engineers employed prior year) x (% annual growth rate) x (% diversity goal) x (1 / hire rate) x (1 / sourcing response rate)

In practice, companies typically need to contact many people for any single job opening, since there is plenty of inherent variability in the sourcing and interview process. This line describes how many people your company would have to reach to fill all new job openings created, based on assumptions about your company’s hiring practices.

*Thank you to the interviewing.io data & eng team for all the data modeling, projections, and visualizations, as well as everyone who proofread the myriad long drafts.*

1. The Kapor Center has some great, data-rich reports on attrition in tech as well as systemic factors that contribute to the leaky pipeline. For a detailed study of attrition in tech, please see the [Tech Leavers Study](https://www.kaporcenter.org/wp-content/uploads/2017/08/TechLeavers2017.pdf). For a comprehensive look at systemic factors that contribute to the leaky tech pipeline (and a series of long-term solutions), please see this [comprehensive report](https://www.kaporcenter.org/wp-content/uploads/2018/02/KC18001_report_v6-1.pdf). For a survey of the deplorable state of computer science education in California schools, please see [this report](https://www.kaporcenter.org/wp-content/uploads/2019/06/Computer-Science-in-California-Schools.pdf). [↩](#user-content-fnref-1)
2. For instance, MIT keeps their race stats behind a [login wall](https://idp.mit.edu/idp/Authn/MIT?conversation=e1s1). [↩](#user-content-fnref-2)
3. According to a [HackerRank study](https://www.cio.com/article/222270/where-in-the-world-are-the-women-software-developers.html), “India, the United Arab Emirates, Romania, China, Sri Lanka, and Italy are the six countries with the highest percentage of women developers, [whereas the] U.S. came in at number 11.” [↩](#user-content-fnref-3)
4. We assumed a 1:7 ratio of PMs to engineers and a 1:7 ratio of designers to engineers on product teams. Removing PMs and designers from our numbers does not mean to imply that their representation in tech doesn’t matter but rather to scope this post specifically to software engineers.) [↩](#user-content-fnref-4)
5. The [National Center for Education Statistics](https://nces.ed.gov/programs/digest/d18/tables/dt18_322.50.asp?current=yes) doesn’t yet list graduation rates beyond 2017… the new numbers might be a bit higher, as you’ll see when you look at enrollment numbers for CS a bit further down in the post. [↩](#user-content-fnref-5)
6. This is *not* independent of the idea that deep, systemic issues within the fabric of our society (such as the ones we mention at the beginning of the post) are keeping women from entering engineering in droves. But, as I mentioned in the intro, laying the blame at the feet of these systemic issues entirely paralyzes us and prevents us from fixing the things we can fix. [↩](#user-content-fnref-6)
7. We couldn’t find a public record of women’s numbers by year and so are relying on graduation rates from Course Report as a proxy. [↩](#user-content-fnref-7)
8. These rates might seem high to recruiters reading this post. They *might* be high. We did try to correct for 2 things, both of which made our estimates higher: 1) this includes university and junior candidates, who tend to be way more responsive, and 2) this isn’t per sourcing attempt but over the lifetime of a candidate, so it includes followups, outreach to candidates who had previously been declined, and so on. However, if this still seems off, please write in and tell us! [↩](#user-content-fnref-8)
9. There are two good sources that look at attrition among women in tech. One is [Women in Tech: The Facts](https://wpassets.ncwit.org/wp-content/uploads/2021/05/13193304/ncwit_women-in-it_2016-full-report_final-web06012016.pdf), published by the National Center for Women&IT. The other is the excellent [Tech Leavers Study](https://www.kaporcenter.org/wp-content/uploads/2017/08/TechLeavers2017.pdf) by the Kapor Center. [↩](#user-content-fnref-9)


# [Want to work from home? That’ll be $46,282.](https://interviewing.io/blog/cost-of-working-remotely-and-other-reasons-we-leave-money-on-the-table)

By Elliot Plant | Published: February 13, 2024

**TL;DR: We sacrifice a lot to get the job we want, but the data says it’s not more than $50k.**

Software engineering jobs come with a lot of perks. We have modern offices, our hours are flexible, and recruiters assure us that our projects will change the world. But that doesn’t mean our jobs are perfect. Work requires us to commute, reduces the time we can spend with family, increases our stress levels, and forces us to deal with stressed-out teammates. And sometimes we work for companies with questionable morals and use technologies we don’t enjoy.

For some of us, it’s worth trading cash for a job that fits into our lives better. But figuring out just how much money to give up is difficult without any data.

So we got the data.

We asked the interviewing.io audience about times they accepted a job offer with lower total compensation because they prioritized other aspects of the job. Surprisingly, it turns out that most engineers have never accepted an offer with lower monetary compensation. And for the engineers who have taken lower pay for higher values, we found that $50k was as much as most had sacrificed.

In this post, we’re going to dive into what drives our compensation choices and how that can help you in your next job hunt.

## When do we make compensation tradeoffs?

Let’s say you’re on your way to getting a new job. You’ve applied to enough companies, performed well in your interviews, and now you have a stack of exciting offers in your inbox. Of course, you’re negotiating your offers to make sure you’re getting the best deals you can, but your first choice company is still offering you the third most compensation. Should you take the deal?

Or, instead, imagine that you’re working with a team you can’t stand. Before starting to look for a new gig, wouldn’t it be nice to figure out how much it costs to find a better team?

We all value perks differently, but it’s important to know the market value of our values before making major life decisions.

## How did you calculate $46,282?

We sent a survey to interviewing.io users that asked about times they’d received multiple offers and had taken the offer with lower compensation. We wanted to know *why* they selected the lower offer and how much money they left on the table.

For respondents who had never taken a lower-comp offer, we asked what they *would* sacrifice compensation for. The results below are based on the primary (first ranked) reason respondents said they did (or would) accept a lower offer. We also asked everyone if they had advice for themselves for their next job hunt in a free-form response, which gave us some insight into how people make career decisions.

Let’s dive into the results to see which quality-of-life factors are valued the most.

![Pie chart showing responses to a survey asking whether respondents had ever accepted a job offer with lower total compensation, with 70.4% selecting 'No' and 29.6% selecting 'Yes'.](https://strapi-iio.s3.us-west-2.amazonaws.com/ever_accepted_6455051b5c.png)

Of the 480 responses, 70% have never taken an offer with lower compensation. It's clear that while money doesn’t solve all of our problems, it certainly must solve a lot of them. There’s no shame in taking the money — the majority of people do.

Some respondents were in the fortunate position where their highest cash offer was also most aligned with other aspects of the job. If you find yourself in this scenario, consider yourself lucky.

The remaining 30% of respondents sacrificed an average of $47k to get something that was important to them.

![Bar chart showing average pay cut based on primary reason for accepting lower job offer: Mission $51,300, Commute/Remote $46,282, Work/Life Balance $42,563, Technology $42,250, Prestige $41,214, Team $36,737](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Cut_Avg_Reason_c1a932f1f7.png)

The differences in sacrificed pay between different reasons are quite small. The only significant difference was between Team ($37k) and Mission ($51k), which had the smallest and largest average loss in pay.

Mission may have required the largest pay drop because the type of companies that have positive missions tend to have less funding and profit (unfortunately). Their payrolls simply aren’t as big as their “evil” competitors’ payrolls.

Team may have required a significantly lower pay cut because it’s not as concrete of a value as the others, so it’s less worth paying for. Unlike commute time saved by working from home, it’s much harder to quantify an increase in the likelihood you’ll get along with your new teammates. We expect that makes it harder to justify a reduction in compensation.

![Bar chart titled 'Pay difference bucketed by amount for all reasons' showing percentage of respondents per pay cut size range: 18% took cuts of $0-$9k and $20k-$24k, 13% took $30k-$34k cuts, 21% took $40k-$44k cuts, 9% took $50k-$54k cuts, 4% took $60k-$64k cuts, 1% each took $70k-$74k, $80k-$84k, $90k-$94k cuts, 6% took $95k-$99k cuts, 8% took over $100k pay cuts.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Bucket_Reason_61625cfd30.png)

Around the $50k mark, we see a considerable drop in the number of people who have taken lower pay for their values. There’s a limit to how big of a pay cut we will accept for any reason, and that limit appears to be about $50k.

While the reason for a pay cut appears to not be very significant, the *size* of the pay cut plays a huge role. We know that [money matters when recruiters reach out to us](https://blog.alinelerner.com/what-i-learned-from-reading-8000-recruiting-messages/), so it’s not surprising that it also matters when we’re accepting offers.

## What about visas?

Surely visa sponsorship doesn’t abide by the $50k cutoff, right? After all, many of the items above could be considered a luxury, but visa sponsorship is not, and in some cases, you may not have many options when making this decision (e.g., if you were laid off and then have 60 days to find a new job before you have to leave the country[1](#user-content-fn-1)). After computing our initial set of results above, we realized that we had inadvertently left visas out, so we created a second survey about pay sacrifices for visa sponsorships. We asked our audience if they had ever taken a lower paying job to secure a visa, and we asked them to specify what type of visa they received: H-1B, H-1B transfer, non-H-1B, or non-H-1B transfer.

We expected visa sponsorship needs would lead to larger pay cuts than the less tangible requirements in our first survey. Surprisingly, we still found ourselves around the $50k mark.

![Bar chart titled 'Average pay cut for visa types and non-visa' showing average difference from maximum offer in thousands of dollars: $51,428 for New Non H-1B visas, $49,051 for Transfer H-1B visas, $46,680 for New H-1B visas, $46,060 for Non-Visa situations.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Cut_Avg_Visa_d4c90718d8.png)

The averages for new H-1B, new non-H-1B, and transferred H-1B visas were very close to each other and also not significantly different from the average pay cut for intangible, “non-visa” reasons (from the first part of the post). Once again we see a cliff around $50k, beyond which we find few engineers willing to take a pay cut even for immigration status changes.

![Bar chart titled 'Pay difference bucketed by amount for visa seekers' showing percentage of respondents per pay cut size range: 3% took $0-$4k cuts, 17% took $10k-$14k and $20k-$24k cuts, 22% took $40k-$44k cuts, 14% took $30k-$34k cuts, 8% took $50k-$54k cuts, 4% took $70k-$74k cuts, 1% took $80k-$84k cuts, 6% each took $90k-$94k and $95k-$99k cuts, 3% took over $100k pay cuts.](https://strapi-iio.s3.us-west-2.amazonaws.com/pay_Diff_Bucket_Visa_0271d34130.png)

The scarcity of accepted offers beyond a $50k pay cut shows us how highly engineers value cash. Both quality-of-life factors and more concrete rewards like visas are equally unlikely to entice them to accept less.

## How do our projections compare to our actions?

Finally, we wanted to know if the reasons engineers actually took lower paying jobs reflected the reasons that engineers said they would take lower jobs. For the engineers who had accepted a lower offer, we grouped them by the primary reason they stated for accepting that offer. For engineers who had never accepted a lower offer, we asked them what reasons, if any, they would hypothetically accept a lower offer, and we grouped those engineers by their primary reason.

![Two pie charts comparing the primary reasons engineers have accepted or would accept lower paying job offers including work-life balance, commute, prestige, team, mission, and technology](https://strapi-iio.s3.us-west-2.amazonaws.com/reason_compare_41f6b761e9.png)

The reasons we *actually* accept lower-paying jobs appear to line up closely with the reason we *think* we would accept those jobs. Work/Life Balance is still the clear leader, but Commute/Remote takes up a much larger slice in the hypothetical situation than the real one. That may be a symptom of companies moving back into the office while their engineers want to stay remote. If you just lost your WFH flexibility, consider asking yourself how much you would pay to get it back.

## Your next job hunt

So, what can you do with this information? At the beginning of your next job hunt, think critically about your values and consider how much you would pay to keep them. Most engineers in your shoes haven’t sacrificed more than $50k for any reason, so be wary of extending beyond that limit.

If you value working from home but your WFH option pays $10k less, you’re actually getting a great deal compared to your peers. If, on the other hand, you have an offer for a team you’d like to work with more, but it comes with a $80k pay cut, consider that you’re losing roughly twice the average of other engineers looking for the same thing.

If you set yourself up in a strong negotiating position by getting multiple offers, leverage them to make your favorite company compensate you as well as your not-quite-as-great-but-higher-paying choice. If you can’t get them to match, at least try to get them within $50k.

Life isn’t all about money, but it’s up to you to make sure you’re paid what you’re worth.

![Author avatar](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FElliot_Headshot_a87ff0cadc.jpeg&w=384&q=75 "Elliot Plant")

Elliot Plant

Elliot Plant enjoys everything about building. He has a degree in mechanical engineering, a job writing software, and he tinkers with electronics in his free time. Elliot currently works for Anthropic, and previously spent time at Google and Tesla as well his own startup. When the weather is nice, you can find Elliot walking his dog in the Oakland hills. Online, you can find him on [GitHub](https://github.com/elliotaplant), [LinkedIn](https://linkedin.com/in/elliotplant), or [Twitter](https://twitter.com/plant_elliot).

1. This is terrible, and we wholeheartedly believe these laws need to change. [↩](#user-content-fnref-1)


# [Impostor syndrome strikes men just as hard as women... and other findings from thousands of technical interviews](https://interviewing.io/blog/impostor-syndrome-strikes-men-just-as-hard-as-women)

By Aline Lerner | Published: October 29, 2018; Last updated: May 1, 2023

The modern technical interview is a rite of passage for software engineers and (hopefully!) the precursor to a great job. But it’s also a huge source of stress and endless questions for new candidates. Just searching “how do I prepare for a technical interview” turns up millions of Medium posts, coding bootcamp blogs, Quora discussions, and entire books.

Despite all this conversation, people struggle to know how they’re even doing in interviews. [In a previous post](https://interviewing.io/blog/own-interview-performance), we found that a surprisingly large number of interviewing.io’s users consistently underestimate their performance, making them more likely to drop out of the process and ultimately harder to hire. Now, and with considerably more data (over 10k interviews led by real software engineers!), we wanted to go deeper: **what seems to make candidates worse at gauging their own performance?**

We know some general facts that make accuracy a challenge: people aren’t always great at assessing or even remembering their performance on difficult cognitive tasks like writing code.[1](#user-content-fn-1) Technical interviews can be particularly hard to judge if candidates don’t have much experience with questions with no single right answer. Since many companies don’t share any kind of detailed post-interview feedback (beyond a yes/no) with candidates for liability reasons, many folks never get any sense of how they did, what they did well, or what could have been better.[2](#user-content-fn-2),[3](#user-content-fn-3) Indeed, pulling back the curtain on interviewing, *across the industry,* was one of the primary motivators for building interviewing.io!

But to our knowledge there’s little data out there looking specifically at how people feel after real interviews on this scale, across different companies–so we gathered it, giving us the ability to test interesting industry assumptions about engineers and coding confidence.

One big factor we were interested in was **impostor syndrome**. Impostor syndrome resonates with a lot of engineers[4](#user-content-fn-4), indicating that many wonder whether they truly match up to colleagues and discount even strong evidence of competence as a fluke. Impostor syndrome can make us wonder whether we can count on the positive performance feedback that we’re getting, and how much our opportunities have come from our own effort, versus luck. Of particular interest to us was whether this would show up for women on our platform. There’s a lot of research evidence that candidates from underrepresented backgrounds experience a greater lack of belonging that feeds impostor syndrome[5](#user-content-fn-5), and this could show up as inaccuracy about judging your own interview performance.

interviewing.io is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.

When an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (feel free to [watch this process in action on our interview recordings page](https://interviewing.io/mocks)). After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.

Here’s an example of an interviewer feedback form:

![Screenshot of the Interviewing.io interviewer feedback form highlight the question: How were their technical skills?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fdaa_new_interviewer_feedback_circled_8e5be18c42.png&w=750&q=75 "How were their technical skills?")

Immediately after the interview, candidates answered a question about how well they thought they’d done on the same 1-4 scale:

![Screenshot of the Interviewing.io interviewee feedback form highlight the question: How well do you think you did?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F15c64_new_interviewee_feedback_circled_edb300c741.png&w=640&q=75 "How well do you think you did?")

For this post, we looked at over 10k technical interviews led by real software engineers from top companies. In each interview, a candidate was rated by an interviewer on their problem-solving ability, technical ability, and communication skills, as well as whether the interviewer would advance them to the next round. This gave us a measure of how different someone’s self-rating was from the rating that the interviewer actually gave them, and in which direction. In other words, how skewed was their estimation from their true performance?

Going in, we had some hunches about what might matter:

- **Gender.** Would women be harder on their coding performance than men?
- **Having been an interviewer before.** It seems reasonable that having been on the other side will pull back the curtain on interviews.
- **Being employed at a top company.** Similar to above.
- **Being a top-performing interviewee** on interviewing.io — people who are better interviewees overall might have more confidence and awareness of when they’ve gotten things right (or wrong!)
- **Being in the Bay Area** or not. Since tech is still so geographically centered on the Bay Area, we considered that folks who live in a more engineering-saturated culture could have greater familiarity with professional norms around interviews.
- **Within the interview itself, question quality and interviewer quality.** Presumably, a better interviewer is also a better communicator, whereas a confusing interviewer might throw off a candidates’ entire assessment of their performance. We also looked at whether it was a practice interview, or for a specific company role.
- For some candidates, we could also look at few measures of their **personal brand** within the industry, like their number of GitHub and Twitter followers. Maybe people with a strong online presence are more sure of themselves when they interview?

## So what did we find?

### Women are just as accurate as men at assessing their technical ability

Contrary to expectations around gender and confidence, we *didn’t* find a reliable statistically significant gender difference in accuracy. At first, it looked like female candidates were more likely to underestimate their performance, but when we controlled for other variables, like experience and rated technical ability, it turned out **the key differentiator was experience.** More experienced engineers are more accurate about their interview performance, and men are more likely to be experienced engineers, but experienced female engineers are just as accurate about their technical ability.

Based on previous research, we hypothesized that impostor syndrome and a greater lack of belonging could result in female candidates penalizing their interview performance, but we didn’t find that pattern[6](#user-content-fn-6). However, our finding echoes [a research project from the Stanford Clayman Institute for Gender Research](https://gender.stanford.edu/sites/gender/files/climbing_the_technical_ladder.pdf), which looked at 1,795 mid-level tech workers from high tech companies. They found that women in tech aren’t necessarily less accurate when assessing their own abilities, but do have significantly different ideas about what success requires (e.g., long working hours and risk-taking). In other words, **women in tech may not doubt their own abilities but might have different ideas about what’s expected**. [And a survey from Harvard Business Review](https://hbr.org/2014/08/why-women-dont-apply-for-jobs-unless-theyre-100-qualified)  asking over a thousand professionals about their job application decisions also made this point. Their results emphasized that gender gaps in evaluation scenarios could be more about **different expectations for how scenarios like interviews are judged.**

That said, we did find one interesting difference: women went through fewer practice interviews overall than men did. The difference was small but statistically significant, and harkens back to our earlier finding that women leave interviewing.io roughly 7 times as often as men do, after a bad interview.

But in that same earlier post, we also found that masking voices didn’t impact interview outcomes. This whole cluster of findings affirms what we suspected and what the folks doing [in-depth studies of gender in tech](https://gender.stanford.edu/sites/gender/files/climbing_the_technical_ladder.pdf) have found: **it’s complicated.** Women’s lack of persistence in interviews can’t be explained only by impostor syndrome about their *own* abilities, but it’s still likely that they’re interpreting negative feedback more severely and making different assumptions about interviews.

Here’s the distribution of accuracy distance for both female and male candidates on our platform (zero indicates a rating that matches the interviewer’s score, while negative values indicate underestimated score, and positive values indicate an overestimated score). The two groups look pretty much identical:

### What else didn’t matter?

Another surprise: **having been an interviewer didn’t help**. Even people who had been interviewers themselves don’t seem to get an accuracy boost from that. **Personal brand was another non-finding**. **People with more GitHub followers weren’t more accurate** than people with few to no GitHub followers. **Nor did interviewer rating matter** (i.e. how well an interviewer was reviewed by their candidates), although to be fair, interviewers are generally rated quite highly on the site.

### So what was a statistically significant boost to accurate judgments of interview performance? Mostly, experience.

Experienced engineers have a better sense for how well they did in interviews, compared with engineers earlier in their careers.[7](#user-content-fn-7) But it doesn’t seem to *just* be that you’re better at gauging your interview performance because you’re better at writing code; although there is a small lift from this, with higher rated engineers being more accurate. But when you look at junior engineers, **even top-performing junior candidates struggled to accurately assess their performance**.[8](#user-content-fn-8)

Our data mirrors a trend seen in [Stack Overflow’s 2018 Developer survey](https://insights.stackoverflow.com/survey/2018#connection-and-competition). They asked respondents several questions about confidence and competition with other developers, and noted that more experienced engineers feel less competitive and more confident.[9](#user-content-fn-9) This isn’t necessarily surprising: experience is correlated with skill level, after all, and highly skilled people are likely to be more confident. But our analysis let us control for performance and code skill within career groups, and we *still* found that experienced engineers were better at predicting their interview scores. There are probably multiple factors here: experienced engineers have been through more interviews, have led interviews themselves, and have a stronger sense of belonging, all of which may combat impostor syndrome.

**Insider knowledge and context also seems to help:** Being in the Bay Area and being at a top company both made people more accurate. Like the experienced career group, engineers who seem more likely to have *contextual industry knowledge* are also more accurate. We found small but statistically significant lifts from factors like being located in the Bay Area and working at a top company. However, the lift from working at a top company seems to mostly measure a lift from overall technical ability: being at a top company is essentially a proxy measure for being a more experienced, higher quality engineer.

**Finally, as you get better at interviewing and move into company interviews, you do get more accurate.** People were more accurate about their performance in company interviews compared to practice interviews, and their overall ranking on the interviewing.io site also predicted improved accuracy: interviewing.io also gives users an overall ranking, based on their performance over multiple interviews and weighted toward more recent measures. People who scored in the top 25% were more likely to be accurate about their interview performance.

In general, how are people at gauging their interview performance overall? [We’ve looked at this before](https://interviewing.io/blog/own-interview-performance), with roughly a thousand interviews, and now, with ten thousand, the finding continues to hold up. Candidates were accurate about how they did in only 46% of interviews, and underestimated themselves in 35% of interviews (and the remaining 19%, of course, are the overestimators). Still, candidates are generally on the right track — it’s not like people who score a 4 are always giving themselves a 1.[10](#user-content-fn-10) Self-ratings *are* statistically significantly predictive for actual interview scores (and positively correlated), but that relationship is noisy.

## The implications

Accurately judging your own interview performance is a skill in its own right and one that engineers need to learn from experience and context in the tech industry. But we’ve also learned that **many of the assumptions we made about performance accuracy didn’t hold up to scrutiny —** female engineers had just as accurate a view of their own skills as male ones, and engineers who had led more interviews or were well known on GitHub weren’t particularly better at gauging their performance.

What does this mean for the industry as a whole? First off, impostor syndrome appears to be the bleary-eyed monster that attacks across gender ability, and how good you are, or where you are, or how famous you are isn’t that important. Seniority does help mitigate some of the pain, but impostor syndrome affects everyone, regardless of who they are or where they’re from. So, maybe it’s time for a kinder, more empathetic interviewing culture. And a culture that’s kinder to everyone, because though marginalized groups who haven’t been socialized in technical interviewing are [hit the hardest by shortcomings in the interview process](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview), no one is immune to self-doubt.

We’ve previously discussed what makes someone a good interviewer, and [empathy plays a disproportionately large role](https://interviewing.io/blog/best-technical-interviews-common). And we’ve seen that [providing immediate post-interview feedback is really important for keeping candidates from dropping out](https://interviewing.io/blog/own-interview-performance). So, whether you’re motivated by kindness and ideology or cold, hard pragmatism, a bit more kindness and understanding toward your candidates is in order.

*[Cat Hicks](https://www.drcathicks.com/), the author of this guest post, is a researcher and data scientist with a focus on learning. She’s published empirical research on learning environments, and led research on the cognitive work of engineering teams at Google and Travr.se. She holds a PhD in Psychology from UC San Diego.*

1. Self-assessment has been explored in a number of domains, and often used to measure learning. One important criticism is that it’s highly impacted by people’s motivation and emotional state at the time of asking. See: Sitzmann, T., Ely, K., Brown, K. G., & Bauer, K. N. (2010). Self-assessment of knowledge: A cognitive learning or affective measure?. *Academy of Management Learning & Education*, *9*(2), 169-191. [↩](#user-content-fnref-1)
2. Designing a good technical interview is no small task on the interviewer side. For an informal discussion of this, [see this post.](https://erikbern.com/2018/05/02/interviewing-is-a-noisy-prediction-problem.html)  [↩](#user-content-fnref-2)
3. For some anecdotal conversation about interview self-assessment, [see this one](https://www.gayle.com/blog/2011/03/31/why-your-interview-performance-is-impossible-to-judge). [↩](#user-content-fnref-3)
4. E.g., [this article](https://hired.com/blog/candidates/truth-imposter-syndrome-tech-workers/) and [this one.](https://www.cnet.com/science/tech-employees-likely-to-suffer-from-impostor-syndrome/) [↩](#user-content-fnref-4)
5. Some examples of further reading in social science research: Good, C., Rattan, A., & Dweck, C. S. (2012). Why do women opt out? Sense of belonging and women’s representation in mathematics. *Journal of personality and social psychology*, *102*(4), 700. Master, A., Cheryan, S., & Meltzoff, A. N. (2016). Computing whether she belongs: Stereotypes undermine girls’ interest and sense of belonging in computer science. *Journal of Educational Psychology*, *108*(3), 424. [↩](#user-content-fnref-5)
6. One complication for our dataset is the representation of experienced female engineers: we simply didn’t have very many, which is true to the demographics of the tech industry, but also means that selection biases in the small group of experienced female engineers we do have are more likely to be present, and this isn’t the be-all and end-all of exploring for group differences. We’d like to continue looking at interviews with female participants to explore this fully. [↩](#user-content-fnref-6)
7. These effects and the previous non-findings were all explored in a linear mixed model. Significant results for the individual effects are all p < .05 [↩](#user-content-fnref-7)
8. Experienced engineers have an average skew of -.14; Junior engineers have an average skew of -.22, New Grads have an average skew of -.25. [↩](#user-content-fnref-8)
9. See also: <https://www.dice.com/career-advice/imposter-syndrome-tech-pros-age> [↩](#user-content-fnref-9)
10. Another wrinkle with the design behind this data is that there’s a floor and a ceiling on the scale: people who always score a 4, for example, *can’t* ever overrate themselves, because they’re already at the top of the scale. We dealt with this a couple of ways: by excluding people at the floor and ceiling and re-running analyses on the middle subset, and by binning skew into either accurate or not and looking at that. The findings hold up across this. [↩](#user-content-fnref-10)


# [I’ve conducted over 600 technical interviews on interviewing.io. Here are 5 common problem areas I’ve seen.](https://interviewing.io/blog/ive-conducted-over-600-technical-interviews-on-interviewing-io-here-are-5-common-problem-areas-ive-seen)

By Ian Douglas | Published: October 14, 2020; Last updated: May 1, 2023

*Hey, Aline (founder of interviewing.io) here. This is the second post in our Guest Author series The first post talked about* [red flags you might encounter while interviewing with companies](https://interviewing.io/blog/6-red-flags-i-saw-while-doing-60-technical-interviews-in-30-days)*. Complementarily, this post,* *authored by one of our prolific, long-time interviewers*, *explores common missteps that interviewees make.*

*One of the things I’m most excited about with the Guest Author series is the diversity of opinions it’s bringing to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I still don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*

![Author avatar](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FIan_Douglas_44c37f40a1.jpg&w=384&q=75 "Ian Douglas")

Ian Douglas

William Ian Douglas goes by “Ian”, and uses he/him pronouns. He lives in the Denver, Colorado region and graduated from a Computer Engineering program in 1996. His career spans back-end systems, API architecture, DevOps/DBA duties and security, and has been a team lead managing small teams, and Director of Engineering. Ian branched out into professional technical interview coaching in 2014, and in 2017 pivoted his entire career to teaching software development for the Turing School of Software & Design in the Denver area. He joined interviewing.io as a contract interviewer in the summer of 2017 and is a big fan of the data analytics blog posts that IIO produces to help expose and eliminate bias in our tech industry interviews. Ian writes technical coaching information at [https://techinterview.guide](https://techinterview.guide/) and you can reach him on [Twitter](https://twitter.com/iandouglas736), [LinkedIn](https://www.linkedin.com/in/iandouglas736) and [GitHub](https://github.com/iandouglas).

**I recently conducted my 600th interview on interviewing.io (IIO). I’d like to share lessons learned, why I approach interviews the way that I do, and shed some light on common problem areas I see happen in technical interviews.** Every interviewer on the platform is different, and so your results may vary. We have some excellent folks helping out on the platform, and have a wonderful community working to better ourselves.

## The interviewing.io Mock Interview

During our interviews on IIO, we rate people on three 4-point scales. A score of 1 means they did extremely poorly, and a 4 means they did extremely well in that category. I typically start my interview where everyone gets 3 out of 4 points right away, and then earn/lose points as the interview goes on.

Every interviewer on the platform will have some aspect that they favor over others. My own bias as an interviewer tends to be around communication and problem solving, which I’ll point out below.

### Technical Proficiency

In this category, I grade a candidate on how proficient they seem in their language of choice, whether they had significant problems coding an algorithm of a particular style, if I needed to give a lot of hints during coding.

### Problem Solving

Here, I grade a candidate on how well they break the problem into smaller pieces, come up with a strategy for solving the smaller problems, and also debugging issues along the way. The ability to think through problems while debugging is just as important as writing the code in the first place. Are they stumped when a problem happens, or are they able to find the root cause on their own?

Interviewers really want to hear your decision-making process. This is also very important when debugging code. I tended to hire folks who would fit in well on smaller teams or clusters of developers. With that in mind, collaboration and easy communication are a good way to win me over.

## Common Problem Areas I See in Interviews

Here are the top problem areas I see in interviews, not just on IIO, but in general. I hope you find this advice helpful.

### Common Problem Area 1: Jumping into code too soon

I see this in developers of all types and levels, but mostly in the “intermediate” level of 2-5 years of experience. They hear a problem, talk about a high-level design for 30 seconds or less, and are eager to get coding. They feel like they’re on a timer. They want to rush to get things finished. It’s a race to the finish line. First one across the finish line is the winner.

Right?

Nope.

Please, slow down. Plan your work. And share your thought process along the way.

People who take time to think out a mid-level design, whether that’s pseudocode or just writing out notes of their approach, tend to spend less time debugging their code later. Folks who jump right into coding fall into what I call “design-as-you-go” problems, where you’re spending lots of time refactoring your code because you need to change a parameter passed or a return value, or wait, that loop is in the wrong place, etc.. This is very easy to spot as an interviewer.

Spending some time on mid-level design doesn’t guarantee your success, but it might save you time in the long run by thinking through your plan a little deeper, and that extra time you bought could be used to fix problems later.

Also, as an interviewer, I want to see you succeed. Especially if you’re “on-site” (in person, or remote nowadays) because you’re costing our company a lot more money in an on-site interview process. While I need to be fair to all candidates in the amount of help I can give, if I can see your design ahead of time, and spot a flaw in that design, I can ask leading questions to guide you to the problem and correct your approach earlier.

**If you jump straight into code, I have no idea if your implementation is even going to work, and that’s not a great place to put your interviewer. It’s much harder for me to correct a design when you have 100 lines of Java code written before I really understand what’s going on in your code.**

I saw this lack of planning backfire in a horrible way in a real interview in 2012. The candidate was brought to my interview room by someone in HR, asked if they would like a bottle of water, and promised to return. We introduced ourselves and got down to the technical challenge. The candidate shared no details, no design, barely talked about a high-level approach, wrote nothing down, and started writing code on a whiteboard. (This would be my second-to-last whiteboard interview I ever conducted, I hate whiteboard interviews!) HR showed up a few minutes later, knocking loudly on the door, offering the bottle of water and leaving. The candidate, grateful for a drink, uncapped the bottle and started to take a sip when this awful, draining look came over their face. **The distraction of delivering a bottle of water made them completely lose their train of thought, and I couldn’t help them recover because they hadn’t shared any details with me about their approach.** They spent several minutes re-thinking the problem and starting over.

On the “other side” of this coin, however, you can spend “too long” on the design stage and run out of time to implement your genius plan. I’ve seen candidates talk through a mid-level design, then write notes, then manually walk through an example with those notes to really make sure their plan is a good one, and now they only have a few minutes left to actually implement the work. Extra points on communication, maybe, but we need to see some working code, too.

So what’s the best approach here?

I typically recommend practicing until you spend about 5 minutes thinking through high-level design choices, 5 minutes to plan and prove the mid-level design, and then get to work on code. The good news here is that “practice makes better” — the more you practice this design break-down and problem solving, the better you’ll get. More on this later.

### Common Problem Area 2: Communicating “Half-thoughts”

This is a term I’ve coined over the years, where you start to say a thought out loud, finish the thought in your head, and then change something about your code. It usually sounds something like this:

*“Hmm, I wonder if I could … … … no, never mind, I’ll just do this instead.”*

Back to my bias for communication.

Interviewers want to know what’s going on in your thought process. It’s important that they know how you’re making decisions. How are you qualifying or disqualifying ideas? Why are you choosing to implement something in a particular way? Did you spot a potential problem in your code? What was it?

This missing information is a hidden treasure for your interviewer. It takes mere seconds to change your communication to something more like this:

*“I wonder if … hmm … well, I was thinking about implementing this as a depth-first-search, but given a constraint around \_\_\_ I think a better approach might be \_\_\_, what do you think?”*

**That took maybe 2 or 3 extra seconds, and you’ve asked for my opinion or buy-in, we can consider possibilities together, and now we’re collaborating on the process.** You already feel like my future coworker!

### Common Problem Area 3: Not asking clarifying questions

An interview challenge I often ask as a warm-up question goes something like this:

*You have a grouping of integer numbers. Write a method that finds two numbers that add up to a given target value, stop immediately, and report those numbers. Return two ‘null’ values if nothing is found.*

This is a great question that shows me how you think about algorithms and the kinds of assumptions you make when you hear a problem.

I’ve been coding for a pretty long time. Since 1982, actually. There’s no data structure called “a grouping” in any language I’ve ever used. So what assumptions are you going to make about the problem?

Most candidates immediately assume the “grouping” of numbers is in an array. You can successfully solve this problem by using an array to store your numbers. Your algorithm will likely be an O(n^2) (n-squared) algorithm because you’ll be iterating over the data in an exponential way: for each value, iterate through the rest of the values. There’s a more efficient way to solve this in O(n) time by choosing a different data structure.

Go ahead and ask your interviewer questions about the problem. If they tell you to make your own assumptions that’s different, but ask if they’re good assumptions. Ask if there are alternate data sets that you’ll be using as test cases which could impact your algorithm.

### Common Problem Area 4: Assuming your interviewer sets all the rules

Wait, what?

Yeah, you read me right.

Yes, you’re there for the interview, but you’re there to show them how you’ll work on the team, and teams work best when there is clear, open communication and a sense of collaboration. Spend the first few minutes of the interview setting expectations, especially around communication and work process.

**There’s nothing wrong with having this kind of chat with your interviewer: “My typical work process in a technical challenge like this is to spend a minute or two thinking quietly about the problem and writing down notes, I’ll share those thoughts with you in a moment to get your input. Then, while I code, I tend to work quietly as well, but I’ll be sure to pause now and then to share my thought process as I go, and then walk you through the code more thoroughly before we run it the first time. Would that be okay with you, or do you have different expectations of how you’d like me to communicate or work through the problem?”**

I promise you’ll blow their mind. Most interviewers won’t be ready for you to take their expectations into consideration like this. It shows that you’ll work well on a team. You’re setting the environment where you’re advocating for yourself, but also being considerate of others. You’re stating your intentions up front, and giving them the opportunity to collaborate on the process.

### Common Problem Area 5: Not asking for help sooner

As your interviewer, I have a small amount of help that I’m likely able to provide during a technical challenge. I can’t coach you through everything, obviously, but I’d rather give you a hint, deduct a point on a rubric, and see you ultimately succeed at the problem, than to struggle silently and spin in circles and make us both feel like the interview is a waste of time.

As a professional interviewer and an instructor at a software school, I’ve become pretty good at asking leading questions to guide you to a realization or answer without me giving you the solution.

**It’s okay to admit when you’re stuck. It doesn’t make you a failure, it makes you human. Let your interviewer know what you’re thinking and where you’re having problems.** Listen very carefully to their response, they might be offering a clue to the problem, or might give you more thorough advice on how to proceed.

## My Favorite Resources to Share

When our interviews at IIO are over, I like to dive into a lot of feedback on their process and where I think they could use extra practice to improve. Generally, I spend 10 to 20 minutes, sometimes going way beyond my one-hour expected time slot, to answer questions for someone, and going into more detail on things. I **LOVE** to help people on IIO.

Here are a few common areas of advice I offer to folks.

There’s nothing worse than listening to your own recorded voice. But all IIO interviews are recorded, and I often tell folks in my feedback and in the review notes I type up afterward to listen to the last few minutes of the interview recording to review the feedback I give them. You can also pause those recordings and grab a copy of your code at any time. (These recordings are of course private to your and your interviewer.)

**During the playback, listen to your own thought process and how you communicate your ideas. As you work through other challenges, find a way to record yourself talking through the problem out loud if possible, and play that back for yourself. You’ll get better at articulating full and complete thoughts.**

### Problem Solving and Mid-Level Design

The more common practice sites like HackerRank, CodeWars, LeetCode, etc, are great for writing a coded algorithm, but don’t give you any way to exercise your design process.

I send my students to [Project Euler](https://projecteuler.net). **Euler was a mathematician, so the problems on the website will generally be pretty math-heavy, but you can change the problems to be whatever you’re comfortable building. If you don’t know how to calculate a prime number, that’s fine, swap that out for whether a number is equally divisible by 17 or something instead.**

I like Project Euler because the challenges there are just word problems. You have to think of everything: the algorithm, which data structure(s) to use, and especially how to break the problem into smaller pieces.

One of my favorite problems is #19 in their archive: counting how many months between January 1901 and December 1999 began on a Sunday. They give you the number of days in each calendar month, tell you January 1st 1900 is a Monday, and how to calculate a leap year. The rest is up to you.

The more you expose yourself to different types of problems, the better you’ll get at spotting patterns.

### Practice, Practice, Practice

One piece of advice we give our students is to practice each technical challenge several times. Our executive director, Jeff Casimir, tells students to practice something 10 times. That feels like a big effort. I aim more for 3 to 4 times, and here’s my reasoning:

The first time you solve a problem, all you’ve done is solve the problem. You might have struggled through certain parts, but your only real achievement here is finishing.

If you erase your work and start it a second time, you might think of a different approach to solving the problem, maybe a more efficient solution. Maybe not, but at least you’re getting practice with this kind of problem.

Now erase your work and do it a third time. Then a fourth time. These are the times when you will start to actively build a memory of the strategy it takes to solve this particular problem. This “muscle memory” will help you when you see other technical challenges, where you’ll start to spot similarities. “Oh, this looks like the knapsack problem” and because you’ve solved that several times, the time you take on high level design and mid-level design just shortened quite a lot.

One of my favorite technical challenges can be solved using a handful of different algorithms (DFS, BFS, DP, etc). If you think you can solve a problem in a similar fashion, solve it 3 or 4 times with each of those algorithms as well. You’ll get REALLY good at spotting similarities, and have a great collection of strategies to approach other technical problems.

## Shameless Self-Promotion

I’ve been writing up notes for aspiring new developers at <https://techinterview.guide>. It’s not complete, but I have a lot of my own thoughts on preparing for technical interviews, networking and outreach, resumes and cover letters, and so on. I still have a few chapters to write about negotiation tactics and graceful resignations, but I’m happy to take feedback from others on the content.

I also have a daily email series covering several kinds of interview questions, but not from a perspective of how to answer the questions perfectly, there are plenty of resources out there to do that. Instead, I examine questions from an interviewer’s perspective — what am I really asking, what do I hope you’ll tell me, what do I hope you won’t say, and so on. A small preview, for example: when you’re asked “Tell me about yourself” they’re not really asking for your life story. They’re really asking “Tell me a summary of things about you that will make you a valuable employee here”.

### [Permutation in String](/questions/permutation-in-string)

[Given two strings s1 and s2, return true if s2 contains a permutation of s1, or false otherwise.](/questions/permutation-in-string)


# [The Eng Hiring Bar: What the hell is it?](https://interviewing.io/blog/the-eng-hiring-bar-what-the-hell-is-it)

By Atomic Artichoke | Published: March 30, 2020; Last updated: July 14, 2023

Recursive Cactus has been working as a full-stack engineer at a well-known tech company for the past 5 years, but he’s now considering a career move.

Over the past 6 months, Recursive Cactus (that’s his anonymous handle on [interviewing.io](https://interviewing.io/signup)) has been preparing himself to succeed in future interviews, dedicating as much as 20-30 hours/week plowing through LeetCode exercises, digesting algorithms textbooks, and of course, practicing interviews on our platform to benchmark his progress.

**Recursive Cactus’s typical weekday schedule**

| **Time** | **Activity** |
| --- | --- |
| 6:30am – 7:00am | Wake up |
| 7:00am – 7:30am | Meditate |
| 7:30am – 9:30am | Practice algorithmic questions |
| 9:30am – 10:00am | Commute to work |
| 10:00am – 6:30pm | Work |
| 6:30pm – 7:00pm | Commute from work |
| 7:00pm – 7:30pm | Hang out with wife |
| 7:30pm – 8:00pm | Meditate |
| 8:00pm – 10:00pm | Practice algorithmic questions |

**Recursive Cactus’s typical weekend schedule**

| **Time** | **Activity** |
| --- | --- |
| 8:00am – 10:00am | Practice algorithmic questions |
| 10:00am – 12:00pm | Gym |
| 12:00pm – 2:00pm | Free time |
| 2:00pm – 4:00pm | Practice algorithmic questions |
| 4:00pm – 7:00pm | Dinner with wife & friends |
| 7:00pm – 9:00pm | Practice algorithmic questions |

But this dedication to interview prep has been taking an emotional toll on him, his friends, and his family. Study time crowds out personal time, to the point where he basically has no life beyond work and interview prep.

“It keeps me up at night: what if I get zero offers? What if I spent all this time, and it was all for naught?”

We’ve all been through the job search, and many of us have found ourselves in a similar emotional state. But why is Recursive Cactus investing so much time, and what’s the source of this frustration?

He feels he can’t meet the engineer hiring bar (aka “The Bar”), that generally accepted minimum level of competency that all engineers must exhibit to get hired.

To meet “The Bar,” he’s chosen a specific tactic: to look like the engineer that people want, rather than just be the engineer that he is.

It seems silly to purposefully pretend to be someone you’re not. But if we want to understand why Recursive Cactus acts the way he does, it helps to know what “The Bar” actually is. And when you think a little more about it, you realize there’s not such a clear definition.

## Defining “The Bar”

What if we look at how the [FAANG companies (Facebook, Amazon, Apple, Netflix, Google)](https://interviewing.io/blog/how-know-ready-interview-faang) define “The Bar?” After all, it seems those companies receive the most attention from pretty much everybody, job seekers included.

Few of them disclose specific details about their hiring process. Apple doesn’t publicly share any information. [Facebook](https://www.facebookcareers.com/facebook-life/how-we-hire/?page=1) describes the stages of their interview process, but not their assessment criteria. [Netflix](https://jobs.netflix.com/culture) and [Amazon](https://www.amazon.jobs/en/landing_pages/assessments) both say they hire candidates that fit their work culture/leadership principles. Neither Netflix nor Amazon describes exactly how they assess against their respective principles. However, Amazon does share how interviews get conducted as well as software [topics that could be discussed for software developer](https://www.amazon.jobs/en/landing_pages/software-development-topics) positions.

The most transparent of all FAANGs, [Google publicly discloses](https://careers.google.com/how-we-hire/interview/) its interview process with the most detail, with Laszlo Bock’s book [*Work Rules!*](https://www.amazon.com/Work-Rules-Insights-Inside-Transform/dp/1455554790) adding even more insider color about how their interview process came to be.

And speaking of tech titans and the recent past, Aline (our founder) mentioned the 2003 book [*How Would You Move Mount Fuji?*](https://www.amazon.com/How-Would-Move-Mount-Fuji-ebook/dp/B000Q67H6I) in a prior [blog post,](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview) which recounted Microsoft’s interview process when they were the pre-eminent tech behemoth of the time.

In order to get a few more data points about how companies assess candidates, I also looked at Gayle Laakmann McDowell’s “Cracking the Coding Interview”, which is effectively the Bible of interviewing for prospective candidates, as well as Joel Spolsky’s [Guerilla Guide to Interviewing 3.0](https://www.joelonsoftware.com/2006/10/25/the-guerrilla-guide-to-interviewing-version-30/), written by an influential and well-known figure within tech circles over the past 20-30 years.

### Definitions of “The Bar”

| **Source** | **Assessment Criteria** |
| --- | --- |
| Apple | Not publicly shared |
| [Amazon](https://www.amazon.jobs/en/landing_pages/assessments) | Assessed against [Amazon’s Leadership principles](https://interviewing.io/guides/amazon-leadership-principles) |
| Facebook | Not publicly shared |
| Netflix | Not publicly shared |
| [Google](https://careers.google.com/how-we-hire/interview/#onsite-interviews) | 1. General cognitive ability 2. Leadership 3. “Googleyness” 4. Role-related knowledge |
| [*Cracking the Coding Interview*](https://www.crackingthecodinginterview.com/) – Gayle Laakmann McDowell | – Analytical skills – Coding skills – Technical knowledge/computer science fundamentals – Experience – Culture fit |
| [Joel Spolsky](https://www.joelonsoftware.com/2006/10/25/the-guerrilla-guide-to-interviewing-version-30/) | – Be smart – Get things done |
| [Microsoft](https://www.amazon.com/How-Would-Move-Mount-Fuji-ebook/dp/B000Q67H6I) (circa 2003) | – “The goal of Microsoft’s interviews is to assess a general problem-solving ability rather than a specific competency.” – “Bandwidth, inventiveness, creative problem-solving ability, outside-the-box thinking” – “Hire for what people can do rather than what they’ve done” – Motivation |

## Defining “Intelligence”

It’s not surprising that coding and technical knowledge would be part of any company’s software developer criteria. After all, that is the job.

But beyond that, the most common criteria shared among all these entities is a concept of intelligence. Though they use different words and define the terms slightly differently, all point to some notion of what psychologies call “cognitive ability.”

| **Source** | **Definition of cognitive ability** |
| --- | --- |
| Google | *“General Cognitive Ability. Not surprisingly, we want smart people who can learn and adapt to new situations. Remember that this is about understanding how candidates have solved hard problems in real life and how they learn, not checking GPAs and SATs.”* |
| Microsoft (circa 2003) | *“The goal of Microsoft’s interviews is to assess a general problem-solving ability rather than a specific competency… It is rarely dear what type of reasoning is required or what the precise limits of the problem are. The solver must nonetheless persist until it is possible to bring the analysis to a timely and successful conclusion.”* |
| Joel Spolsky | *“For some reason most people seem to be born without the part of the brain that understands pointers.”* |
| Gayle Laakmann McDowell | *“If you’re able to work through several hard problems (with some help, perhaps), you’re probably pretty good at developing optimal algorithms. You’re smart.”* |

All these definitions of intelligence resemble early 19th-century psychologist [Charles Spearman’s theory of intelligence](https://en.wikipedia.org/wiki/Two-factor_theory_of_intelligence), the most widely acknowledged framework for intelligence. After performing a series of cognitive tests on school children, Spearman found that those who did well in one type of test tended to also perform well in other tests. This insight led Spearman to theorize that there exists a single underlying general ability factor (called “g” or “g factor”) influencing all performance, separate from specific, task-specific abilities (named “s”).

If you believe in the existence of “g” (many do, some do not… there exist [different theories of intelligence](https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences)), finding candidates with high measures of “g” aligns neatly with the intelligence criteria companies look for.

While criteria like leadership and culture fit matter to companies, “The Bar” is not usually defined in those terms. **“The Bar” is defined as having technical skills but also (and perhaps more so) having general intelligence.** After all, candidates aren’t typically coming to interviewing.io to specifically practice leadership and culture fit.

The question then becomes how you measure these two things. Measuring technical skills seems tough but doable, but how do you measure “g?”

## Measuring general intelligence

Mentioned in Bock’s book, Frank Schmidt’s and John Hunter’s 1998 paper “[The Validity and Utility of Selection Methods in Personnel Psychology](https://www.researchgate.net/publication/232564809_The_Validity_and_Utility_of_Selection_Methods_in_Personnel_Psychology)” attempted to answer this question by analyzing a diverse set of 19 candidate selection criteria to see which predicted future job performance the best. The authors concluded general mental ability (GMA) best predicted job performance based on a statistic called “predictive validity.”

![Table showing candidate selection criteria and validity in predicting job performance](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F4_Table1_1_bd3ec75215.png&w=1920&q=75 "Candidate selection criteria and validity in predicting job performance")

In this study, a GMA test referred to an IQ test. But for Microsoft circa 2003, puzzle questions like “How many piano tuners are there in the world?” appear to have taken the place of IQ tests for measuring “g”. Their reasoning:

> *“At Microsoft, and now at many other companies, it is believed that there are parallels between the reasoning used to solve puzzles and the thought processes involved in solving the real problems of innovation and a changing marketplace. Both the solver of a puzzle and a technical innovator must be able to identify essential elements in a situation that is initially ill-defined.”*
>
> – “How Would You Move Mount Fuji?” – page 20

Fast forward to today, Google [denounces this practice](https://rework.withgoogle.com/guides/hiring-use-structured-interviewing/steps/avoid-brainteasers/), concluding that “performance on these kinds of questions is at best a discrete skill that can be improved through practice, eliminating their utility for assessing candidates.”

So here we have two companies who test for general intelligence, but who also fundamentally disagree on how to measure it.

## Are we measuring specific or general intelligence?

But maybe as Spolsky and McDowell have argued, the traditional algorithmic and computer science-based interview questions are themselves effective tests for general intelligence. Hunter & Schmidt’s study contains some data points that could support this line of reasoning. Among all single-criteria assessment tools, work sample tests possessed the highest predictive validity. Additionally, when observing the highest validity regression result of two-criteria assessment tool (GMA test plus work sample test), the standardized effect size on the work sample rating was larger than that of the GMA rating, suggesting a stronger relationship with future job performance.

If you believe algorithmic exercises function as work sample tests in interviews, then the study suggests traditional algorithm-based interviews could predict future job performance, maybe even more than a GMA/IQ test.

Recursive Cactus doesn’t believe there’s a connection.

> *There’s little overlap between the knowledge acquired on the job and knowledge about solving algorithmic questions. Most engineers rarely work with graph algorithms or dynamic programming. In application programming, lists and dictionary-like objects are the most common data structures. However, interview questions involving those are often seen as trivial, hence the focus on other categories of problems.*
>
> – Recursive Cactus

In his view, algorithms questions are similar to Microsoft’s puzzle questions: you learn how to get good at solving interview problems, which to him don’t ever show up in actual day-to-day work, which, if true, wouldn’t actually fit with the Schmidt & Hunter study.

Despite Recursive Cactus’s personal beliefs, interviewers like Spolsky still believe these skills are vital to being a productive programmer.

> *A lot of programmers that you might interview these days are apt to consider recursion, pointers, and even data structures to be a silly implementation detail which has been abstracted away by today’s many happy programming languages. “When was the last time you had to write a sorting algorithm?” they snicker.*
>
> *Still, I don’t really care. I want my ER doctor to understand anatomy, even if all she has to do is put the computerized defibrillator nodes on my chest and push the big red button, and I want programmers to know programming down to the CPU level, even if Ruby on Rails does read your mind and build a complete Web 2.0 social collaborative networking site for you with three clicks of the mouse.*
>
> – Joel Spolsky

Spolsky seems to concede that traditional tech interview questions might not mimic actual work problems, and therefore wouldn’t act as work samples. Rather, it seems he’s testing for general computer science aptitude, which is general in a way, but specific in other ways. General intelligence within a specific domain, one might say.

That is, unless you believe intelligence in computer science *is* general intelligence. McDowell suggests this:

> *There’s another reason why data structure and algorithm knowledge comes up: because it’s hard to ask problem-solving questions that don’t involve them. It turns out that the vast majority of problem-solving questions involve some of these basics.*
>
> – Gayle Laakmann McDowell

This could be true assuming you view the world primarily through computer science lenses. Still, it seems pretty restrictive to suggest people who don’t speak the language of computer science would have more difficulty solving problems.

At this point, we’re not really talking about measuring general intelligence as Spearman originally defined it. Rather, it seems we’re talking about specific intelligence, defined or propagated by those grown or involved in traditional computer science programs, and conflating that with general intelligence (Spolsky, McDowell, Microsoft’s Bill Gates, and 4 of 5 FAANG founders studied computer science at either some Ivy League university or Stanford).

Maybe when we’re talking about “The Bar,” we’re really talking about something subjective, based on whoever is doing the measuring, and whose definition might not be consistent from person-to-person.

Looking at candidate assessment behavior from interviewers on the interviewing.io platform, you can find some evidence that supports this hypothesis.

## “The Bar” is subjective

On the interviewing.io platform, people can practice technical interviews online and anonymously, with interviewers from top companies on the other side. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from companies like Google, Facebook, Dropbox, Airbnb, and more. Check out our [interview showcase](https://interviewing.io/mocks) to see how this all looks and to watch people get interviewed. After every interview, interviewers rate interviewees on a few different dimensions: technical skills, communication skills, and problem-solving skills. Each dimension gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. You can see what our feedback form looks like below:

![Screenshot of the Interviewing.io interview feedback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_1ca42f5eec.png&w=750&q=75 "Interviewing.io interview feedback form")

If you do well in practice, you can bypass applying online/getting referrals/talking to recruiters and instead immediately book real technical interviews directly with our partner companies (more on that in a moment).

When observing our most frequent practice interviewers, we noticed differences across interviewers in the percent of candidates that person would hire, which we call the passthrough rate. Passthrough rates ranged anywhere between 30% and 60%. At first glance, certain interviewers seemed to be a lot stricter than others.

![Chart showing passthrough rates by interviewer](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F08ff4_blog_post_passthrough_rates_by_interviewer_3488f7303d.png&w=1080&q=75 "Passhrough Rates by Interviewer")

Because interviewees and interviewers are anonymized and matched randomly[^1], we wouldn’t expect the quality of candidates to vary much across interviewers, and as a result, wouldn’t expect interviewee quality to explain the difference. Yet even after accounting for candidate attributes like experience level, differences in passthrough rates persist.[^2]

Maybe some interviewers choose to be strict on purpose because their bar for quality is higher. While it’s true that candidates who practiced with stricter interviewers tended to receive lower ratings, they also tended to perform better on their next practice.

This result could be interpreted in a couple of ways:

- Stricter interviewers might systematically underrate candidates
- Candidates get so beat up by strict interviewers that they tended to improve more between practices, striving to meet their original interviewer’s higher bar

If the latter were true, you would expect that candidates who practiced with stricter interviewers would perform better in real company interviews. However, we did not find a correlation between interviewer strictness and future company interview passthrough rate, based on real company interviews conducted on our platform.[^3]

![Chart showing interviewer's passthrough rate vs. interviewee's future performance](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffb594_blog_post_practice_vs_company_passthrough_ea25d930f7.png&w=1080&q=75 "Interviewer's Passthrough Rate vs. Interviewee's Future Performance")

Interviewers on our platform represent the kinds of people a candidate would encounter in a real company interview, since those same people also conduct phone screens and onsites at the tech companies you’re all applying to today. And because we don’t dictate how interviewers conduct their interviews, these graphs could be describing the distribution of opinions about your interview performance once you hang up the phone or leave the building.

This suggests that, independent of your actual performance, **whom you interview with could affect your chance of getting hired**. In other words, “The Bar” is subjective.

This variability across interviewers led us to reconsider our own internal definition of “The Bar,” which determined which candidates were allowed to interview with our partner companies. Our definition strongly resembled Spolsky’s binary criteria (“be smart”), heavily weighing an interviewer’s Would Hire opinion way more than our other 3 criteria, leading to the bimodal, camel-humped distribution below.

![Chart showing Interviewing.io's Old Internal Score Distribution](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F09df4_blog_post_old_score_distribution_af49004df0.png&w=1080&q=75 "Interviewing.io's Old Internal Score Distribution")

While our existing scoring system correlated decently with future interview performance, we found that an interviewer’s Would Hire rating wasn’t as strongly associated with future performance as our other criteria were. We lessened the weight on the Would Hire rating, which in turn improved our predictive accuracy.[^4] Just like in *“Talledega Nights”* when [Ricky Bobby learned there existed places other than first place and last place in a race](https://www.youtube.com/watch?v=20iio0wLpPA), we learned that it was more beneficial to think beyond the binary construct of “hire” vs. “not hire,” or if you prefer, “smart” vs. “not smart.”

Of course, we didn’t get rid of all the subjectivity, since those other criteria were also chosen by the interviewer. And this is what makes assessment hard: an interviewer’s assessment is itself the measure of candidate ability.

If that measurement isn’t anchored to a standard definition (like we hope general intelligence would be), then the accuracy of any given measurement becomes less certain. It’s as if interviewers used measuring sticks of differing lengths, but all believed their own stick represented the same length, say 1 meter.

When we talked to our interviewers to understand how they assessed candidates, it became even more believable that different people might be using measuring sticks of differing lengths. Here are some example methods of how interviewers rated candidates:

- Ask 2 questions. Pass if answer both
- Ask questions of varying difficulty (easy, medium, hard). Pass if answers a medium
- Speed of execution matters a lot, pass if answers “fast” (“fast” not clearly defined)
- Speed doesn’t matter much, pass if have a working solution
- Candidates start with full points. When candidates make mistakes, start docking points

Having different assessment criteria isn’t necessarily a bad thing (and actually seems totally normal). It just introduces more variance to our measurements, meaning our candidates’ assessments might not be totally accurate.

The problem is, when people talk about “The Bar,” that uncertainty around measurement usually gets ignored.

You’ll commonly see people advising you only to hire the highest quality people.

> *A good rule of thumb is to hire only people who are better than you. Do not compromise. Ever.*
>
> – Laszlo Bock

> *Don’t lower your standards no matter how hard it seems to find those great candidates.*
>
> – Joel Spolsky

> *In the Macintosh Division, we had a saying, “A player hire A players; B players hire C players”–meaning that great people hire great people.*
>
> – Guy Kawasaki

> *Every person hired should be better than 50 percent of those currently in similar roles – that’s raising the bar.*
>
> – Amazon Bar Raiser blog post

All of this is good advice, assuming “quality” could be measured reliably, which as we’ve seen so far, isn’t necessarily the case.

Even when uncertainty does get mentioned, that variance gets attributed to the candidate’s ability, rather than the measurement process or the person doing the measuring.

> *[I]n the middle, you have a large number of “maybes” who seem like they might just be able to contribute something. The trick is telling the difference between the superstars and the maybes, because the secret is that you don’t want to hire any of the maybes. Ever.*
>
> …
>
> *If you’re having trouble deciding, there’s a very simple solution. NO HIRE. Just don’t hire people that you aren’t sure about.*
>
> – Joel Spolsky

Assessing candidates isn’t a fully deterministic process, yet we talk about it like it is.

## Why **“The Bar” is so high**

“Compromising on quality” isn’t really about compromise, it’s actually about decision-making in the face of uncertainty. And as you see from the quotes above, the conventional strategy is to only hire when certain.

No matter what kind of measuring stick you use, this leads to “The Bar” being set really high. Being really certain about a candidate means minimizing the possibility of making a bad hire (aka “false positives”). And companies will do whatever they can to avoid that.

> *A bad candidate will cost a lot of money and effort and waste other people’s time fixing all their bugs. Firing someone you hired by mistake can take months and be nightmarishly difficult, especially if they decide to be litigious about it.*
>
> – Joel Spolsky

Hunter and Schmidt quantified the cost of a bad hire: “The standard deviation… has been found to be at minimum 40% of the mean salary,” which in today’s terms would translate to $40,000 assuming a mean engineer salary of $100,000/year.

But if you set “The Bar” too high, chances are you’ll also miss out on some good candidates (aka “false negatives”). McDowell explains why companies don’t really mind a lot of false negatives:

> *“From the company’s perspective, it’s actually acceptable that some good candidates are rejected… They can accept that they miss out on some good people. They’d prefer not to, of course, as it raises their recruiting costs. It is an acceptable tradeoff, though, provided they can still hire enough good people.”*

In other words, it’s worth holding out for a better candidate if the difference in their expected output is large, relative to the recruiting costs from continued searching. Additionally, the costs of HR or legal issues downstream from potentially problematic employees also tilt the calculation toward keeping “The Bar” high.

This is a very rational cost-benefit calculation. But has anyone ever done this calculation before? If you have done it, we’d love to hear from you. Otherwise, it seems difficult to do.

Given that nearly everyone is using hand-wavy math, if we do the same, maybe we can convince ourselves that “The Bar” doesn’t have to be set quite so high.

As mentioned before, the distribution of candidate ability might not be so binary, so Spolsky’s nightmare bad hire scenario wouldn’t necessarily happen with all “bad” hires, meaning the expected difference in output between “good” and “bad” employees might be lower than perceived.

Recruiting costs might be higher than perceived because finding and employing +1 standard deviation employees gets increasingly difficult. By definition, fewer of those people exist as your bar rises. Schmidt and Hunter’s “bad hire” calculation only compares candidates within an applicant pool. The study does not consider the relative cost of getting high-quality candidates into the applicant pool to begin with, which tends to be the more significant concern for many of today’s tech recruiting teams. And when you consider that other tech companies might be employing the same hiring strategy, competition would increase the average probability that offers get rejected, extending the time to fill a job opening.

Estimating the expected cost of HR involvement is also difficult. No one wants to find themselves interacting with HR. But then again, not all HR teams are as useless as [Toby Flenderson](https://theoffice.fandom.com/wiki/Toby_Flenderson).

Taken together, if the expected output between “good” and “bad” candidates were less than expected, and the recruiting costs were higher than perceived, it would make less sense to wait for a no-brainer hire, meaning “The Bar” might not have to be set so high.

Even if one does hire an underperformer, companies could adopt the tools of training and employee management to mitigate the negative effects from some disappointing hires. After all, people can and do become more productive over time as they acquire new skills and knowledge.

Employee development seems to rarely get mentioned in conjunction with hiring (Laszlo Bock makes a few connections here and there, but the topics are mostly discussed separated). But when you add employee development into the equation above, you start to see the relationship between hiring employees and developing employees. You can think of it as different methods for acquiring more company output from different kinds of people: paying to train existing employees versus paying to recruit new employees.

You can even think of it as a trade-off. Instead of developing employees in-house, why not outsource that development? Let others figure out how to develop the raw talent, and later pay recruiters to find them when they get good. Why shop the produce aisle at Whole Foods and cook at home when you can just pay Caviar to deliver pad thai to your doorstep? Why spend time managing and mentoring others when you can spend that time doing “real work” (i.e. engineering tasks)?

Perhaps “The Bar” is set high because companies don’t develop employees effectively, which puts more pressure on the hiring side of the company to yield productive employees.

Therefore, companies can lower their risk by shifting the burden of career development onto the candidates themselves. In response, candidates like Recursive Cactus have little choice but to train themselves.

Initially, I thought Recursive Cactus was a crazy outlier in terms of interview preparation. But apparently, he’s not alone.

## Candidates are training themselves

Last year we surveyed our candidates about how many hours they spent preparing for interviews. Nearly half of the respondents reported spending 100 hours or more on interview preparation.[^5]

![Chart showing responses to the survey question: how many hours spent preparing for interviews](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fb81df_blog_post_survey_hours_65812fae4a.png&w=1080&q=75 "Survey: Hours Spent Preparing For Interviews")

We wondered whether hiring managers and recruiters had the same expectations for candidates they encounter, Aline asked a similar question on Twitter, and results suggest they vastly underestimate the work and effort candidates endure prior to meeting with a company.

> Are you a hiring manager or recruiter? If so, please vote on my poll! Results will be shared in an upcoming, titillating blog post.  
>   
> How many dedicated hours of interview prep do you believe candidates should invest in their job search?
>
> — Aline Lerner (@alinelernerLLC) [July 3, 2019](https://twitter.com/alinelernerLLC/status/1146468041212907520?ref_src=twsrc%5Etfw)

Decision makers clearly underestimate the amount of work candidates put into job hunt preparation. The discrepancy seems to reinforce the underlying and unstated message pervading all these choices around how we hire: If you’re not one of the smart ones (whatever that means), it’s not our problem. You’re on your own.

## “The Bar” revisited

So this is what “The Bar” is. “The Bar” is a high standard set by companies in order to avoid false positives. It’s not clear whether companies have actually done the appropriate cost-benefit analysis when setting it, and it’s possible it can be explained by an aversion to investing in employee development.

“The Bar” is in large part meant to measure your general intelligence, but the actual instruments of measurement don’t necessarily follow the academic literature that underlies it. You can even quibble about the academic literature.[^6] “The Bar” does measure specific intelligence in computer science, but that measurement might vary depending on who conducts your interview.

Despite the variance that exists across many aspects of the hiring process, we talk about “The Bar” as if it were deterministic. This allows hiring managers to make clear binary choices but discourages them to think critically about whether their team’s definition of “The Bar” could be improved.

And that helps us understand why Recursive Cactus spends so much time practicing. He’s training himself partially because his current company isn’t developing his skills. He’s preparing for the universe of possible questions and interviewers he might encounter because hiring criteria varies a lot, which cover topics that won’t necessarily be used in his day-to-day work, all so he can resemble someone that’s part of the “smart” crowd.

That’s the system he’s working within. Because the system is the way it is, it’s had significant impact on his personal life.

> *My wife’s said on more than one occasion that she misses me. I’ve got a rich happy life, but I don’t feel I can be competitive unless I put everything else on hold for months. No single mom can be doing what I’m doing right now.*
>
> – Recursive Cactus

This impacts his current co-workers too, whom he cares about a lot.

> *This process is sufficiently demanding that I’m no longer operating at 100% at work. I want to do the best job at work, but I don’t feel I can do the right thing for my future by practicing algorithms 4 hours a day *and* do my job well.*
>
> *I don’t feel comfortable being bad at my job. I like my teammates. I feel a sense of responsibility. I know I won’t get fired if I mail it in, but I know that it’s them that pick up the slack.*
>
> – Recursive Cactus

It’s helpful to remember that all the micro decisions made around false positives, interview structure, brain teasers, hiring criteria, and employee development add up to define a system that, at the end of the day, impacts people’s personal lives. Not just the lives of the job hunters themselves, but also all the people that surround them.

Hiring is nowhere near a solved problem. Even if we do solve it somehow, it’s not clear we would ever eliminate all that uncertainty. After all, projecting a person’s future work output after spending an hour or two with them in an artificial work setting seems kinda hard. While we should definitely try to minimize uncertainty, it might be helpful to accept it as a natural part of the process.

This system can be improved. Doing so requires not only coming up with new ideas, but also revisiting decades-old ideas and assumptions, and expanding upon that prior work rather than anchoring ourselves to it.

We’re confident that all of you people in the tech industry will help make tech hiring better. We know you can do it, because after all, you’re smart.

### [Number of Subarrays with Bounded Maximum](/questions/number-of-subarrays-with-bounded-maximum)

[Given an integer array nums and two integers left and right, return the number of contiguous non-empty subarrays such that the value of the maximum array element in that subarray is in the range [left, right].](/questions/number-of-subarrays-with-bounded-maximum)


# [There is a real connection between technical interview performance and salary. Here’s the data.](https://interviewing.io/blog/there-is-a-real-connection-between-technical-interview-performance-and-salary-heres-the-data)

By Aline Lerner | Published: February 25, 2019; Last updated: May 1, 2023

At the end of the day, money is a huge driver for the decisions we make about what jobs to go after. In the past, we’ve written about [how to negotiate your salary](https://interviewing.io/blog/negotiate-salary-recruiter), and there are a lot of [labor statistics](https://www.bls.gov/opub/btn/volume-7/high-tech-industries-an-analysis-of-employment-wages-and-output.htm) and reports out there looking at salaries in the tech industry as a whole. But as with many things in eng hiring, there’s very little concrete data on whether technical interview performance plays a role in compensation offers.

So we set out to gather the data and asked our users who had gone on to successfully get jobs after using our platform to share their salary info. **With our unique dataset of real coding interviews, we could ask questions like:**

- **Does interview performance matter when it comes to compensation packages?**
- **Do engineers who prioritize other parts of a role over compensation (e.g. values alignment) end up with lower salaries?**
- **What else seems to matter in getting a higher salary?**

To be clear, this is an exploration of past average interview performance and its connection with current salary, versus looking at how someone did in an interview and then what salary they got when they took that specific job. In other words, we haven’t paired job interviews with the salary for that same job. We believe that looking at these more general measures is more informative than trying to match single interviews and job offers, given [how volatile individual interview performance can be](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary). But our interviewing platform allowed us to look at performance across multiple interviews for respondents, which gave us more stability and more data.

On the interviewing.io platform, people can practice technical interviews online and anonymously, with real engineers on the other side.

After every interview, interviewers rate interviewees on a few different dimensions: technical skills, communication skills, and problem solving skills. These each get rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!”. On our platform, a score of 3 or above has generally meant that the person was good enough to move forward. You can see what our feedback form looks like below:

![Screenshot showing Interviewing.io interview feedback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6f86b_screenshot_2017_11_29_09_10_46_6a2be62174.webp&w=1920&q=75 "Interviewing.io interview feedback form")

With this in mind, we surveyed interviewing.io users about their current roles, including salary, bonuses, and how satisfied they felt in their job and then tied their comp back to how they did in interviews on our platform. We ended up with responses from 494 engineers[1](#user-content-fn-1), and because compensation packages are so complex and vary from company to company, we analyzed the data in several different ways, looking at annual salary numbers, bonuses, and equity. **Then we tied compensation data to performance in technical interviews to see whether it matters, and if so, how much.**

We looked at the relationships between interview performance (technical skills, communication ability, and problem solving ability) and the following: base salary, bonuses, and equity. In all cases, we corrected for location (being in the Bay Area means a higher salary) and experience (senior engineers make senior salaries), and where we could, we corrected for company size (bigger companies can generally pay bigger salaries).

The mean yearly salary for all survey participants was around $130k, and 57% of them reported a yearly bonus. For that group, the average yearly bonus was $20k. For people who reported a dollar amount for equity, the average was $54k. Below is the distribution of experience level/seniority of survey respondents.

Here’s what we found.

### Better technical skills correlate with higher compensation

As probably comes as no surprise, **people who score higher on technical skills during interviews do make more money**. First, let’s look at base salary.[2](#user-content-fn-2)

Bonuses, too, correlate with technical skills, with an additional point in performance potentially worth about 10k[3](#user-content-fn-3):

### The relationship between compensation and other interviewing skills

We also looked at the two other ratings that interviewers give after interviews: communication and problem solving. **Better communication scores had a small but statistically significant correlation with salaries** (r = .15, p < .01), but we found no significant relationship for problem solving scores in isolation:

We also didn’t see a relationship between either communication ability or problem solving ability when it came to bonuses.

The non-relationships didn’t surprise us too much, to be honest, because with a relatively small sample size it’s notoriously difficult to get subcomponents of ratings to show a relationship to something distal and complicated like salaries. It’s very possible these relationships do exist, and with many determiners besides actual interview performance, like seniority and market salary norms, we’d like to repeat this survey at a bigger scale to inform this question.

## What else?

We asked engineers whether they felt satisfied with their role, and found that engineers who felt satisfied earned an average $14k more than engineers who felt dissatisfied.[4](#user-content-fn-4)

We also looked at people’s perceptions of their own performance. [In a previous post](https://interviewing.io/blog/impostor-syndrome-strikes-men-just-as-hard-as-women), we explored how people rated their own technical performance after an interview compared to how the interviewer rated them and found that even experienced engineers aren’t great at guessing how they did. **For this project, we were curious about whether overconfident engineers might net higher salaries (perhaps they negotiate harder!). So we also looked at people who rated their performance higher than their actual interview score — but found no difference in their compensation packages.**

Another thing we were curious about was whether people who valued money over other factors while making a job decision would have higher salaries. So we asked people to rank the most important variables in their job decisions. 32% of respondents said that a compensation package was the most important part of their decision; the next highest response was “matches my interests and values”. But these question didn’t have any predictive value for the actual salary amount: **people who said money matters the most didn’t have significantly different salaries from people who say money matters the least**. It’s possible that with salaries being impacted by so many outside factors, like location and role type, candidates don’t truly have a lot of negotiating power over that salary number.

We looked at equity as well, and the average reported equity package size was 54k. We did not find any significant association between interview performance and reported equity packages. That said, enormous amounts of research have documented various salary gaps based on gender, race, and other important sociocultural and demographic factors, and we hope to repeat this analysis when we have more data.

## What do these findings mean for you?

Interview performance doesn’t just get you in the door or not: it can have a demonstrable connection to your eventual compensation. For instance, **doing just a point better in your technical interview could be worth 10k or more, and with bonus, it could add 20k to your annual comp**.

Given how much technical interview performance matters, we’d be remiss if we didn’t suggest [signing up for free, anonymous mock interviews](https://interviewing.io/signup) on our platform. So, please go do that.

And, if you’re curious about what [our salary survey](https://interviewingio.wufoo.com/forms/salary-survey) looked like or want to participate and contribute to v2 of this post, please do so too!

1. Our salary survey ended up with 494 respondents, but because some people filled out our survey but had not yet done an interview on our platform, only a subset of folks had both salary data and interview data: N = 234, or 47% of our salary sample. Therefore in all the analyses where we compare salaries to interview performance, it’s only for this subgroup. [↩](#user-content-fnref-1)
2. We ran both a correlation between these factors and a regression to correct for confounding factors like seniority and location. For the correlation, r = .22 and p < 0.001. For the regression, F = 16.06 and p < .001. [↩](#user-content-fnref-2)
3. As with base salary above, we ran both a correlation between these factors and a regression to correct for confounding factors like seniority and location. For the correlation, r = .17 and p < 0.05. For the regression, F = 1.63 and p < .05. [↩](#user-content-fnref-3)
4. Looking at it as a binary, satisfied engineers earned significantly more than non-satisfied engineers in a predictive test, F = 5.2398, p < 0.02, and satisfaction and salary amount are also positively correlated. [↩](#user-content-fnref-4)


# [3 exercises to craft the kind of employer brand that actually makes engineers want to work for you](https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you)

By Aline Lerner | Published: May 13, 2019; Last updated: May 1, 2023

If I’m honest, I’ve wanted to write something about employer brand for a long time. One of the things that really gets my goat is when companies build employer brand by over-indexing on banalities (“look we have a ping pong table!”, “look we’re a startup so you’ll have a huge impact”, etc.) instead of focusing on the narratives that make them special.

Hiring engineers is really hard. It’s hard for tech giants, and it’s hard for small companies… but it’s especially hard for small companies people haven’t quite heard of, and they can use all the help they can get because talking about impact and ping pong tables just doesn’t cut it anymore.

At interviewing.io, making small companies shine is core to our business, and I’ll share some of what we’ve learned about branding as a result. I’ll also walk you through 3 simple exercises you can do to help you craft and distill your employer brand… in a way that highlights what’s actually special about you and will make great engineers excited to learn more.

If I have my druthers, this will be one of three posts on brand. This first one will focus on how to craft your story, the second will show you how to use that story to write great job descriptions, and the final one will focus on how to take the story you’ve created and get it out into the world in front of the kinds of people you’d want to hire.

So, onto the first post!

## What is employer brand, and why does it matter?

Companies like Google have built a great brand, and this brand is largely what makes it possible for them to hire thousands of good engineers every year — when you think of Google (or, more recently, Microsoft!), you might think of an army of competent, first-principles thinkers competently building products people love… or maybe a niche group of engineers working on moonshots that will change the world.

This is employer brand. Put simply, it’s what comes to mind when prospective candidates think about your company. Employer brand encompasses every aspect of your narrative: whether people use/like your product, the problem you’re trying to solve, your founding story, your mission, your culture, and what generally what it’s like to work for you. Put another way, all of these attributes (and others besides!) coalesce into the visceral feeling people get when they imagine working for you.

Brand is the single most important thing for getting candidates in the door — even if you have a stellar personal network, in most cases, that’ll usually only last you until your first 30 hires or so — after that, your networks begin to sort of converge on one another. Even so, despite how important brand is for hiring, building it is one of the psychologically hardest things to do at the beginning of your journey because the opportunity cost of spending your time on ANYTHING is staggering, and it’s really hard to justify writing blog posts and hosting events and speaking at conferences when you have to build product and make individual hires and do 50 kabillion other things.

**But, until you build a brand and get it out in the world, you’re going to be hacking through the jungle with a proverbial machete, making hires one by one, trying to charm each one by telling them your story. And once you have a brand, all of a sudden, sourcing is going to feel really, really different (just like it feels when you’ve found product market fit!).**

Over time, if you continue to tell your own story, you, too, will see how much easier sourcing and hiring can be. So, let’s talk about how to craft the right narrative and then proudly shout it from the rooftops.

## Why interviewing.io knows about branding

I mentioned earlier that a lot of what we do at interviewing.io is help our customers put their best foot forward and present themselves to engineers in a way that’s authentic and compelling. I’ll show you some examples of good copy in a moment, but here’s a bit of our flow to put it in context.  
When engineers do really well in practice, they unlock our jobs portal, where they can see a list of companies and roles, like so:

![d988c-ff3-compressed.webp](https://strapi-iio.s3.us-west-2.amazonaws.com/d988c_ff3_compressed_38c9b5f5ba.webp)

As you can see, companies simply describe who they are and what they do, and top-performing engineers just book real interviews with one click. Because our goal is to remove obstacles from engineers talking to other engineers, we don’t have talent managers or talent advocates or, as they’re often called, recruiters, on staff to talk to our candidates and try to convince them to interview at a certain company. As a result, we often find ourselves coaching companies on how to present themselves, given limited time and space. We do work with quite a few companies whose brands are household names, but a good chunk of our customers are smaller, fast-growing companies. **What’s interesting is that while, on our platform, a household name can have 7X the engagement of a company no one’s heard of, companies no one’s heard of that have exceptional brand narratives aren’t far behind burgeoning unicorns!** (We define engagement as the frequency with which candidates who look at our jobs portal then choose to visit that employer’s page.)

And that’s why having a brand story matters… and why we’re equipped to talk about it at some depth.

## What constitutes brand

Below are some attributes that can make up a brand story. As you look at the list, think about what each of these corresponds to in your company, and then, think about which of these are the most unique to you. For instance, every early-stage startup can say they have a culture characterized by autonomy and the potential for huge impact. It’s become a trope that doesn’t differentiate anyone in any way anymore and is therefore probably not worth emphasizing. On the other hand, if you are solving a problem that a lot of your candidates happen to have or if you use a really cool tech stack that attracts some niche community, that’s really special and worth emphasizing.

- Your product and whether people have heard of it/like it
- Your growth numbers if they’re impressive
- Your tech stack and how flashy and cool it is
- Your founding story and mission… are you working on a problem that people care about personally? If not, are you disrupting some outdated, inefficient way to do things?
- How hard are the problems you’re solving? Both technically and otherwise?
- How elite is your team?
- What is it like to work for you, both with regard to overall culture and then eng culture specifically?[1](#user-content-fn-1)
  - Overall culture:
    - Are you known for kindness/work-life balance? Or grueling hours? (Either can be good depending on whom you want to attract.)
    - What portion of your employees have gone on to found startups?
    - Do you have a lot of organization/structure or are you chaos pirates?
  - Eng culture:
    - Are you more pragmatic/hacker-y vs. academic?
    - Do you subscribe to or actively reject any particular development methodologies (e.g. agile)?
    - Do you tend to roll your own solutions in house or do you try to use 3rd party tools wherever possible?
    - Do you open source parts of your code? Or regularly contribute to open source?

## How to craft your story in 3 easy exercises!

There isn’t a single good formula for what to focus on or highlight when crafting this story, but there are a few exercises that we’ve seen be effective. You can do them in the order below, and by the end, you should have a concise, authentic, pithy narrative that you can shout proudly to the world.

We’ve found that it makes sense to do these exercises in the order below. **First, you’re going to go with your gut and craft a high-level story. Then you’ll embellish it with details that make you unique and with anecdotes from your employees. And finally, you’ll edit it down into something crisp.** As you work, you can use the list from the “What constitutes brand” section above as a reference. Note that, in general, your story plus details shouldn’t have more than 3 distinct ideas total or it’ll start to feel a bit all over the place.

### Exercise 1 – The story itself

Imagine you have a smart, technical friend you respect but who doesn’t know anything about your company’s space. Quick, how would you describe what your company does and why it matters to them? Write it down (and target 5-6 sentences… but don’t worry too much about editing it yet… we’ll do that later).

If you’re feeling a bit stuck, here are some questions to get you started — think about how you might answer if your friend were asking each of these:

- Why does the company exist/what does your product do, and why does that matter?
- Why are you going to succeed where others have failed?
- Why does the company matter to you personally?
- What do you know that no one else does about your space?
- What is your company doing that no one else is doing, and why does that matter?

As you do this exercise, note that when talking to your friend, you dispense with flowery language and explain things succinctly and clearly in simple terms! And that’s the point — the audience you’re selling to is not different than your friend, and your friend probably shares the same cynicism about disingenuous branding that they do!

### Exercise 2 – The unique embellishments

Once you have the story you came up with above, which will likely be at a pretty high level, it’s time to drill down into the details that make you special. These details will likely be 2nd order, in the sense that they won’t be as broad or all-encompassing as the attributes that came to mind in the first exercise, but they might still be special and unique and worth noting.

Some examples of unique embellishments can be:

- Your tech stack
  - Do you use any cutting-edge programming languages that one might not often see being used in production? If so, it might be a bit polarizing but attract the community around that language. More on the value of polarization when it comes to unique embellishments below.
- Unfettered access to some type of user/a specific group you care about that your product impacts
  - Do you build products for Hollywood? Or for VCs? Or for schools? Some portion of your candidates, depending on their interests outside of work or their future career ambitions, are going to be really excited that they’ll get more direct access to users who operate in these worlds.
- Unique lifestyle/work culture stuff like working remotely or pair programming
  - E.g. 37Signals and Pivotal respectively
- Access to a ton of data/ability to work on massively distributed systems
  - E.g. even in its early days, Twitch had almost as much ingest as YouTube, and this was a meaningful selling point to candidates who wanted to work at scale but didn’t necessarily want to work at FAANG

#### The surprising value of polarization

Today’s job seeker is in equal parts jaded and savvy, and we’re currently in a labor market climate where talent has the power. The latter makes branding especially important, and by now, engineers have been told all manners of generalities about how much impact they’re going to have if they join a small startup and how whatever you’re working on is going to change the world… to the point where these things have become cliches that shows like Silicon Valley deride with impunity. To avoid cliches like this, think about what TRULY makes you special, and even if it’s a bit polarizing, own it. It’s your story, and the more honest you are about who you are and what you do, the more trust you’ll build with your audience and the more they’ll want to engage.

Another way to say this is that the most memorable stories might be shrouded in a bit of controversy. That’s not to say that you have to be controversial or contrive it when it isn’t there, but if you do operate in a space or have some aspect to your culture or tech stack that not everyone agrees with, you might find that the resulting self-selection among candidates can work to your advantage. Below are some examples of polarizing narratives.

- **Your work style.** Some companies really value work-life balance, whereas others exalt burning the midnight oil. Some run on chaos and some take a more orderly approach. Some work by the book, and some choose more of a bulldoze your way to success and ask forgiveness rather than permission approach. An example of the latter is Uber — for a long time, their culture was known for a take-no-prisoners approach to getting things done, and this approach has a certain type of appeal for the right people.
- **Your tech stack.** Certainly choosing your tech stack is, first and foremost, and engineering decision, but this decision has some implications for hiring, and choosing a fringe or niche stack or language can be a great way to attract the entire community around it. The more culty a language, the more fiercely passionate its acolytes will be about working for you (e.g. Rust… though by the time this guide comes out it might be something else!). Note that it doesn’t have to be thaaaat fringe of a language choice as long as there’s a tight-knit community around it, e.g. C#.
- **Your engineering culture.** Do you subscribe to any particular type of methodology that might be controversial, e.g. are you super into TDD? Are you adamant about rolling all your own everything?

Note that there is no right or wrong here — to loosely paraphase Tolstoy, every startup is broken in its own way, and one saying we’ve heard is that, especially during the early days, The only thing you can do wrong is not own who you are — if you misrepresent how you work or make decisions, you’ll find yourself in one of two regrettable positions: either your hires will leave well before their time or you’ll have a bunch of people marching in different directions or completely paralyzed and unable to choose the right course of action on their own.

### Exercise 3 – Your employees’ unique perspective

As your team grows beyond you, you will find that your employees’ reasons for working for you are likely different than the answers to the questions above. Talking to them (or, if you don’t want to put them on the spot, having another team member do so) can surface gold for your narrative. In particular, when I was a recruiter, one of the most useful exercises I did was asking my client to introduce me to a very specific handful of engineers. **In particular, I was looking for people who 1) didn’t come to the company through personal referrals and 2) had a lot of good offers during their last job search. Why this mix of people? Because they’re the ones who, despite no personal connection to the company and despite other having other options, actively chose to work for you!** You’d be surprised what stories I heard, and they’re rarely just about the company mission. For instance, one candidate I spoke to was really excited about the chance to closely interact with investors because he wanted to start his own company one day. Another was stoked at the chance to use Go in production.

Sometimes you’ll be surprised by what you’ll hear because the people working at your company might be there for very different reasons than you, but these anecdotes help flesh out your narrative and make it feel a bit more personal and real.

Once you have a few choice tidbits from employees, ask yourself whether each one is somehow charming or unusual and whether it’s a reason that a lot of people would find compelling about your company. If it’s all of these things, it should likely make it in your narrative. If it’s not particularly original (e.g. short commute) it may not be worth calling out in your primary narrative, but it’s well worth repeating and telling once you actually interact with candidates.

## The finished product

So, what should the finished product look like? At a minimum, it’ll be some concise, compelling copy that you can use in your job descriptions. Hopefully, though, it’s more than that. Hopefully it becomes a consistent refrain you and your team use during calls, interviews, maybe investor pitches… a way to highlight all the things you’re most proud of about your company and the things that make you special… without having to reinvent the wheel every time.

## Is brand the be-all and end-all of hiring? Not quite.

In closing, I’d like to leave you with a word or two of encouragement. Sure, as you saw in this post, brand matters. Having a great story will you get somewhere, but it won’t get you everywhere with candidates, and the truth is that the more established you are, the more candidates will come to you. But… there’s one piece of data we found in our interviewing and hiring adventures that flies in the face of brand completely proscribing your hiring destiny.

When we looked at how often candidates wanted to work at companies after interviewing there as a function of brand strength, its impact was not statistically significant. In other words, we found that brand strength didn’t matter at all when it came to either whether the candidate wanted to move forward or how excited the candidate was to work at the company. This was a bit surprising, so I decided to dig deeper. Maybe brand strength doesn’t matter overall but matters when the interviewer or the questions they asked aren’t highly rated? In other words, can brand buttress less-than-stellar interviewers? Not so, according to our data. Brand didn’t matter even when you corrected for interviewer quality. In fact, of the top 10 best-rated companies on our platform, half have no brand to speak of, 3 are mid-sized YC companies that command respect in Bay Area circles but are definitely not universally recognizable, and only 2 have anything approaching household name status.

So, what’s the takeaway here? **Maybe the most realistic thing we can say is that while brand likely matters a lot for getting candidates in the door, once they’re in, no matter how well-branded you are, they’re yours to lose.**

So, take heart.

*Portions of this post will also appear in part in an upcoming, comprehensive [Guide to Technical Recruiting and Hiring](https://www.holloway.com/g/technical-recruiting-hiring/about) published by Holloway (where you can sign up if you’d like to read, review, or contribute to it).*

1. The [2019 Stack Overflow Developer Survey](https://insights.stackoverflow.com/survey/2019#work) recently came out, and it turns out that in the US the most important thing for engineers is office/company culture… which realistically refers to the eng team culture because that’s engineers will spend most of their time. Anything you can do to call yours out (assuming, well, that it’s good) is going to be a win. [↩](#user-content-fnref-1)


# [Lessons from a year’s worth of hiring data](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data)

By Aline Lerner | Published: June 21, 2013; Last updated: May 1, 2023

*Note: This post is syndicated from [Aline Lerner’s personal blog](http://blog.alinelerner.com). Aline is the CEO and co-founder of interviewing.io, and results like these are what inspired her to start this company.*

I ran technical recruiting at [TrialPay](http://www.trialpay.com) for a year before going off to start my own agency. Because I used to be an engineer, one part of my job was conducting first-round technical interviews, and between January 2012 and January 2013, I interviewed roughly 300 people for our back-end/full-stack engineer position.

TrialPay was awesome and gave me a lot of freedom, so I was able to use my intuition about whom to interview. As a result, candidates ranged from self-taught college dropouts or associate’s degree holders to PhD holders, ACM winners, MIT/Harvard/Stanford/Caltech students, and Microsoft, Amazon, Facebook, and Google interns and employees with a lot of people in between.

**While interviewing such a wide cross section of people, I realized that I had a golden opportunity to test some of the prevalent folk wisdom about hiring.** The results were pretty surprising, so I thought it would be cool to share them. Here’s what I found:

- typos and grammatical errors matter more than anything else
- having attended a top computer science school doesn’t matter
- listing side projects on your resume isn’t as advantageous as expected
- GPA doesn’t seem to matter

And the least surprising thing that I was able to confirm was that:

- having worked at a top company matters

Of course, a data set of size 300 is a pittance, and I’m a far cry from a data scientist. Most of the statistics here is done with the help of [Statwing](http://www.statwing.com) and with Wikipedia as a crutch. With the advent of more data and more rigorous analysis, perhaps these conclusions will be proven untrue. But, you gotta start somewhere.

## Why any of this matters

In the status quo, most companies don’t run exhaustive analyses of hiring data, and the ones that do keep it closely guarded and only share [vague generalities](http://www.businessinsider.com/big-data-in-the-workplace-2013-5) with the public. As a result, a certain mysticism persists in hiring, and great engineers who don’t fit in “the mold” end up getting cut before another engineer has the chance to see their work.

Why has a pedigree become such a big deal in an industry that’s supposed to be a meritocracy? At the heart of the matter is scarcity of resources. When a company gets to be a certain size, hiring managers don’t have the bandwidth to look over every resume and treat every applicant like a unique and beautiful snowflake. As a result, the people doing initial resume filtering are not engineers. Engineers are expensive and have better things to do than read resumes all day. Enter recruiters or HR people. As soon as you get someone who’s never been an engineer making hiring decisions, you need to set up proxies for aptitude. Because these proxies need to be easily detectable, things like a CS degree from a top school become paramount.

**Bemoaning that non-technical people are the first to filter resumes is silly because it’s not going to change. What can change, however, is how they do the filtering.** We need to start thinking analytically about these things, and I hope that publishing this data is a step in the right direction.

## Method

To sort facts from folk wisdom, I isolated some features that were universal among resumes and would be easy to spot by technical and non-technical people alike and then ran statistical significance tests on them. My goal was to determine which features were the strongest signals of success, which I defined as getting an offer. I ran this analysis on people whom we decided to interview rather than on every applicant; roughly out 9 out of 10 applicants were screened out before the first round. The motivation there was to gain some insight into what separates decent candidates from great ones, which is a much harder question than what separates poor candidates from great ones.

Certainly there will be some sampling bias at play here, as I only looked at people who chose to apply to TrialPay specifically, but I’m hoping that TrialPay’s experience could be a stand-in for any number of startups that enjoy some renown in their specific fields but are not known globally. It also bears mentioning that this is a study into what resume attributes are significant when it comes to getting hired rather than when it comes to on-the-job performance.

Here are the features I chose to focus on (in no particular order):

- BS in Computer Science from a top school (as determined by U.S. News and World Report)
- Number of grammatical errors, spelling errors, and syntactic inconsistencies
- Frequency of buzzwords (programming languages, frameworks, OSes, software packages, etc.)
- How easy it is to tell what someone did at each of their jobs
- Highest degree earned
- Resume length
- Presence of personal projects
- Work experience in a top company
- Undergraduate GPA

## TrialPay’s hiring bar and interview process

Before I share the actual results, a quick word about context is in order. TrialPay’s hiring standards are quite high. We ended up interviewing roughly 1 in 10 people that applied. Of those, after several rounds of interviewing (generally a phone screen followed by a live coding round followed by onsite), we extended offers to roughly 1 in 50, for an ultimate offer rate of 1 in 500. The interview process is pretty standard, though the company shies away from asking puzzle questions that depend on some amount of luck/clicking to get the correct answer. Instead, they prefer problems that gradually build on themselves and open-ended design and architecture questions. For a bit more about what TrialPay’s interview process (used to) look like, check out [Interviewing at TrialPay 101](https://web.archive.org/web/20150201092525/http://enginerds.trialpay.com/2013/03/08/trialpay-engineering-interviews/).

Now, here’s what I discovered. The bar height represents [effect size](http://en.wikipedia.org/wiki/Effect_size). Every feature with a bar was statistically significant, and if you mouse over each bar, you can also see the [p-value](http://en.wikipedia.org/wiki/P-value). These results were quite surprising, and I will try to explain and provide more info about some of the more interesting stuff I found.

![Chart showing the effect size of various resume features](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F47643_effect_size_of_resume_features_313437aa69.png&w=1920&q=75 "Effect Size of Resume Features")

The most significant feature by far was the presence of typos, grammatical errors, or syntactic inconsistencies.

Errors I counted included everything from classic transgressions like mixing up “its” and “it’s” to typos and bad comma usage. In the figure below, I’ve created a fictional resume snippet to highlight some of the more common errors.

![Sample resume fragment highlighting common errors](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fissues_5ce006ca10.png&w=1920&q=75)

This particular result was especially encouraging because it’s something that can be spotted by HR people as well as engineers. When I surveyed 30 hiring managers about which resume attributes they thought were most important, however, no one ranked number of errors highest. Presumably, hiring managers don’t think that this attribute is that important for a couple of reasons: (1) resumes that are rife with mistakes get screened out before even getting to them and (2) people almost expect engineers to be a bit careless with stuff like spelling and grammar. With respect to the first point, keep in mind that the resumes in this analysis were only of people whom we decided to interview. With respect to the 2nd point, namely that engineers shouldn’t be held to the same writing standards as people in more humanities-oriented fields, I give you my next chart. Below is a breakdown of how resumes that ultimately led to an offer stacked up against those that didn’t. (Here, I’m showing the absolute number of errors, but when I ran the numbers against number of errors adjusted for resume length, the results were virtually identical.)

If you want to play with these histograms, just click on the image, and an interactive version will pop up in a separate window.

![Charts showing error frequencies among candidates with and without offers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ferrors_histogram_f9a2291694.png&w=1920&q=75 "Error frequencies among candidates")

**As you can see, the distributions look quite different between the group of people who got offers and those that didn’t. Moreover, about 87% of people who got offers made 2 or fewer mistakes**.

In startup situations, not only are good written communication skills extremely important (a lot of heavy lifting and decision making happens over email), but I have anecdotally found that being able to write well tends to correlate very strongly with whether a candidate is good at more analytical tasks. Not submitting a resume rife with errors is a sign that the candidate has strong attention to detail which is an invaluable skill when it comes to coding, where there are often all manners of funky edge cases and where you’re regularly being called upon to review others’ code and help them find obscure errors that they can’t seem to locate because they’ve been staring at the same 10 lines of code for the last 2 hours.

It’s also important to note that a resume isn’t something you write on the spot. Rather, it’s a document that you have every opportunity to improve. You should have at least 2 people proofread your resume before submitting it. When you do submit, you’re essentially saying, “This is everything I have done. This is what I’m proud of. This is the best I can do.” So make sure that that is actually true, and don’t look stupid by accident.

## Top company

No surprises here. The only surprise is that this attribute wasn’t more significant. Though I’m generally not too excited by judging someone on pedigree, having been able to hold down a demanding job at a competitive employer shows that you can actually, you know, hold down a demanding job at a competitive employer.

Of all the companies that our applicants had on their resumes, I classified the following as elite: Amazon, Apple, Evernote, Facebook, Google, LinkedIn, Microsoft, Oracle, any Y Combinator startup, Yelp, and Zynga.

## Undergraduate GPA

After I ran the numbers to try to figure out whether GPA mattered, the outcome was a bit surprising: GPA appeared to not matter at all. Take a look at the GPA distribution for candidates who got offers versus candidates that didn’t (click to get a bigger, more interactive version).

![Charts showing GPAs for candidates with and without offers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgpa_f345e799bf.png&w=1920&q=75 "GPAs for candidates with/without offers")

As a caveat, it’s worth mentioning that roughly half of our applicants didn’t list their GPAs on their resumes, so not only is the data set smaller, but there are probably some biases at play. I did some experiments with filling in the missing data and separating out new grads, and I will discuss those results in a future post.

## Is it easy to tell what the candidate actually did?

Take a look at this role description:

![A fragment from a good role description](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frole_desc_1521a16212.png&w=1920&q=75 "Role description fragment #1")

Now take a look at this one:

![A fragment from a bad role description](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbad_example_ac35863614.png&w=1920&q=75 "Role description fragment #2")

In which of these is it easier to tell what the candidate did? I would argue that the first snippet is infinitely more clear than the second. In the first, you get a very clear idea of what the product is, what the candidate’s contribution was in the context of the product, and why that contribution matters. In the second, the candidate is using some standard industry lingo as a crutch — what he said could easily be applied to pretty much any software engineering position.

Judging each resume along these lines certainly wasn’t an exact science, and not every example was as cut-and-dry as the one above. Moreover, while I did my best to avoid confirmation bias while deciding whether I could tell what someone did, I’m sure that the system wasn’t perfect. All this said, however, I do find this result quite encouraging. **People who are passionate about and good at what they do tend to also be pretty good at cutting to the chase.** I remember the feeling of having to write my resume when I was looking for my first coding job, and I distinctly remember how easily words flowed when I was excited about a project versus when I knew inside that whatever I had been working on was some bullshit crap. In the latter case is when words like “software development life cycle” and a bunch of acronyms reared their ugly heads… a pitiful attempt to divert the reader from lack of substance by waving a bunch of impressive sounding terms in his face.

This impression is further confirmed by a word cloud generated from candidate resumes that received an offer versus those that didn’t. For these clouds, I took words that appeared very frequently in one data set relative to how often they appeared in the other one.

![Word cloud showing frequent words in successful resumes](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcloud_1_00190b3201.png&w=1920&q=75 "Frequent words in successful resumes")

Offer

![Word cloud showing frequent words in unsuccessful resumes](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcloud_2_15eace8bc4.png&w=1920&q=75 "Frequent words in unsuccessful resumes")

No offer

As you can see, “good” resumes focused much more on action words/doing stuff (“manage”, “ship”, “team”, “create”, and so on) versus “bad” resumes which, in turn, focused much more on details/technologies used/techniques.

## Highest degree earned

![A quote](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fquote_3_1a25aba753.png&w=1080&q=75 "A quote")

Though highest degree earned didn’t appear to be significant in this particular data set, there was a definite trend that caught my attention. Take a look at the graph of offers extended as a function of degree.

![Chart showing offer likelihood as a function of highest degree earned by candidates](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Foffer_4_e24fb96866.png&w=1920&q=75 "Offer Likelihood as a Function of Highest Degree Earned")

As you can see, the higher the degree, the lower the offer rate. I’m confident that with the advent of more data (especially more people without degrees and with master’s degrees), this relationship will become more clear. **I believe that self-motivated college dropouts are some of the best candidates around because going out of your way to learn new things on your own time, in a non-deterministic way, while juggling the rest of your life is, in some ways, much more impressive than just doing homework for 4 years.** I’ve already ranted quite a bit about [how worthless I find most MS degrees to be](http://blog.alinelerner.com/how-different-is-a-b-s-in-computer-science-from-a-m-s-in-computer-science-when-it-comes-to-recruiting/), so I won’t belabor the point here.[1](#user-content-fn-1)

## BS in Computer Science from a top school

*But wait*, you say, *even if highest degree earned doesn’t matter, not all BS degrees are created equal! And, having a BS in Computer Science from a top school must be important because it’s in every fucking job ad I’ve ever seen!*

And to you I say, *Tough shit, buddy.* Then I feel a bit uncomfortable using such strong language, in light of the fact that n ~= 300. **However, roughly half of the candidates** (122, to be exact) **in the data set were sporting some fancy pieces of paper. And yet, our hire rate was not too different among people who had said fancy pieces of paper and those that didn’t. In fact, in 2012, half of the offers we made at TrialPay were to people without a BS in CS from a top school.** This doesn’t mean that every dropout or student from a 3rd rate school is an unsung genius — there were plenty that I cut before interviewing because they hadn’t done anything to offset their lack of pedigree. However, I do hope that this finding gives you a bit of pause before taking the importance of a degree in CS from a top school at face value.

![Product placement: Pedigree brand](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpedigree_5_b98917bcd8.png&w=1080&q=75 "Pedigree brand")

In a nutshell, when you see someone who doesn’t have a pedigree but looks really smart (has no errors/typos, very clearly explains what they worked on, shows passion, and so forth), do yourself a favor and interview them.

## Personal projects

Of late, it’s become accepted that one should have some kind of side projects in addition to whatever it is you’re doing at work, and this advice becomes especially important for people who don’t have a nice pedigree on paper. Sounds reasonable, right? Here’s what ends up happening. To game the system, applicants start linking to virtually empty GitHub accounts that are full of forked repos where they, at best, fixed some silly whitespace issue. In other words, it’s like 10,000 forks when all you need is a glimmer of original thought.

![A pile of forks](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffork_6_a18c62785b.png&w=1200&q=75 "Many forks")

Yay forks.

Outside of that, there’s the fact that not all side projects are created equal. I can find some silly tutorial for some flashy UI thing, copy the code from it verbatim, swap in something that makes it a bit personal, and then call that a side project on my resume. Or I can create a new, actually useful JavaScript framework. Or I can spend a year bootstrapping a startup in my off hours and get it up to tens of thousands of users. Or I can arbitrarily call myself CTO of something I spaghetti-coded in a weekend with a friend.

Telling the difference between these kinds of projects is somewhat time-consuming for someone with a technical background and almost impossible for someone who’s never coded before. **Therefore, while awesome side projects are a HUGE indicator of competence, if the people reading resumes can’t (either because of lack of domain-specific knowledge or because of time considerations) tell the difference between awesome and underwhelming, the signal gets lost in the noise.**

When I started this project, it was my hope that I’d be able to debunk some myths about hiring or at least start a conversation that would make people think twice before taking folk wisdom as gospel. I also hoped that I’d be able to help non-technical HR people get better at filtering resumes so that fewer smart people would fall through the cracks. Some of my findings were quite encouraging in this regard because things like typos/grammatical errors, clarity of explanation, and whether someone worked at an elite company are all attributes that a non-technical person can parse. I was also especially encouraged by undergraduate pedigree not necessarily being a signal of success. At the end of the day, spotting top talent is extremely hard, and much more work is needed. I’m optimistic, however. As more data becomes available and more companies embrace the spirit of transparency, proxies for aptitude that don’t stand up under scrutiny will be eliminated, better criteria will take their place, and smart, driven people will have more opportunities to do awesome things with their careers than ever before.

- [TrialPay](http://www.trialpay.com), for letting me play with their data and for supporting my ideas, no matter how silly they sounded.
- [Statwing](http://www.statwing.com), for making statistical analysis civilized and for saving me from the horrors of R (or worse, Excel).
- Everyone who suggested features, helped annotate resumes, or proofread this monstrosity.

Lastly, see [Hacker News](https://news.ycombinator.com/item?id=6326477 "Hacker News") for some good discussion.

Looking for a job yourself? Work with a recruiter who’s a former engineer and can actually understand what you’re looking for. Drop me a line at [aline@alinelerner.com](mailto:aline@alinelerner.com).

1. It is worth mentioning that my statement about MS degrees potentially being a predictor of poor interview performance does not contradict this data — when factoring in other roles I interviewed for, especially more senior ones like Director of Engineering, the (negative) relationship is much stronger. [↩](#user-content-fnref-1)


# [interviewing.io is finally out of beta. Anonymous technical interview practice for all!](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all)

By Aline Lerner | Published: June 3, 2020; Last updated: May 1, 2023

I started [interviewing.io](https://interviewing.io/) 5 years ago. After working as both an engineer and a recruiter, my frustration with how inefficient and unfair hiring had reached a boiling point. What made me especially angry was that despite mounting evidence that resumes are [poor predictors of aptitude](http://blog.alinelerner.com/resumes-suck-heres-the-data/%22%20target), employers were obsessed with where people had gone to school and worked previously. In my mind, any great engineer, regardless of how they look on paper, should have the opportunity to get their foot in the door wherever they choose.

So, we set out to build a better system. On interviewing.io, software engineers can book anonymous mock interviews with senior engineers from companies like Facebook, Google, and others, and if they do well in practice, get fast-tracked with top employers regardless of how they look on paper. Fast-tracking means that you bypass resume screens, scheduling emails, and recruiter calls, and go straight to the technical interview (which, by the way, is still anonymous [1](#user-content-fn-1)) at companies of your choice. **Because we use interview data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Twitch, Lyft, Dropbox, and many others, and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds.** What’s nuts is that many of our candidates have literally been rejected based on their resumes by the same employer who later hired them when they came through interviewing.io. **One notable candidate was rejected *three* times from a top-tier public company based on his resume before he got hired at that same company through us.**

Over the past 5 years, we’ve hosted over 50,000 technical interviews (both practice and real) on our platform. Our [YouTube channel](https://www.youtube.com/channel/UCNc-Wa_ZNBAGzFkYbAHw9eg), where you can watch other people interview, has gotten over 3.5M views, and, most importantly, we have helped thousands of engineers get great jobs.

![Screenshot of a live and anonymous interview](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F7ff46_screenshot_2020_06_03_15_57_12_894fcecad3.webp&w=1920&q=75 "A live and anonymous interview")

All practice interviews are completely anonymous and include actionable, high-fidelity feedback

**Despite that for our entire, multi-year existence, we’ve been in beta. Over the past year or so, this increasingly inaccurately named “beta” became kind of a smoke screen.** Our product was stable, we had plenty of interviewers, but sadly, we couldn’t serve many of the people who needed us most. Because we made money by charging companies for hires, despite a growing waitlist of >180,000 engineers, we could only serve the ones whom we had a shot at placing, i.e. engineers who 1) were located in a city where we had customers and 2) had 4 or more years of experience⁠—sadly, despite our best efforts, employers across the board were not willing to pay for junior hires.

Then, COVID-19 happened and with it, a deluge of hiring slowdowns and freezes. In a matter of weeks, we found ourselves down from 7-figure revenue to literally nothing. Companies didn’t really want or need to pay for hiring anymore.

But these hiring slowdowns freezes weren’t just affecting us. In parallel, we saw a growing sea of layoffs, and we realized that, soon, more and more candidates would be vying for a shrinking number of jobs. **On top of that, because a disproportionate amount of layoffs targeted recruiters, an overworked in-house skeleton recruiting team would go back to relying on resumes and other old-school proxies, unintentionally marginalizing non-traditional candidates once again**. We also realized that many of the folks getting laid off would be here on visas, which meant that they’d have a paltry 60 days to find their next job or risk deportation.

So, we made a hard call. You may know that historically, we’ve offered *completely free* mock interviews. What you may not know, is that we pay our professional interviewers, as it’s the only way to ensure that we have seasoned, experienced engineers delivering a consistent, realistic, and high-quality experience. This is often our largest expense.

Since we previously funded practice by charging companies for hires, we had to find another revenue stream to continue. In the face of either shutting down the company, unable to provide any practice at all, or to begin to charge for premium practice interviews, we made the choice to launch a paid tier. But, because charging for practice felt anathema to our mission, we knew we needed some ground rules in place. I started this company to fix hiring, after all, and that’s why the people who work at interviewing.io are here, too.

After 50,000 interviews, our data shows that [where someone goes to school has no correspondence to their interview performance](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt). Despite that, we are all too aware that just because aptitude is uniformly distributed among populations, resources are not. We understand that paying for interviews will be prohibitive to many of the people who need help most, so our ground rules and goals for this pivot were as follows:

- We’d ALWAYS have a free tier
- We’d immediately start working on a fellowship for engineers from underrepresented backgrounds or in a visa crisis experiencing financial hardship
- We’d find a way to let people defer their payments

There is an upside to our new model though. Now that we’re no longer strictly beholden to employers, we’re able to open up interviewing.io to engineers of all seniority levels and many more locations. And because our revenue isn’t coming from placements but directly from practice, we don’t have to constrain our users to a limited number of practice interviews.

**So, as of today, interviewing.io is open to engineers of all experience levels in North America and the UK**. Engineers who sign up can book mock interviews 24 hours out, and top performers will still get fast-tracked for great jobs.[2](#user-content-fn-2)

![Screenshot of the interview scheduler wizard - picking an interviewer type, the type of interview and a timeslot](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fedfb7_dashboard_1_71ade2a5c3.webp&w=1920&q=75 "Scheduling an interview")

You can book either free (peer practice with other users) or premium interviews with experienced interviewers from FAANG, as early as 24 hours out

- **If you’re able to pay, you can now book unlimited interviews with professional interviewers from FAANG and other top companies.** We’re no longer constraining you to 2 or 3, and we have the interviewer supply to make this work. Interviews cost between $100 and $225.
- **If you can’t pay, there’s a free tier where you can do peer practice with other users.**

What about the goals and ground rules above? We’ve already made some headway against these. The free tier was there from day one. Moreover, a number of our professional interviewers have stepped up and volunteered their time to help the people who need it most prepare for free (if you’d like to volunteer your time to help engineers from underrepresented groups practice interviewing, please email [interviewers@interviewing.io](mailto:interviewers@interviewing.io) with the subject line ‘Volunteer’), and a formal fellowship is in the works. Lastly, we’re working on a deferred payment plan, where users who buy premium interviews will not have to pay us for them until they find their next job.

![Screenshot of a zoom meeting with the whole team](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F73996_teamzoom_1aeec85ecf.webp&w=1920&q=75 "The team")

COVID-19 has changed a lot of things for our team (that’s us above doing the remote work thing). But not how much we want to fix hiring.

Look, whether you got laid off, are a student who’s reeling from your internship being withdrawn, lost your visa, whether you’re fortunate enough to still be employed but worry about your job stability, or if you just want to help making hiring fairer, we hope you’ll [take us for a spin](https://interviewing.io/). Technical interviews are hard, and hiring is broken. And whether you’re new to interviewing or are just rusty after being off the market, and whether you can pay or not, we have your back.

The coming months are going to be hard. We know that in the current climate more people than ever are feeling helpless, and the world feels like it’s burning. And it might not even seem like technical interviews matter that much. But they do to us… because this is our way of creating a world where access to opportunity isn’t determined by who you are or where you come from but by what you can do.

P.S. If you don’t need practice for yourself but you or your organization want to help others get awesome at technical interviews, check out our [new gifting feature](https://interviewing.io/gift-practice-interviews).

1. "Unlike other hiring marketplaces, we will always be an anonymous platform. No public profiles, no sharing of candidate information, and no one but you gets access to your interview history. You get to decide after each interview if you want to unmask, one interview at a time." [↩](#user-content-fnref-1)
2. If you were waitlisted previously and have been patiently waiting to get your invite, we’re issuing thousands of invites a day and we should get to you within a few days. [↩](#user-content-fnref-2)

### [Count Complete Tree Nodes](/questions/count-complete-tree-nodes)

[Given the root of a complete binary tree, return the number of nodes in the tree.](/questions/count-complete-tree-nodes)


# [I love meritocracy, but all the recent anti-DEI rhetoric is bad](https://interviewing.io/blog/i-love-meritocracy-but-all-the-recent-anti-dei-rhetoric-is-bad)

By Aline Lerner | Published: October 2, 2024; Last updated: October 31, 2024

I’m the founder of interviewing.io, an anonymous technical recruiting marketplace. In some ways, I’m the meritocracy hipster who was [writing about how eng hiring should be meritocratic](https://blog.alinelerner.com/silicon-valley-hiring-is-not-a-meritocracy/) and about [how quotas are bad](https://blog.alinelerner.com/diversity-quotas-suck-heres-why/), way before saying either was cool. At interviewing.io, my team and I have been trying to make hiring meritocratic for the last decade. Briefly, we do anonymous mock interviews. If people perform well in those interviews, they get introduced directly to decision-makers at top-tier companies, regardless of how they look on paper. 40% of the thousands of people we’ve helped were excellent engineers who did not look good on paper and would NOT have gotten in the door through traditional, “meritocratic” channels. Many of those engineers were rejected based on their resumes by the very same companies where they were later hired through us.

Recently, there’s been a lot of pro-meritocracy, anti-DEI rhetoric. The most salient example is [Alexandr Wang’s (CEO of Scale AI) tweet about how their hiring process has to be meritocratic](https://twitter.com/alexandr_wang/status/1801331034916851995) (including a catchy new acronym, “MEI”).

> Today we’ve formalized an important hiring policy at Scale. We hire for MEI: merit, excellence, and intelligence.  
>   
> This is the email I’ve shared with our [@scale\_AI](https://twitter.com/scale_AI?ref_src=twsrc%5Etfw) team.  
>   
> ———————————————————  
>   
> MERITOCRACY AT SCALE  
>   
> In the wake of our fundraise, I’ve been getting a lot of questions…
>
> — Alexandr Wang (@alexandr\_wang) [June 13, 2024](https://twitter.com/alexandr_wang/status/1801331034916851995?ref_src=twsrc%5Etfw)

The post got a resounding “Great!” from Elon Musk a half hour later, followed by a wall of accolades from the Twitterverse. Since then, a [“meritocracy board”](https://www.meritocracy.com/) has sprung up as well.

If you read Wang’s post carefully, you’ll see that he provides no specific examples of how Scale AI makes hiring meritocratic and doesn’t share any details about their current hiring process. I don’t know anyone from the Scale AI team personally, but after doing eng hiring in some form or another for over 15 years, I have questions. Does Scale AI’s hiring process differ substantially from other companies’? Or are they doing the same thing as everyone else: recruiters look at resumes, pick people who have top brands on their resume, and interview them?

**If their process is indeed like everyone else’s, no matter what they say, they’re no more meritocratic than the companies who tout DEI hiring practices… and are just virtue signaling on Twitter.**

I’ll be the first to admit that DEI is ideologically flawed because of its emphasis on race and gender-based outcomes and its insistence on equality of those outcomes. In the last decade, we’ve seen some pretty bad DEI practices, the most egregious being a company looking up candidate photos on LinkedIn and rejecting qualified white, male candidates. (I talk more about the worst-offending hiring practices I’ve seen over the last decade in the section called *The dark side of diversity… and two stories of diversity initiatives gone wrong* below. If you just want the juicy bits, read that part.)

**However, yelling “Meritocracy!” as if it’s a fait accompli is just as harmful as the worst parts of DEI.** In the last decade, we’ve seen countless companies claim to be meritocratic but refuse to talk to candidates unless they had a CS degree from one of a select few schools. There is nothing meritocratic about that. After seeing the pendulum swing back and forth a bunch in this space, **I’d even go so far to say that, ironically, the DEI movement has done more for meritocracy than the loud pro-meritocracy movement is doing right now**.

**I’m delighted that “meritocracy” is no longer a dirty word. But, just saying it isn’t enough. We have to change our hiring practices.** We need to stop using meritocracy as a shield to preserve the status quo. **If we could instead put into practice the best part of DEI – openness to hiring candidates from non-traditional backgrounds while eliminating the toxic hyperfocus on race and gender and the insistence on equality of outcomes, then we could create a real meritocracy, which is what most reasonable people actually want.**[1](#user-content-fn-1)

*A quick disclaimer before we go further. To the right, DEI has come to mean mediocrity, and as such, it’s pitted, apples to apples, against meritocracy. That is not the intent here. When I talk about DEI, I’m not talking about the political side of it or how it’s often co-opted by the left as a gateway to Marxism. Similarly, meritocracy has been co-opted by the right to justify racism, eugenics, and god knows what other horrid things. Both extremes are bad. I’m trying to shed all political associations from either word and to just talk about them purely as hiring ideologies.*

## DEI’s outcomes problem

On its face, increasing diversity sounds great. Some groups are underrepresented in tech, likely because of inequality of opportunity. Talent is distributed uniformly, opportunity is not. Let’s fix it!

Twitter threads like this one (from an engineering leader at Google) are hard to argue with. You should read the whole thing — it’s about a (white) lumberjack’s son who ended up as one of the founding employees at SpaceX.

> Everyone loves SpaceX, and thinks of Elon as the genius founder that invents new types of rockets that are cheaper, faster, more efficient.  
>   
> It's fun to think of it as SpaceX versus NASA, or Silicon Valley vs Aerospace.  
>   
> But let's talk about D&I, and logs. Logs as in timber. 🌲
>
> — Mekka 💉x7 @mekkaokereke@hachyderm.io (@mekkaokerekebye) [January 5, 2019](https://twitter.com/mekkaokerekebye/status/1081619342377156608?ref_src=twsrc%5Etfw)

And indeed, ostensibly, DEI is hard to argue against because it speaks to our innate desire for fairness and equal access to opportunity. Many DEI leaders honestly believe this. However, despite the good intentions, in practice, DEI tends to laser focus on race and gender *outcomes*, and that is hard to argue *for*.

Over the years, I’ve seen claims that diverse teams perform better, as well as claims that one must have a diverse workforce if one has a diverse customer base. Though it’s often stated as fact, the former is inconclusive — there are studies with clear results for AND clear results against.[2](#user-content-fn-2) To the best of my knowledge, the latter point is unsubstantiated as well — isn’t the hallmark of a good designer that they be able to design for customers who are different than they are?[3](#user-content-fn-3)

The arguments for diversity are inconclusive, and as such, the ultimate measure of success for diversity isn’t about the performance of an organization or about customer satisfaction. Those are packaged up as obvious side benefits. The way we measure success for diversity is tautological: success is measured by the diversity of our workforce.

What does that mean? **In practice, recruiting orgs usually define success by looking at some demographic and its representation in the general population.** So, in the case of women in tech, women make up half the U.S. population, so 50% of engineers in an organization should be women. Similarly, 12% of the U.S. population is Black, so for hiring to be equitable, 12% of the engineers in an organization should be Black. Likewise, 19% of the U.S. population is Hispanic, so 19% of engineers should be Hispanic, and so on.

What’s the problem with this approach? It does not account for inputs. The most basic input is: How many female engineers are there in the US? And how many Black or Hispanic engineers are there in the US?

The answer: not enough. Only 20% of CS graduates in the US are women. And there are also not enough engineers of color to get to race parity either. Only 6% of CS graduates in the US are Black, and only 7% are Hispanic.[4](#user-content-fn-4)

Those numbers get even more grim when you pare them down to how companies usually hire: from top-ranked schools. We’ll talk more about this pedigree-based approach to hiring when we discuss the pitfalls of meritocracy. For now, suffice it to say that a few years ago, [we ran the numbers to show that getting to gender parity in software engineering is mathematically impossible](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring) given companies’ focus on pedigree; though it was unfashionable to admit it, we called out that there really is a pipeline problem.

And then there’s this issue: What portion of those candidates are even applying to your company in the first place? And what portion of those applicants are actually qualified to do the work? The ONLY way to really take race and gender bias off the table is to do blind as much of the hiring process as possible and then to accept that you may not get the numbers you want but that your outcomes will *actually* be fair.

## The dark side of diversity (and two stories of diversity initiatives gone wrong)

In addition to mock interviews, interviewing.io also helps companies source engineering candidates. We know how people perform in mock interviews, and that lets us reliably predict who’ll do well in real interviews. We identify the top performers from practice and introduce them to companies. We’ve been doing it for a while, and our top performers have consistently outperformed candidates from other sources by about 3X.

**I promised in the beginning of this post that I’d spill some juicy tidbits. Here goes.**

Years ago, we pitched Facebook’s university recruiting team on using us to hire for their intern class. The pitch was that [we had thousands of college students from all over the U.S.](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools) who had done a bunch of mock interviews, and that we knew who the top performers were. Many of our students did NOT come from the handful of top-tier schools that Meta typically recruited from. If they were to recruit through us, they’d have to do way fewer interviews (we had already taken care of technical vetting), and they’d get a much more diverse slice of the population.

Our only process request was that they conduct interviews with students anonymously, on our platform, so they wouldn’t be biased against top-performing students who didn’t go to top schools.

**We didn’t get the gig. The main bit of pushback from Facebook was that anonymity violated guidelines set by the OFCCP.** The [OFCCP](https://en.wikipedia.org/wiki/Office_of_Federal_Contract_Compliance_Programs) (Office of Federal Contract Compliance Programs) is part of the U.S. Department of Labor and is “responsible for ensuring that employers doing business with the Federal government comply with the laws and regulations requiring nondiscrimination.” One of the many things that the OFCCP requires you to track, if you do business with the federal government, is the race and gender of your applicants. We couldn’t agree to this. While the requirement makes sense on the surface — as they say, you can’t fix what you can’t measure — in this case, it was a Kafkaesque roadblock to achieving the very thing that the OFCCP is fighting for: reducing discrimination.[5](#user-content-fn-5)

More broadly, you can’t take an outcomes-based approach unless your inputs are homogenous and the playing field is level. The biggest advocates of DEI will argue, correctly, that the playing field is not level. Given that it’s not level, focusing exclusively on outcomes creates all manners of perverse incentives — the dark side of diversity is the logical conclusion of an outcomes-based approach: incentivizing the selection of candidates based on race and gender and ultimately discriminating against non-URM candidates.

We’ve worked with companies of all sizes, from seed stage startups to FAANG, and at one point or another, we’ve worked with most FAANGs and FAANG-adjacent companies. We’ve seen it all. In 2022, at the height of diversity fever, one well-known FAANG-adjacent customer approached us with a specific request. Let’s call them AcmeCorp (name obviously changed; they’re a household name, but I don’t want to rake them over the coals publicly because they were a great partner to us until this thing happened).

**AcmeCorp’s recruiting team wanted us to do some pre-filtering on the candidates we introduced to them.**

We already do some pre-filtering: location, years of experience, visa status, and obviously historical performance in mock interviews. Only the top few percent of our candidates get to talk to employers.

But on our platform, everything is candidate driven. We don’t have a searchable candidate database, and we don’t share candidate data with companies. Rather, we list the companies who hire through us, and our top-performing users can connect with them.

Over our lifetime, plenty of companies have approached us asking if they could get access to *just* top-performing women and people of color on our platform. It makes sense. Recruiters are given marching orders to find more “diverse” candidates, and this is the result. And it’s a convenient way to pass on liability. Now, instead of their sourcers having to filter out candidates who aren’t “diverse”, we have to do it.

Of course, we’ve always denied these requests. We’re not a “diversity” platform, and we can’t imagine a world where we’d block what jobs and employers our users could see based on their race and gender (information we don’t collect systematically in the first place).[6](#user-content-fn-6)

Even though, on their face, these requests weren’t really OK, we got so many of them that, over time, we got desensitized and would joke internally about how yet another company wanted a [binder full of women](https://en.wikipedia.org/wiki/Binders_full_of_women).

However, AcmeCorp’s request was more egregious than the rest because it gave us visibility into how many companies were behaving internally when faced with diversity goals. It was common knowledge that many companies were doing diversity-specific sourcing, so we weren’t shocked when we were asked to help with that. What wasn’t common knowledge is that companies were blatantly rejecting qualified applicants who didn’t meet their diversity criteria.

AcmeCorp had a fairly complex candidate filtering process in place, and they wanted us to run that same process on any of our top performers who expressed interest in working there.

Here’s how their process worked. Note that AcmeCorp, like many companies, pays differently depending on where you live.

- List a remote job that’s hiring all over the United States.
- When candidates apply from high cost of living areas (e.g., the SF Bay Area, NYC), only consider women and people of color. Reject the rest.
- When candidates apply from lower cost of living areas (e.g., small towns in the Midwest), consider everyone.

**In other words, a white man from San Francisco would have no shot at getting an interview at this company — he would be auto-rejected and left to wonder what was wrong with his resume.**

Why did this company take this approach? They were willing to pay top dollar for women and people of color but not for other types of engineers, and they hid behind geography to do it. Because of the geographical element, it’s not as blatant as outright rejecting people based on race and gender, but for all intents and purposes, it’s the same.

Outside of this practice being questionably legal at best, it’s also unethical. You can argue that companies should be able to do outreach to any demographic groups that they want. It’s much harder to argue that it’s ok to reject applicants based on their race and gender.

We terminated the relationship.[7](#user-content-fn-7)

Unfortunately, when you tie the success of your recruiting program to gender and race outcomes, these are the behaviors that inevitably arise. For all its flaws, though, the DEI movement, coupled with increasing demand for engineers, propelled companies to make deep changes to their hiring processes. For every DEI horror story, there is an equal and opposing story about a Head of Talent or investor engineering leader who persuaded their eng hiring managers to stop looking just at students from MIT and Stanford, to change their interview processes, to blind resumes, and to do a bunch of other useful things that benefitted every non-traditional candidate.

But, back to what’s happening today. You don’t just get to say “meritocracy” and be done with it. In practice, meritocratic hiring doesn’t really exist, and what companies call meritocracy is anything but.

## The false promise of meritocracy

For most sane people, the concept of meritocracy is hard to argue against. Shouldn’t the most qualified person get the job?

**Unfortunately, because the definition of “qualified” is murky, meritocracy often becomes a justification for over-indexing on pedigree: where people went to school or where they worked previously.** “We just hire the best” often means “we hire people from FAANG, MIT, and Stanford.” Unfortunately, those are proxies for ability, not actual measures of it. Our research has consistently shown that where people go to school isn’t very predictive of what they can do. Where they’ve worked is *somewhat* predictive, but it’s not the most important thing.[8](#user-content-fn-8)

Despite that, those are the main criteria that companies use when they decide whom to interview, and because that’s the first step in a hiring funnel, it’s the one that gets applied to the most candidates. Any attempts at making the process meritocratic after the resume review (e.g., training interviewers, doing anonymous interviews) are bound to be less impactful because they affect 10X-100X fewer candidates.

Fortunately, for all their flaws, at least technical interviews do focus on ability — once you get in the door, it’s not about how you look on paper but about how you perform. As a result, all other things being equal, how you decide which candidates to let into your process is the litmus test for whether your process is truly meritocratic or not.

Unfortunately, the pedigree-based approach isn’t particularly meritocratic. In our 9 years, we’ve diligently tracked the backgrounds of our candidates, and as I mentioned in the intro to this post, about 40% of our top performers don’t look good on paper (but do as well as or outperform their pedigreed counterparts in interviews).

One of our users got rejected from a top-tier social network company three times… THREE TIMES… based on his resume before he got hired there through us, after doing very well in an anonymous interview. I’ve shared a few diversity horror stories, but the sad reality is that (faux) meritocracy horror stories like this one happen every day. I wish I had a real meritocracy horror story to share, but as far as I know, eng hiring has never been truly meritocratic. If you know otherwise, please do share.

Our data also shows that pedigree has very little bearing on interview performance. Where people went to school has no bearing on their interview performance, and though where people have worked does carry some signal, it’s not nearly as important as other traits — in past research, we’ve found that not having typos/grammatical errors on your resume is a much stronger signal than whether they’ve worked at a top company, as is whether they’ve done a lot of autodidactic work.[8](#user-content-fn-8)

Moreover, in two separate studies completed a decade apart, where recruiters had to judge resumes and try to pick out the strong candidates, we consistently saw that recruiters are only as accurate as a coin flip and largely disagree with each other about what a good candidate looks like.[9](#user-content-fn-9)

**That’s why posts like the one from Scale AI get my hackles up. You don’t get to say that you’re meritocratic if you’re just scanning resumes for top brands. That’s not meritocracy. It’s co-opting a hot-button word for clout.**

And it’s not just Scale AI. This is how tech companies define being meritocratic and hiring the best. It’s just that not all of them are so self-congratulatory about it.

So how do you ensure that your hiring is *actually* meritocratic?

## How to actually “do” meritocracy, if you mean it

In a [recent study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we looked at how recruiters read resumes and how good they are at finding talent. As you saw above, we learned that recruiters are barely better than a coin flip. Another thing we looked at in the same study was what made them pick certain resumes over others.

**The two traits that were the most predictive of whether a recruiter would pick you? First, whether you had top brands on your resume, and second, whether you were Black or Hispanic. This is how recruiters work today. If you don’t intervene and make changes, today’s competing approaches will *both* be implemented by your team simultaneously, resulting in a farcical chimera of fake meritocracy and outcomes-based diversity goals.**

So what can you actually do, if you, in good faith, want to run a meritocratic hiring process? (By the way, if you believe that talent is distributed uniformly, by definition, this approach will entail being open to talent from traditionally underrepresented backgrounds.)

**First, you have to move away from identity politics and expand the definition of “underrepresented.”** You have to believe, in your heart of hearts, that great talent can come from anywhere and must stop focusing arbitrarily on one marginalized group at the expense of another. Basically, you have to be open to any candidate who’s good, regardless of how they look on paper, without prioritizing race and gender. This certainly includes race and gender, but it also includes socioeconomic status, educational background (or lack thereof), and any number of other traits that have nothing to do with someone’s ability to do the job. Hell, why not just stop worrying about candidate backgrounds and have a process that welcomes all and surfaces the best? Following this path will logically require moving away from race and gender outcomes-based goals.

**Then, you have to accept and internalize that your current method of deciding who gets to interview, which is very likely focused on brands (where people have worked or where they’ve gone to school), is not only NOT meritocratic but also ineffective.** We talked above about how pedigree is very limited in its ability to predict performance.

**If you accept both of these premises — expanding the definition of “underrepresented” and moving away from focusing on brands — the hard work begins.** Companies have used resumes (and brands by extension) since time immemorial because they’re easy, and as you saw in our data above, they do carry *some* signal. But even though they carry a little signal, recruiters are not very good at extracting it.

**Here’s what you should do to pragmatically and realistically revamp your hiring process to be more meritocratic.** I challenge Scale AI and all the leaders on the “meritocracy board” to publicly commit to at least two of these — or to name the specific, actionable approaches they plan to take.

- Change how you read resumes.[10](#user-content-fn-10)
- Give candidates the option of submitting some writing about a past project they’re proud of.
- Give candidates the option of doing a take-home or an asynchronous assessment. Put enough work into those assignments such that you’ll trust their outcome enough to not have to look at a resume.
- [EXTRA CREDIT] Invest what you can in closing the gaps.

### Change how you read resumes

First, SLOW DOWN. In [the study I mentioned above](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we saw that recruiters take a median of 31 seconds to judge a resume, but spending just 15 extra seconds reading a resume could improve your accuracy by 34%.

Our second piece of advice is this. More than 20 years ago, Freada Kapor Klein from Kapor Capital coined the term “distance traveled,” referring to what someone accomplished, in the context of where they started. For instance, Kapor Klein recommends that, in their admissions processes, universities should consider not just the number of AP tests a candidate has passed but the number of AP tests taken divided by the total number offered at their high school. For example, if an applicant took 5 AP tests and their school offered 27, that paints a very different picture from another applicant who also took 5 AP tests when that’s the total number offered at their school. Kapor Capital uses distance traveled as one of their metrics for determining which entrepreneurs to fund. One can easily apply this concept to hiring as well.

The data shows that slowing down is important, and as part of slowing down, when you read a resume, try to evaluate candidates’ achievements, not in a vacuum, but in the context of where they came from. Think about the denominator. But don’t think for a moment that we recommend that you lower the bar — absolutely not. On interviewing.io, we regularly see nontraditional candidates objectively outperforming their FAANG counterparts.

### Give candidates the option of submitting some writing about a past project they’re proud of

My friends at KeepSafe and I [ran an experiment about a decade ago where we tried replacing resumes with a writing sample about a past project](https://blog.alinelerner.com/what-happens-when-you-stop-relying-on-resumes/). It was a huge success.

Even today, when we hire at interviewing.io, we use this approach. We mostly hire off of our own platform (we just list our own open positions alongside others). However, not all of our users have done enough mock interviews to have a rating, and for those users, we have a different flow where we ask them to write about a past project. Boy, are the results telling.

Here’s what our application form looks like. Steal it if you want.

![typeform application paragraph about project](https://strapi-iio.s3.us-west-2.amazonaws.com/typeform_application_paragraph_about_project_8777b4bcfc.png)

### Give candidates the option of doing a take-home or an asynchronous assessment

Take-homes and asynchronous assessments are not well-loved by candidates, primarily because of value asymmetry. They ask a lot of the candidate but nothing of the company, and it’s not uncommon for a candidate to have to do hours of work and then never hear anything back.

To be clear, this is NOT the setup we’re advocating. Here’s what we’d advise instead:

Give candidates the option of doing a take-home/assessment that takes no more than 1 hour, *instead of submitting their resume*. When we say option, we mean that the candidate can decide whether they want to do the take-home or not. If they choose not to, then you’ll read their resume, hopefully using our suggestions above. If they choose to complete the take-home, then you forgo their resume and make your go/no-go decision based entirely on the results of the take-home.

If you choose this route, it’s critical to [come up with an assessment whose results you trust](https://interviewing.io/blog/why-engineers-dont-like-take-homes-and-how-companies-can-fix-them). Many companies use a take-home in addition to getting the resume and will still not move forward with candidates who look good on paper. That’s not meritocratic. Take the time you need to come up with a question that’s hard to cheat on and that gets you the signal you need. Yes, coming up with a good assessment takes work. But no one said that making your hiring process meritocratic was easy.

### [EXTRA CREDIT] Invest what you can in closing the gaps

This advice probably applies more to big companies than smaller ones, because bigger ones have more resources to effect change. Regardless, if you believe in meritocracy, then you understand that a true meritocracy is not possible without a level playing field for your candidates. One of the best things about the DEI movement is that it’s made us aware how unlevel the playing field really is. Whether you subscribe to DEI or not, this is probably not a controversial statement, and if you want to see true, meritocratic hiring, you have some obligation to help promote equality of *opportunity*.

Where to begin?

Although I expect that it’s not level in many places, and there are plenty of opportunities to effect change, starting with elementary education[11](#user-content-fn-11), I'll talk about the inequality I’ve observed firsthand repeatedly over the last decade: [the technical interview practice gap](https://interviewing.io/blog/technical-interview-practice-gap). How much you practice is the biggest predictive factor of interview performance — not seniority, not gender, and not a CS degree. And so is socialization. After all, if you’re around people going through the same thing, like at a top-tier CS school, rather than beating yourself up after a disappointing interview, you’ll start to internalize that technical interviewing is flawed and that the outcomes are sometimes unpredictable. Fortunately, there are interventions one can make to close these gaps, and the simplest is to provide practice and community for people who don’t have access to them. Reach out to us about this, find a non-profit that helps people practice, donate to your favorite university if they have a good practice program, or any number of other things.

Ultimately, which gap you choose to help close and how you choose to do it is up to you. But if your company has the means, it’s your responsibility to invest in gap-closing measures. You don’t have to donate money. You can offer mock interviews to your candidates before their real interviews. You can start an apprenticeship program. You can encourage your engineers to do some tutoring. However you approach it, though, you can’t talk about meritocracy with a straight face and not do *something* to level the playing field.

1. In fairness, the Scale AI post positioned them as symbiotic. I believe that as well. [↩](#user-content-fnref-1)
2. There are many sources arguing for and against diversity leading to better-performing teams. Here are some examples:
     
   For: <https://www.mckinsey.com/featured-insights/diversity-and-inclusion/diversity-wins-how-inclusion-matters>  
   Against: <https://medium.com/the-liberators/in-depth-the-double-edged-sword-of-diversity-in-teams-765ff72a55da> (except for “age diversity”) and <https://corpgov.law.harvard.edu/2021/05/25/diversity-and-performance-in-entrepreneurial-teams/> [↩](#user-content-fnref-2)
3. One of the most insulting examples of the “we need a diverse workforce to serve our diverse customer base” argument occurred when I was pitching Amazon on using interviewing.io to hire. This was years ago, and back then, out of curiosity, I’d always ask the organizations we were pitching why they valued diversity. I don’t think I ever got a good answer, but this one was especially bad. One of the recruiters we met with went on a long diatribe about how Amazon sells lots of shoes and you need women on the eng team because women understand shoes better than men. [↩](#user-content-fnref-3)
4. Getting more women and people of color to study computer science is a worthy cause. Hell, getting anyone who’s historically been marginalized to study computer science is worthwhile. It’s great for our economy, and it’s currently one of the best levers for upward social mobility available. But, while we hope more companies do these things, it is not reasonable to expect that companies can be responsible for educational interventions that often need to start at the elementary school level. Of course, companies should do what they can. But expecting them to pull off mathematical impossibilities is irrational, and the DEI movement’s stalwart refusal to acknowledge the pipeline problem undermines the movement as a whole. [↩](#user-content-fnref-4)
5. I was actually able to get in touch with a former OFCCP higher-up who admitted that rejecting anonymity in hiring was against the spirit of OFCCP requirements. But they sadly wouldn’t go on the record. [↩](#user-content-fnref-5)
6. The closest we’ve ever come to doing this is our Fellowship program, where we gave free practice to engineers from traditionally underrepresented backgrounds. It was a great program, but what made it great was that our interviewers were eager to help these candidates. We were able to do free practice because our interviewers graciously agreed not to charge. That said, if I were to run this program again, I’d probably focus more on socioeconomic status and non-traditional credentials rather than just race and gender. [↩](#user-content-fnref-6)
7. Here’s the email we ended the relationship with. I’m including it because it was hard to write and even harder to hit send on, but I think we did the right thing, and maybe someone else will need to write something like this in the future… in which case, please steal our copy. ![interviewing.io termination letter to partner related to dei](https://strapi-iio.s3.us-west-2.amazonaws.com/Termination_letter_ef5d2c81ed.png) [↩](#user-content-fnref-7)
8. Research that shows that having attended a top school isn’t very predictive and that, while experience at a top company is somewhat predictive, it’s not the most important thing:

- [Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews)
   - [We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)
   - [Lessons from a year’s worth of hiring data](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data)[↩](#user-content-fnref-8)
   [↩2](#user-content-fnref-8-2)
9. Anyone who’s read my writing for a long time will pause here and wonder why I’m OK with resumes and recommending anything about reading them. Until recently, I was stalwartly against resumes and convinced that they carried no signal whatsoever. Then, as part of [the recruiter study I mentioned](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes), we built some simple ML models to judge resumes and compared their performance to human recruiters. They all outperformed recruiters, and that surprising result made me reverse my stance. [↩](#user-content-fnref-9)
10. First study (2014): <https://interviewing.io/blog/resumes-suck-heres-the-data>   
    Second study (2024): <https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes> [↩](#user-content-fnref-10)
11. There are other gaps that start way before someone gets to college. Enumerating the is out of scope of this piece, but [this writeup by the National Math and Science Initiative](https://www.nms.org/Resources/Newsroom/Blog/2023/October/Math-Science-Education-Gap-Underserved-Communities.aspx#:~:text=The%20Scope%20of%20the%20Math%20and%20Science%20Gap%20Problem&text=Similar%20gaps%20exist%20in%20science,pursue%20STEM%20majors%20and%20careers) is a good place to start. [↩](#user-content-fnref-11)


# [How many engineers have gotten laid off in 2022 and 2023 so far?](https://interviewing.io/blog/how-many-engineers-laid-off-in-2022-and-2023-so-far)

By Aline Lerner | Published: January 26, 2023; Last updated: May 6, 2024

I recently ran a Twitter poll asking my followers to estimate how many engineers had been laid off from US-based startups and tech companies in 2022 and 2023 so far.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/image_9749decedb.png)

###

As you can see, 75% of respondents thought that 50K or more engineers had been laid off, and over a third of respondents thought it was at least 100K. Only about a quarter of respondents guessed 10K or fewer.

As it turns out, most people were off by roughly an order of magnitude – the real number is likely somewhere between 7.5K and 10K… which means that the majority of people are operating on incorrect information when they make important career decisions. For instance, how might your answers change to questions like these if you knew that layoffs aren’t as pervasive as you thought?

*How do I act at work? Should I even look for a job; it probably makes sense to hunker down and keep my head down, right?? If I’m in the middle of a job search, should I take the first offer? If I just got an offer, should I even try to negotiate… won’t they rescind it if I ask for more money?*

We’ll attack some of these questions, head on, in a future post, but in this post, we’ll just share our estimate and how we got there, and hopefully we’ll dispel some of the panic around eng layoffs and help engineers make decisions based on data rather than fear.

To get to our much lower estimate, we first looked at how many people total were laid off in 2022 and 2023 so far, independent of department. Then we did some analysis to figure out how many of those people were engineers.

## How many total people got laid off in 2022 and 2023 (so far)?

To figure out how many people got laid off, we looked at layoffs.fyi. At some point in the last 2.5 years, you’ve probably visited layoffs.fyi. It was launched by Roger Lee in February 2020, just when concerns that this COVID-19 thing might affect the economy went mainstream. The site does exactly what it sounds like – it tracks layoffs at tech companies. Every time a company conducts a public round of layoffs, it gets added to a growing list. Each entry includes a layoff count, and a small subset of entries include a link to a list (usually in a Google doc) of actual people who were let go, as well as some info about them (name, LinkedIn, geography, title, and so on).

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Layoff_lists_on_layoffs_fyi_1_2ad6abbed9.png)

The meat of layoffs.fyi is a giant Airtable embedded into the site, which means you can filter the data, like so (I had to go city by city rather than just saying “United States” because many rows had the country mislabeled):

![](https://strapi-iio.s3.us-west-2.amazonaws.com/unnamed_f610896c24.png)

Once you filter on just the U.S., you get 163,296 layoffs total in 2022 and 50,263 layoffs total in 2023 so far, making for a grand total of 213,559.

We saw this estimate roughly corroborated in a [recent post by Crunchbase News](https://news.crunchbase.com/startups/tech-layoffs). They also used layoffs.fyi as a source but ended up with a more conservative estimate of 153,000.

**So, TL;DR somewhere between 150K and 200K people got laid off total**.

## How many of those people are engineers?

We recently did an analysis, where we looked at a mix of layoffs.fyi and LinkedIn data to figure out [how much layoffs affected engineers vs. other departments](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments). As it turned out, about 5% of total layoffs were engineers, as of the publication date of that post (October 2022)[1](#user-content-fn-1). I expect that this figure has held up over the past few months.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/of_total_layoffs_each_department_constituted_in_2022_68b8b4e6d6.png)

**If we put all this info together, we’re looking at something like 7.5K-10K engineers who were affected by layoffs between the start of 2022 and now, which is a far cry from the 50K-100K+ that most people had guessed.**[2](#user-content-fn-2)

While our hearts go out to all those affected, especially in cases when the layoff was a function of poor planning by the company rather than anything to do with the engineer’s performance, it’s important to put these numbers in perspective and temper the rising urge to panic with some cold, hard data.

1. If you’re curious, even though only 5% of total layoffs were engineers, about 10% of engineers were laid off. [↩](#user-content-fnref-1)
2. A few high-profile companies with layoff lists are likely outliers: Meta and Coinbase. Both of those companies had engineers comprise more than 5% of layoffs, but in both cases, the lists were primarily composed of junior engineers. That doesn’t make it any less painful for the people involved, of course, but from what we saw, eng layoffs were not generally so skewed toward junior engineers. For a really useful analysis of Meta layoffs specifically, please see this [Github repo](https://github.com/cjporteo/meta-layoffs-analysis). [↩](#user-content-fnref-2)


# [Engineers can't gauge their own interview performance. And that makes them harder to hire.](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire)

By Aline Lerner | Published: December 14, 2015; Last updated: May 25, 2023

[interviewing.io](https://interviewing.io/) is an anonymous technical interviewing platform. We started it because [resumes suck](https://blog.alinelerner.com/resumes-suck-heres-the-data/) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle. **In the past few months, we’ve amassed over 600 technical interviews along with their associated data and metadata.** Interview questions tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role at a top company, and interviewers typically come from a mix of larger companies like Google, Facebook, and Twitter, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more.

Over the course of the next few posts, we’ll be sharing some { unexpected, horrifying, amusing, ultimately encouraging } things we’ve learned. **In this blog’s heroic maiden voyage, we’ll be tackling people’s surprising inability to gauge their own interview performance and the very real implications this finding has for hiring.**

## First, a bit about setup

When an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews. If both people find each other competent and pleasant, they have the option to unmask. **Overall, interviewees tend to do quite well on the platform, with just under half of interviews resulting in a “yes” from the interviewer.**

If you’re curious, we have a few [public recordings](https://interviewing.io/mocks) of interviews done on the platform, so you can watch and see what an interview is really like. In addition to these, our feedback forms are attached below. There is one direct yes/no question, and we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of those questions is about how well they think they did. **In this post, we’ll be focusing on the technical score an interviewer gives an interviewee and the interviewee’s self-assessment (both are circled below).** For context, a technical score of 3 or above seems to be the rough cut-off for hirability.

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F49613_interviewer_feedback_2025030f47.webp%3Fupdated_at%3D2022-12-14T11%3A48%3A18.080Z&w=1920&q=75 "Feedback form for interviewers")

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F8ddad_interviewee_feedback_fd66b6f2fd.webp%3Fupdated_at%3D2022-12-14T11%3A48%3A18.267Z&w=1920&q=75 "Feedback form for interviewees")

## Perceived versus actual performance

Below, you can see the distribution of people’s actual technical performance (as rated by their interviewers) and the distribution of their perceived performance (how they rated themselves) for the same set of interviews.[1](#user-content-fn-1)

You might notice right away that there is a little bit of disparity, but things get interesting when you plot perceived vs. actual performance *for each interview*. Below, is a heatmap of the data where the darker areas represent higher interview concentration. For instance, the darkest square represents interviews where both perceived and actual performance was rated as a 3. You can hover over each square to see the exact interview count (denoted by “z”).

If you run a regression on this data[2](#user-content-fn-2), you get an R-squared of only 0.24, and once you take away the worst interviews, it drops down even further to a 0.16. For context, [R-squared](https://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) is a measurement of how well you can fit empirical data to some mathematical model. It’s on a scale from 0 to 1 with 0 meaning that everything is noise and 1 meaning that everything fits perfectly. **In other words, even though some small positive relationship between actual and perceived performance does exist, it is not a strong, predictable correspondence.**

You can also see there’s a non-trivial amount of impostor syndrome going on in the graph above, which probably comes as no surprise to anyone who’s been an engineer.

Gayle Laakmann McDowell of *Cracking the Coding Interview* fame has written quite a bit about [how bad people are at gauging their own interview performance](https://www.gayle.com/blog/2011/03/31/why-your-interview-performance-is-impossible-to-judge), and it’s something that I had noticed anecdotally when I was doing recruiting, so it was nice to see some empirical data on that front. In her writing, Gayle mentions that it’s the job of a good interviewer to make you feel like you did OK even if you bombed. I was curious about whether that’s what was going on here, but when I ran the numbers, there wasn’t any relationship between how highly an interviewer was rated overall and how off their interviewees’ self-assessments were, in one direction or the other.

Ultimately, this isn’t a big data set, and we will continue to monitor the relationship between perceived and actual performance as we host more interviews, but we did find that this relationship emerged very early on and has continued to persist with more and more interviews — R-squared has never exceeded 0.26 to date.

## Why this matters for hiring

Now here’s the actionable and kind of messed up part. As you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. **As it turns out, there’s a very statistically significant relationship** (*[p](https://en.wikipedia.org/wiki/P-value)* < 0.0008) **between whether people think they did well and whether they’d want to work with the interviewer. This means that when people think they did poorly, they may be a lot less likely to want to work with you**[3](#user-content-fn-3). And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, *despite the fact that they actually did*.

**How can one mitigate these losses? Give positive, actionable feedback immediately (or as soon as possible)!** This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they *totally* didn’t want to work there anyway.

*Lastly, a quick shout-out to [Statwing](https://www.statwing.com) and [Plotly](https://plotly.com/) for making terrific data analysis and graphing tools respectively.*

1. There are only 254 interviews represented here because not all interviews in our data set had comprehensive, mutual feedback. Moreover, we realize that raw scores don’t tell the whole story and will be focusing on standardization of these scores and the resulting rat’s nest in our next post. That said, though interviewer strictness does vary, we gate interviewers pretty heavily based on their background and experience, so the overall bar is high and comparable to what you’d find at a good company in the wild. [↩](#user-content-fnref-1)
2. Here we are referring to linear regression, and though we tried fitting a number of different curves to the data, they all sucked. [↩](#user-content-fnref-2)
3. In our data, people were 3 times less likely to want to work with their interviewers when they thought they did poorly. [↩](#user-content-fnref-3)

[Does posting Open To Work on LinkedIn help or hurt? A tale of two labor markets.](/blog/whos-open-to-work-a-tale-of-two-labor-markets)


# [How hard is it to cheat in technical interviews with ChatGPT? We ran an experiment.](https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews)

By Mike Mroczka | Published: January 30, 2024; Last updated: May 6, 2024

*Edit: This article originally contained a TikTok video of someone cheating in an interview with ChatGPT. Videos like this still can be found online, but companies hate them, so they don't stay up for long*

ChatGPT has revolutionized work as we know it. From helping small businesses automate their administrative tasks to coding entire React components for web developers, its usefulness is hard to overstate.

At interviewing.io, we've been thinking a lot about how ChatGPT will change technical interviewing. **One big question is: Does ChatGPT make it easy to cheat in interviews?** You've probably started to hear concerns about students cheating on their homework with ChatGPT, and we are certain that some people have tried to cheat in interviews with it, too!

Initial responses to cheating software have been pretty much in line with what you’d expect:

- Redditors state that “[ChatGPT is the end of coding as we know it.](https://www.reddit.com/r/singularity/comments/12zyela/chatgpt_spells_the_end_of_coding_as_we_know_it/)”
- YouTubers announce that “[Software engineering is dead. ChatGPT killed it.](https://www.youtube.com/watch?v=OeebS-VcSH0)”
- X (formerly Twitter) questions if “[​​ChatGPT spell(s) the end for Coding Interviews?](https://twitter.com/intx_podcast/status/1635396953109561344)”

It seems clear that ChatGPT can assist people during their interviews, but we wanted to know:

- How much can it help?
- How easy is it to cheat (and get away with it)?
- Will companies that ask LeetCode questions need to make significant changes to their interview process?

To answer these questions, we recruited some of our professional interviewers and users for a cheating experiment! Below, we’ll share everything we discovered and explain what it means for you. As a little preview, just know this: companies need to change the types of interview questions they are asking—immediately.

interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. We have thousands of professional interviewers in our ecosystem, and hundreds of thousands of engineers have used our platform to prepare for interviews.[1](#user-content-fn-1)

### Interviewers

Interviewers came from our pool of professional interviewers. They were broken into three groups, with each group asking a different type of question. **The interviewers had no idea that the experiment was about ChatGPT or cheating; we told them that "[this] research study aims to understand the trends in the predictability of an interviewer’s decisions over time – especially when asking standard vs. non-standard interview questions."**

These were the three question types:

1. **Verbatim LeetCode questions**: questions pulled directly from LeetCode at the interviewer's discretion with no modifications to the question.

Example: The [Sort Colors](https://leetcode.com/problems/sort-colors/) LeetCode question is asked exactly as it is written.

2. **Modified LeetCode questions**: questions pulled from LeetCode and then modified to be similar to the original but still notably different from it.

Example: The [Sort Colors](https://leetcode.com/problems/sort-colors/) question above but modified to have four integers (0,1,2,3) instead of just three integers (0,1,2) in the input.

3. **Custom questions**: questions that aren’t directly tied to any question that exists online.

Example: You are given a log file with the following format:
- `<username>: <text> - <contribution score>`
- Your task is to identify the user who represents the median level of engagement in a conversation. Only consider users with a contribution score greater than 50%. Assume that the number of such users is odd, and you need to find the one right in the middle when sorted by their contribution scores. Given the file below, the correct answer is SyntaxSorcerer.

```
LOG FILE START
NullPointerNinja: "who's going to the event tomorrow night?" - 100%
LambdaLancer: "wat?" - 5%
NullPointerNinja: "the event which is on 123 avenue!" - 100%
SyntaxSorcerer: "I'm coming! I'll bring chips!" - 80%
SyntaxSorcerer: "and something to drink!" - 80%
LambdaLancer: "I can't make it" - 25%
LambdaLancer: "🙁" - 25%
LambdaLancer: "I really wanted to come too!" - 25%
BitwiseBard: "I'll be there!" - 25%
CodeMystic: "me too and I'll brink some dip" - 75%
LOG FILE END

For more information about question types and about how we designed this experiment, please read the [Interviewer Experiment Guidelines](https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit) doc that we shared with participating interviewers.

### Interviewees

Interviewees came from our pool of active users and were invited to participate in a short survey. We selected interviewees who:

- Were actively looking for a job in today's market
- Had 4+ years of experience and were applying to senior-level positions
- Rated their “ChatGPT while coding” familiarity as moderate to high
- Identified themselves as someone who thought they could cheat in an interview without getting caught

This selection helped us skew the candidates toward people who could feasibly cheat in an interview, had the motivation to do so, and were already reasonably familiar with ChatGPT and coding interviews.

**We told interviewees that they had to use ChatGPT in the interview, and the goal was to test their ability to cheat with ChatGPT.** They were also told not to try to pass the interview with their own skills — the point was to rely on ChatGPT.

We ended up conducting 37 interviews overall, 32 of which we were able to use (we had to remove 5 because participants didn’t follow directions):

- 11 with the “verbatim” treatment
- 9 with the “modified” treatment
- 12 with the “custom” treatment

A quick disclaimer. Because our platform allows for anonymity, our interviews have audio but no video. We’re anonymous because we want to create a safe space for our users to fail and learn quickly without judgment. It’s great for our users, but we acknowledge that not having video in these interviews makes our experiment less realistic. In a real interview, you will be on camera with a job on the line, which makes cheating harder — but does not eliminate it.

After the interviews, both interviewers and interviewees had to complete an exit survey. We asked interviewees about the difficulties of using ChatGPT during the interview, and interviewers were given multiple chances to express concerns about the interview — we wanted to see how many interviewers would flag their interviews as problematic and report that they suspected cheating.

![Post-survey interviewee questions](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewee_Collage_Survey_88a5b0a095.jpg&w=1200&q=75 "Post-survey interviewee questions")

Post-survey interviewee questions

![Post-survey interviewer questions](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewer_Collage_Survey_a0e0cf80fb.jpg&w=1200&q=75 "Post-survey interviewer questions")

Post-survey interviewer questions

We had no idea what would happen in this experiment, but we assumed that if half the candidates that cheated got away with it and passed the interview, it would be a telling result for our industry.

After removing interviews where participants did not follow instructions[2](#user-content-fn-2), we got the following results. Our control was how candidates performed in interviewing.io mock interviews outside the study: 53%.[3](#user-content-fn-3) Note that most mock interviews on our platform are LeetCode-style questions, which makes sense because that's primarily what FAANG companies ask. We'll come back to this in a moment.

![Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FPass_rate_by_question_type_when_trying_to_cheat_with_Chat_GPT_c20c87712e.png&w=1920&q=75 "Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%")

'Verbatim' questions passed significantly more often, compared to both our platform average and to 'custom' questions. 'Verbatim' and 'modified' questions were not statistically significantly differnt from each other. 'Custom' questions had a significantly lower pass rate than any of the other groups.

### “Verbatim” questions

Predictably, the verbatim group performed the best, passing 73% of their interviews. Interviewees reported that they got the perfect solution from ChatGPT.

The most notable comment from the post-interview survey for this group is below — we think it is particularly telling of what was going on in many of the interviewers’ minds:

> *“It's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before. Normally, I add 1-2 unique twists to the problem to ascertain the difference.”*

Normally, this interviewer would have followed up with a modified question to get more signal, so let’s examine the “modified” group next to see if interviewers actually got more signal by adding a twist to their questions.

### “Modified” questions

Remember, this group may have had a LeetCode question given to them, which was standard but modified in a way that was not directly available online. This means ChatGPT couldn’t have had a direct answer to this question. Hence, the interviewees were much more dependent on ChatGPT's actual problem-solving abilities than its ability to regurgitate LeetCode tutorials.

As predicted, the results for this group weren’t too different from the “verbatim” group, with 67% of candidates passing their interviews. As it turns out, this difference was not statistically significantly different from the "verbatim" group, i.e., “modified” and “verbatim” are essentially the same. This result suggests that ChatGPT can handle minor modifications to questions without much trouble. Interviewees did notice, however, that it took more prompting to get ChatGPT to solve the modified questions. As one of our interviewees said:

> *“Questions that are lifted directly from LeetCode were no problem at all. A follow-up question that was not so much directly LeetCode-style was much harder to get ChatGPT to answer.”*

### “Custom” questions

As expected, the “custom” question group had the lowest pass rate, with only 25% of candidates passing. **Not only is it statistically significantly smaller than the other two treatment groups, it's significantly lower than the control! When you ask candidates fully custom questions, they perform worse than they do when they're not cheating (and getting asked LeetCode-style questions)!**

Note that this number, when initially calculated, was marginally higher, but after reviewing the custom questions in detail, we discovered a problem with this question type we hadn’t anticipated, which had skewed the results minorly toward a higher pass rate. Read the section below called "Companies: Change the questions you are asking immediately!" to find out what that problem was.

## No one was caught cheating!

In our experiment, interviewers were not aware that the interviewees were being asked to cheat. As you recall, after each interview, we had interviewers complete a survey in which they had to describe how confident they were in their assessments of candidates.

**Interviewer confidence in the correctness of their assessments was high, with 72% saying they were confident in their hiring decision.** One interviewer felt so strongly about an interviewee's performance that they concluded we should invite them to be an interviewer on the platform!

> *“The candidate performed very well and demonstrated knowledge of a strong Amazon L6 (Google L5) SWE... and could also be considered to be an interviewer/mentor on interviewing.io.”*

That is a lot of confidence after just one interview — probably too much!

We’ve long known that [engineers are bad at gauging their own performance](https://interviewing.io/blog/own-interview-performance), so perhaps it shouldn’t come as a shock to find that interviewers also overestimate the effectiveness of the questions that they ask.

Of the interviewers who were not confident in their hiring choice (28%), we asked them why. This was the frequency distribution of their reasons.

![Interviewers weren't confident in their assessment because the candidate gave suboptimal solutions, missed edge cases, had messy code, was inarticulate, or occasionally had other niche issues, but no interviewer was called out cheating as a concern](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewer_confidence_61b40b9ce1.png&w=1920&q=75 "Note that cheating isn’t mentioned anywhere!")

Note that cheating isn’t mentioned anywhere!

Most interviewers concerned about their hiring decision expressed specific reasons for their lack of confidence. These issues typically included suboptimal solutions, missed edge cases, messy code, or poor communication. We specifically included an “Other Issue” category to see if they would express a concern that the interviewee was cheating, but digging deeper revealed only minor grievances like “personality issues” and “they need to speed up their coding.”

In addition to having this opportunity to call out cheating, interviewers were prompted three additional times to note any other concerns they had with the interview, including free-form text boxes and several multiple-choice questions with options to explain their concerns.

When an interviewee bombed because they didn’t understand the ChatGPT response, the interviewer chalked up the interviewee’s odd behavior and stilted responses to a lack of practice — not cheating. One interviewer thought the candidate's problem-solving was fine but commented that they were slow and needed to consider edge cases more carefully.

> *“Candidate did not seem prepared for any LeetCode questions."*

> *“Candidate's approach lacked clarity, and they jumped into the coding too early.”*

> *“The candidate was not prepared to tackle even the most basic coding questions on LeetCode.”*

> *“Good problem solving in general, but the candidate needs to be faster at coding and identifying critical edge cases.”*

So, who reported concerns about cheating? And who got caught?

**As it turns out, not a single interviewer mentioned concerns about any of the candidates cheating!**

We were stunned to discover that interviewers reported no suspicions of cheating, and interestingly, interviewees were largely confident that they were getting away with it, too. 81% reported no concerns about being caught, 13% thought they might have tipped off the interviewer, and an astonishingly small 6% of participants thought the interviewer suspected them of cheating.

![81% of interviewees were not worried, 13% were slightly worried, and just 6% were very worried about being caught](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FInterviewee_worry_level_6d1445e631.png&w=1920&q=75 "Interviewees were mostly sure that their cheating went undetected")

Interviewees were mostly sure that their cheating went undetected

The candidates who worried they were caught did have abnormal comments from the interviewers in the post-survey analysis, but they still were not suspected of cheating. **To summarize, most candidates thought they were getting away with cheating — and they were right!**

## Companies: Change the questions you are asking immediately!

The obvious conclusion from these results is that **companies need to start asking custom questions immediately, or they are at serious risk of candidates cheating during interviews (and ultimately not getting useful signal from their interviews)!**

ChatGPT has made verbatim questions obsolete; anyone relying on them will be naively leaving their hiring processes up to chance. Hiring is [already tricky enough](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring) without worrying about cheating. If you’re part of a company that uses verbatim LeetCode questions, please share this post internally!

Using custom questions isn’t just a good way to prevent cheating. It filters out candidates who have memorized a bunch of LeetCode solutions (as you saw, our custom question pass rate was significantly lower than our control). It also meaningfully improves candidate experience, which makes people way more likely to want to work for you. A while ago, we did an [analysis of what makes good interviewers good](https://interviewing.io/blog/best-technical-interviews-common). Not surprisingly, asking good questions was one of the hallmarks, and our best-rated interviewers were the ones who tended to ask custom questions! Question quality was extremely significant in our study, regarding whether the candidate wanted to move forward with the company. It was much more important than the company’s brand strength, which mattered for getting candidates in the door but didn’t matter relative to question quality once they were in process.

As some of our interviewees said…

> *“Always nice to get questions that are more than just plain algorithms.”*

> *“I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted], which made it feel less like a toy problem and more like a pared-down version of a real problem.”*

> *“This is my favorite question that I’ve encountered on this site. It was one of the only ones that seemed to have real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.”*

One more somewhat subtle piece of advice for companies who decide to move to more custom questions. You might be tempted to take verbatim LeetCode questions and change up the wording or some of the window dressing. That makes sense, because it’s certainly easier than coming up with questions from scratch. Unfortunately, that doesn’t work.

As we mentioned earlier, we discovered in this experiment that just because a question looks like a custom question, doesn’t mean it is one. Questions can appear custom and still be identical to an existing LeetCode question. **When making questions to ask candidates, it isn’t enough to obscure an existing problem.** You need to ensure that the problem has unique inputs and outputs to be effective at stopping ChatGPT from recognizing it!

The questions that interviewers ask are confidential, and we cannot share the exact questions that our interviewers used in the experiment. However, we can give you an indicative example. Below is a “custom question” with this critical flaw, which is easy for ChatGPT to beat:

```
For her birthday, Mia received a mysterious box containing numbered cards 
and a note saying, "Combine two cards that add up to 18 to unlock your gift!" 
Help Mia find the right pair of cards to reveal her surprise.

Input: An array of integers (the numbers on the cards), and the target sum (18). 
arr = [1, 3, 5, 10, 8], target = 18

Output: The indices of the two cards that add up to the target sum. 
In this case, [3, 4] because index 3 and 4 add to 18 (10+8).

Did you spot the issue? While this question appears “custom” at first glance, its objective is identical to the popular [TwoSum](https://leetcode.com/problems/two-sum/) question: finding two numbers that sum to a given target. The inputs and outputs are identical; the only thing “custom” about the question is the story added to the problem.

Seeing that this is identical to known problems, it shouldn’t be a surprise to learn that ChatGPT does well on questions that have inputs and outputs identical to existing known problems — even when they have a unique story added to them.

### How to actually create good custom questions

One thing we’ve found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note. These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune a lot of the details and distill the essence of the problem into something that doesn’t take the candidate a lot of work/setup to grok. You’ll also likely have to iterate on these home-grown questions a few times before you get them right — but the payoff can be huge.

To be clear, we’re not advocating the removal of data structures and algorithms from technical interviews. DS&A questions have gotten a bad reputation because of bad, unengaged interviewers and because of companies lazily rehashing LeetCode problems, many of them bad, which have nothing to do with their work. In the hands of good interviewers, those questions are powerful and useful. If you use the approach above, you’ll be able to come up with new data structure & algorithmic questions that have a practical foundation and component that will engage candidates and get them excited about the work you’re doing.

You’ll also be moving our industry forward. It’s not OK that memorizing a bunch of LeetCode questions gives candidates an edge in today’s interview process, nor is it OK that interviews have gotten to a state where cheating starts to look like a rational choice. The solution is more work on the employer’s part to come up with better questions. Let’s all do it together.

## Real talk for job seekers

All right, now, for all of you who are actively looking for work, listen up! Yes, a subset of your peers will now be using ChatGPT to cheat in interviews, and at companies that ask LeetCode questions (sadly, many of them), those peers will have an edge… for a short while.

**Right now, we’re in a liminal state where companies’ processes have not caught up to reality. They will, soon enough, either by moving away from using verbatim LeetCode questions entirely (which will be a boon for our entire industry) or by returning to in-person onsites (which will make cheating largely impossible past the technical screen) or both.**

It sucks that other candidates cheating is another thing to worry about in an [already difficult climate](https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews), but we cannot, in good conscience, endorse cheating to “level the playing field.”

In addition, interviewees who used ChatGPT uniformly reported how much more difficult the interview was to complete while juggling the AI.

Below, you can view one interviewee stumbling through their time complexity analysis after giving a perfect answer to an interview question. The interviewer is confused as the interviewee scrambles to explain how they got to their incorrect time complexity (secretly provided by ChatGPT).

While no one was caught during the study, their cameras were off, and cheating was still difficult for many of our skilled candidates, as evidenced by this clip.

Ethics aside, cheating is difficult, stressful, and not entirely straightforward to implement. Instead, we advise investing that effort into practice, which will serve you well once companies change their processes, which hopefully should be soon. Ultimately, we hope the advent of ChatGPT will be the catalyst that finally moves our industry’s interview standards away from grinding and memorizing to actually testing engineering ability.

*Special thanks to Dwight Gunning and Liz Graves for their help with this experiment. And of course a big thank you to all the interviewees and interviewers who participated!*

1. To be an interviewer on our platform, you have to have at least 4 years of experience and have conducted at least 20 interviews on behalf of a FAANG or FAANG-adjacent company). [↩](#user-content-fnref-1)
2. Five interviews needed to be removed because they did not meaningfully use ChatGPT. In two instances, the interviewee was familiar with the question and chose to solve the problem themselves. In one interview, the interviewee wanted to just try the question on their own and didn't prompt ChatGPT, ignoring our instructions. The last two interviews were "custom" interview questions that were problematic for reasons we’ll outline later in this article. [↩](#user-content-fnref-2)
3. This is a higher passthrough rate than you'd see in the wild. We think it comes down to two factors: selection bias and pre-interview prep. The users who invest in interview prep are a specific, self-selected slice of the population. Moreover, many of our users practice on their own before practicing with a human. [↩](#user-content-fnref-3)


# [A founder’s guide to making your first recruiting hire](https://interviewing.io/blog/a-founders-guide-to-making-your-first-recruiting-hire)

By Aline Lerner | Published: April 25, 2016; Last updated: May 1, 2023

Recently, a number of founder friends have asked me about how to approach their first recruiting hire, and I’ve found myself repeating the same stuff over and over again. Below are some of my most salient thoughts on the subject. Note that I’ll be talking a lot about engineering hiring because that’s what I know, but I expect a lot of this applies to other fields as well, especially ones where the demand for labor outstrips supply.

## Don’t get caught up by flashy employment history; hustle trumps brands

At first glance, hiring someone who’s done recruiting for highly successful tech giants seems like a no-brainer. Google and Facebook are good at hiring great engineers, right? So why not bring in someone who’s had a hand in that success?

There are a couple of reasons why hiring straight out of the Googles and Facebooks of the world isn’t necessarily the best idea. First off, if you look at a typical recruiter’s employment history, you’re going to see a lot of short stints. Very likely this means that they were working as a contractor. While there’s nothing wrong with contract recruiting, per se, large companies often hire contract recruiters in batches, convert the best performers to full-time hires, and ditch the rest.[1](#user-content-fn-1) That said, some of the best recruiters I know started out at Google. But I am inclined to believe they are exceptions.

The second and much more important reason not to blindly hire out of tech giants is the importance of scrappiness and hustle in this hire. **If you work as a recruiter at Google, you’re basically plugged into the matrix. You have a readymade suite of tools that make it much easier to be successful. You have a database of candidates who have previously interviewed that spans a huge portion of the engineering population. Email discovery is easier. Reaching out to people is easier because you have templates that have been proven to work to rely on. And you can lean on the Google brand as a crutch.** Who hasn’t been, at one point in their career, flattered by an email from a Google recruiter? As a result, if you’re sending these emails, you don’t have to go out of your way to write personal messages or to convince people that your company is cool and interesting and worth their time. You get that trust for free.

Contrast this setup with being *the very first person* in the recruiting org. You have no tools. You have no templates. You probably have no brand. You probably have, well, jack shit. You need someone who’s going to think critically about tooling and balance the need for tooling with a shoestring budget, especially in a space where most tooling has a price tag of at least $1K per month. You’re going to need someone whose methods are right for your particular situation rather than someone who does things because that’s just how they’ve always been done. You probably want someone who realizes that paying for a LinkedIn Recruiter seat is a huge fucking waste of money and that sourcing on LinkedIn, in general, is a black hole-level time suck. You want someone who is good at engaging with candidates independently of brand sparkle, which likely means someone who understands the value of personalization in their sourcing efforts. You want someone who compensates for your relatively unknown status with great candidate experience during your interview process. You want someone who won’t just blindly pay tens of thousands of dollars for career fair real estate because that’s just what you do, even though the only companies who get ROI on career fair attendance are ones with preexisting brands. And, apropos, you want someone who can start building a sparkly brand for you from day one because building a brand takes time. (More on brand-building in the last two sections on marketing chops and evangelism.)

## Sales chops are hugely important, and you can test for those

People often ask me if having an engineering background is important for technical recruiters. My answer to that is always, “Yes, but.” Yes, it’s useful, but the main reason it’s useful is that it helps build credibility and rapport with candidates. A good salesperson can do that without all the trappings of engineering experience. **To put it another way, at the end of the day, this is a sales job. Great engineers who are shitty salespeople will not do well at recruiting. Great salespeople with no engineering background will likely do well.**

So, how can you test for sales aptitude? If the candidate is currently an in-house recruiter somewhere, I ask them to pitch me on the company’s product. If they’re an agency recruiter, I ask them to pitch me on one of their clients’ products. Most recruiters do a decent job of pitching the company as a good place to work, but unfortunately, many don’t have a very good understanding of what their company actually does. Given that they’re the first point of contact for candidates, it’s really important to be able to answer basic questions about product-market fit, challenges (both product and engineering), how the company makes money, how much traction there is, what the competition looks like, and so on. Moreover, a lack of interest in something this basic points to a lack of intellectual curiosity in general, and in a technical recruiter, this is a very poor sign because such a huge portion of the job is picking up new concepts and being able to talk about them intelligently to very smart people.

## You want someone who can write

I was on the fence about whether to include this section because it sounds kind of obvious, but writing well is important in this role for two reasons. **First off, your recruiter is likely going to be the first point of contact with candidates. And if you’re an early-ish company without much brand, correspondence with the recruiter will likely be the first time a candidate ever hears of you.** So, you probably want that interaction to shine. And the other reason you want someone who cares about narrative, spelling, and grammar is that they will be the arbiter of these abilities in future recruiting hires. Enough said.

One exercise I like to have candidates for this role go through is writing mock sourcing emails to people at your company, as if they were still at their previous position. This portion of the interview process is probably the best lens into what it’s actually like to work with the candidate. In particular, because candidates are not likely to have a clear idea of what they’re pitching yet, I try to make this part of the process iterative and emphasize that I welcome any number of questions about anything, whether it’s the company’s offering, what companies my firm works with, what certain parts of the engineers’ profiles mean, or anything in between. What questions people ask, how they ask them, and how they deal with the ambiguity inherent in this assignment is part of the evaluation, as is the caliber of the research they did on each mock email recipient.

## You want someone with marketing chops

I talked a bit earlier about how you probably have no brand to speak of at this point. I can’t stress enough how much easier having a brand makes hiring. Until you have one, especially in this climate, you’re going to be fighting so fucking hard for every one-off hire. If you can, you ought to put effort into branding such that you end up in the enviable position of smart people coming to you.

So why don’t early-ish companies do this across the board? **Brand building is a pain in the ass, it takes time, and not all of your outbound efforts are going to be measurable, which can make it harder to get others in your org to buy in.** If you can find someone who’s had even a little bit of marketing experience, they’ll be able to identify channels to get the word out, use their preexisting network to help with outsource-able tasks, and get the ball rolling on things like hosting events, which, if you’ve never done before, can be quite intimidating.

And because recruiting doesn’t live in a vacuum and needs help from other teams to send something high-signal and genuine into the world, someone with some marketing experience will likely have an easier time getting other teams to buy in and put time and resources into this endeavor, which brings me to my next point.

## You want someone who can fearlessly evangelize the importance of recruiting… and get you to take an active role even when you don’t feel like it

The harsh reality is that the primary reason companies hire their first recruiter is so that hiring can be taken off the plate of the founders. It’s tempting to have the “set it and forget it” mentality in a founder’s shoes — recruiters aren’t cheap, so presumably if you pay them enough, they’ll just deal with this pesky hiring thing, and then you can get back to work. I get it. Hiring isn’t that fun, and as a founder, despite having been a recruiter myself, there are definitely days when I just want to pay someone to, for the love of god, take this bullshit off my hands so I can get back to talking to users and selling and figuring out what to build next and all sorts of other things.

But it doesn’t work that way. If you’re a founder, no one can sell your vision as well as you. And all that experience you’ve built up that makes you a subject matter expert probably also makes you pretty good at filtering candidates. You might take a lot of what’s in your head for granted, but transferring that over into someone else’s brain is going to take time and iteration. And you can never really dissociate from hiring entirely because the moment you do, the culture of “hiring is just the domain of recruiting” is going to trickle down into your culture, and over time, it will cost you the best people.

In my recruiting days, at a high level, I saw two types of hiring cultures. One had the hiring managers and teams taking an active role, participating in sourcing, tweaking interview questions to make them engaging and reflective of the work, and taking time to hang out with candidates, even if they weren’t interviewing yet. The other type had the recruiting org be largely disjoint from the teams it was hiring for. In this type of setup, team members would view recruiting as a hassle/necessary evil that took them away from their regular job, and most of the remaining trappings of the hiring process would be left in the hands of recruiters alone.

**You can guess which type of company ends up with an enviable interview process, a sweet blog, cool, hiring-themed easter eggs in their code, and a wistful, pervading, nose-pressed-against-the-glass refrain of “I wish I could work there”. And you can, in turn, guess which company demands a CS degree and 10 years of [insert recent language name here] experience in their job descriptions.**

Despite these realities, founders and hiring managers often forget how critical their role in hiring is because they have a ton of everyday tasks on their plates. This is why having your recruiter be a fearless evangelist is so important. This person needs to cheerfully yet insistently remind the team and especially founders (who are, after all, the ones who dictate culture) that time spent on hiring is part of their jobs. This person needs to be able to march into the CEO’s office and demand that they go and give a talk somewhere or consistently block off time on their calendar every week to send some sourcing emails. Or that they need to write some stuff somewhere on the internet such that people start to realize that their company is a thing. Marching into a CEO’s office and making demands is tough. You need a person who will do this without trepidation and who will be able to convince you, even when the sky is falling, that a few hours a week spent on hiring are a good use of your time.

In addition to these points, all the usual thought points about hiring someone who’s going to be growing a team apply here. Is this person already a strong leader? If not, can they grow into one? Are they going to be able to attract other talent to their team? Are they someone you want around, fighting alongside you in the dark, for a long time to come? And, though in an ideal world I’d choose someone with experience who also meets the criteria I’ve outlined in this guide, if ultimately faced with a choice between experience and someone green with hustle, charisma, writing ability, and smarts, I’ll choose the latter every time.

1. As an aside, this process is an unfortunate side effect of employment law meant to protect contractors from being exploited. The thinking is that by capping the length of time that someone can work as a contractor, you can exert pressure on the company to turn them into full-time hires who have to be given benefits. But as with many well-intentioned, regulatory pieces of legislation, that’s not really what happens in practice. The practical takeaway, though, is that if someone is great at recruiting, they’re probably not going to have a bunch of short contracting stints. [↩](#user-content-fnref-1)


# [How to write (actually) good job descriptions](https://interviewing.io/blog/how-to-write-good-job-descriptions)

By Aline Lerner | Published: September 22, 2021; Last updated: May 1, 2023

We’ve been doing a lot of hiring at interviewing.io recently, which means we’ve been writing a lot of job descriptions. We’ve written before about [how important good copy is and how it can elevate less well-known brands](https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you), but the exercise of having to write job descriptions for ourselves made me realize that giving advice is a lot easier than following it and that even we, who make a living from hiring, tend to fall into some of the same traps that we tell others to avoid.

So I thought I’d write this guide to keep us honest and to hopefully say some useful stuff to others undertaking this somewhat herculean task.

Before I get into the practical do’s and don’ts, a bit of theory.

## How to avoid the “audience paradox”

When you start writing a job description, the first question you should ask yourself is, *Am I trying to attract the right people, or am I trying to keep the wrong people out?*

Then, once you answer it, write for that audience deliberately, because it’s really hard to write for both (if you try, you’ll find that your job description will end up being way too long, and then you’ll attract… neither!)

Why is it hard to write for both? When you’re trying to attract talent, your tone is going to be different. You’re going to focus on cool stuff about the company, projects they’re going to do, the impact they’ll have, initiatives and outcomes that they’re going to own.

When you try to keep people out, the focus tends to be more on a long laundry list of requirements.

Most job descriptions that I’ve seen tend to focus on keeping the wrong people out, though I doubt that doing so was a deliberate choice — when people write these, it’s generally their first instinct to optimize for less noise. Why? Because when you post job descriptions, you inevitably get a bunch of unqualified candidates in. If you’re spending any meaningful time on hiring, others are too, and you’re probably in a hot market. In these markets, more and more unqualified candidates try to get in because salaries go up.

At the same time, in hot markets, fewer and fewer qualified candidates apply for jobs because companies start going after them (this is why sourcers and recruiters exist).

The paradox here is that, despite the noise, writing job descriptions that aim to reduce it will actually get you *fewer* qualified candidates (and will not cut the unqualified ones).

Unqualified candidates have nothing to lose by applying to you, and no amount of bullet points will keep them out. On the other hand, writing job descriptions that, first and foremost, get people excited might actually get you some good people. Below is a succinct way of putting it.

![Chart: the audience paradox](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fjob_descriptions_graph_9d898422e8.webp&w=1200&q=75 "The Audience Paradox")

So, no matter what, your goal should be to attract talent rather than worrying about the noise.

## If they mostly attract noise, why even write job descriptions?

It’s true that in hot markets, most hires don’t come from inbound applicants. However, writing good ones still matters. Here’s why:

1. They help you collect your thoughts and align internally on what you need
2. They help you figure out how you’re going to talk about the job to candidates
3. Many candidates who come from outbound sources (i.e. you go out to get them rather than them coming to you) will still want to see the job description so they can have something concrete to reference when they’re trying to grok the job

## The anatomy of a good job description

### An “About us” section

*(1 paragraph)* Come up with a good general company summary. In a previous post, we listed some [exercises that will help you distill what makes your company special](https://interviewing.io/blog/3-exercises-to-create-the-kind-of-employer-brand-that-actually-makes-engineers-want-to-work-for-you).

*(1 paragraph)* Contextualize the above for the specific team/department you’re hiring for. In other words, how does this specific team/department fit in to what you’re working on as a whole? What has made that department/team special, unique, or impactful til now? For instance, if your eng team is punching above their weight because it’s a small team producing outsized results, mention that. If you’re using some programming language that has a big community around it or you’re an early adopter of a cool language, mention that. If this is a design job description, and you’re a design-first culture or the founder is a designer, mention that. And so on.

### An “About you & what you’ll do here” section

#### About you

List your requirements, but only list actual objective deal-breakers. People tend to get really carried away in this section and come up with huge laundry lists full of subjective things like “Follows coding best practices”. Imagine someone reading the job description, and put yourself in their shoes. Will you really weed someone out because reading that bullet will be the watershed moment in their life, where they honestly admit that they have not, in fact, been following best practices? Remember, the point of a job description is to sell good people, not keep out bad ones! Do you think that putting a bullet about following best practices is going to get someone excited to work for you? Probably not. Following best practices is table stakes, as are most other requirements of that ilk.

Here are examples of good bullets. Note that they’re specific to the role at hand and that not being able to do them would actually be a dealbreaker for the role in question:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_09_2x_e733d9c740.png)

And here are examples of bad bullets (pulled from real job descriptions). Note that these are bad not because they’re not true (they are!) but because they won’t keep anyone out and just create noise and detract attention away from the parts that attract the best people:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_14_2x_b8f55142ca.png)

#### What you’ll do here

List all the cool things that this person will get to do and work on. This is really important. Do NOT put generic things like “Will work with key stakeholders to drive KPIs”. Put very specific things that will excite the best candidates. The purpose is to get the best people excited, nothing more.

Here are examples of good bullets:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_34_2x_d448bfa95d.png)

And here are examples of bad bullets (pulled from real job descriptions). Again, these are bad not because they’re untrue but because they’re generic and unmemorable. A good litmus test is, “Can most companies say this about the work they’re doing?”. Below you can see bullets that pertain to almost every eng role ever:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_17_09_42_2x_6b1b3cfd8e.png)

### [Two Sum](/questions/two-sum)

[Given an array of integers, return the indices of the two numbers that add up to a given target.](/questions/two-sum)


# [I’ve never written code, but I just passed a Google system design interview.](https://interviewing.io/blog/never-written-code-but-passed-google-system-design)

By Kevin Landucci | Published: August 9, 2023; Last updated: August 31, 2023

## README and some disclaimers

*Before you read this post, I (Aline Lerner, founder of interviewing.io) wanted to introduce it properly. It’s about something that’s a little bit nuts: a non-engineer passed a Google system design interview! What??! Surely something is misrepresented here, and surely something is overblown.*

*Here are the facts and some key context:*

- *interviewing.io is an anonymous mock interview platform.*
- *We have hundreds of interviewers who are (or have recently been) senior, staff, and principal engineers at FAANG.*
- *We pay these interviewers to conduct anonymous mock interviews in the styles of their companies, and we instruct them to use at least the same bar that they would use in real life.*
- *We instruct our interviewers to never use proprietary questions or do anything that would jeopardize their NDAs with these companies.*
- *The Google interview in this post is a mock interview in the style of Google, conducted by a current or former Googler on our platform (we won’t say anything more about the interviewer to protect their identity, and we’ve also changed their voice). The interviewer didn’t have any context and thought this was a regular mock interview, and we stand behind its realism.*
- *To corroborate the results, we had a few independent Google interviewers evaluate the interview replay without knowing the context. Most interviewers said the question was realistic. One said that the question was too hard for an L4 candidate. Several said that the interviewer was too lenient, and one said that the candidate squeaked by in part because he made the interviewer believe he was already in process for an L5 role. One said the feedback was on point. We didn’t get consensus… but that somewhat tracks real life, where interviews are messy and outcomes are not deterministic.*
- *Even if the interviewer was a bit lenient, the fact that a non-engineer could pass or get close to passing a system design interview is nuts. It’s a testament to how teachable this material is, and it’s also a commentary on how these interviews are flawed and can be gamed, with enough effort. We are acutely aware of the shortcomings of the status quo and want to see a revolution in our industry as much as you do.*

*At the bottom of this post, you can watch the full interview and judge for yourself.*

My name is Kevin. I am not and have never been a software engineer. I have never written or tested a single line of code, and I have never even worked as a PM. My tech career started in (don’t hate me!) recruiting. Then moved into career coaching and salary negotiation before I got where I am now: creating content for engineers.

With a fraction of the domain knowledge most engineers have, I passed a Google system design interview. I passed because I learned a LOT about system design interviews before taking this interview. I also learned a few tricks along the way. **If these tricks helped *me* pass, then imagine what you’ll be able to do with them**.

## How a non-coder learned a LOT about system design interviews

At interviewing.io, part of my job is talking to candidates about the interview process. Many complain about system design interviews. They don’t know how to approach them or how to handle the ambiguity. Here’s what one of our users had to say:

> *System design interviews are very intimidating. The de facto assumption is that if you have worked for 5+ years, you should have excellent scalable systems experience and if you fail to show that at interviews, you're penalized either by not getting the offer or by getting down leveled significantly. The analogy I can think of is like asking an elephant to fly when the job description is to lift and move heavy things.*

At the time, the most popular system design resources were eerily similar. None of them had good solutions for approaching, handling ambiguity in, or demystifying system design interviews. So, we decided to try to make the best system design guide in the world, and I got to lead the project.

First, I talked to some of our highest-rated system design interviewers. Then, we started work on the guide. My role was to facilitate, listen, project manage, write, edit, and repeat.

After months of work, we published [A Senior Engineer’s Guide to the System Design Interview](https://interviewing.io/guides/system-design-interview) on March 2nd, 2023. [It did well on Hacker News](https://news.ycombinator.com/item?id=34999464). If you Google “system design interview”, it’s the #1 result (as of when this post was published).

## Why I decided to try doing a system design interview myself

After the guide was released, I met again with some of the contributors. Out of nowhere, one said, “I bet you could pass a system design interview.” The other two agreed. I accepted the challenge and decided I wouldn’t study; I’d just use what I learned from making the guide.

I scheduled a mock system design interview with an interviewer from Google on interviewing.io for a Friday morning. My interviewer wouldn’t know who I was and would only know my years of experience (I said I had 4). The only other thing my interviewer would know about me was that I presumably was about to interview at Google and needed help preparing.

![How to book Google system design interviews on interviewing.io](https://strapi-iio.s3.us-west-2.amazonaws.com/sd_g_67800b6ebb.png)

How to book Google system design interviews on interviewing.io

## Selected highlights from my system design interview

I’ll walk you through a few clips from the interview that cover three key aspects of my experience: an unconventional idea, something I failed spectacularly at, and a moment I’m proud of.

### An unconventional idea

Last year, I went to Harvard Law School’s Advanced Negotiation Workshop. They spend a lot of time (33% of the whole workshop!) drilling into the specifics of how negotiations are often won or lost in the first and last 3 minutes. I approached the first 180 seconds of this system design interview with that in mind, and I knew I had to quickly score some social proof with the interviewer.

[The unwritten codes of FAANG interviews](https://interviewing.io/guides/hiring-process) taught me one unobvious way to score social proof. Win the competition about who cares less. **I’m naturally enthusiastic so I had to trick myself. To find that devil-may-care attitude, I’d repeat a lyric from Green Day in my head: *“I don’t care that you don’t care.”***

### Something I failed spectacularly at

Simply put, I forgot you could cache data on a user’s device. More importantly, they grilled me about caching and I took it personally. I lost sight of the interview. Caught in a story of them as an adversary, I stopped listening. I ignored their follow-up questions and, ironically, their hints.

As Marcellus Wallace said in Pulp Fiction, "Pride only hurts." I didn’t know the answers to their caching questions, and I was too proud to say “I don’t know.” Had I done that, they would’ve shifted gears. But I didn’t, so they assumed I actually did know, then the grilling continued. If I could go back in time, I’d do what our system design guide says to do when you’re not certain, but you have an idea.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/certainty_diagram_27234de7d3.webp)

Tell the interviewer:

> *"I don't know, I'm definitely going to look that up right after this interview, but if I had to give my best guess I'd say... [x] and here is why [explanation/thought process]."*

In this clip, I get flustered, yell, and get a hint. Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is preserved.

### A moment I’m proud of

Breaking Google’s record for the number of times “Harry Styles” is repeated in one interview! Just kidding. My actual proud moment was staying calm when the interviewer said something scary. A candidate who gets scared, looks like an outsider. To look like an insider, stay calm.

> *A common system design question is ‘Design Gmail.’ There are so many different dimensions, no one can design Gmail during an interview. That can’t happen. Whenever people ask you to ‘Design Gmail,’ that's to scare you.*
>
> -Former Staff Engineer at Google

The topic of scale came up. And the interviewer said this system should be able to support 800 million active users. That quote from that staff engineer popped into my head, so I–while feigning the most casual tone possible–responded with a neutral “okay.” The word choice and delivery suggest “this is another day at the office for me” which implies “I’m not scared of this.” In my head, what I told myself was “I don’t care that you don’t care.”

Talking about scale like you’ve been there before 😎 Note that we swapped out both my voice and the interviewer’s voice, so it sounds a bit robotic, but the content is correct.

## The interviewer’s feedback from the interview

![](https://strapi-iio.s3.us-west-2.amazonaws.com/feedback_93fb5e447f.png)

**At 41:00, you can hear him give me a “Weak/Lean Hire” for L4.**

## What I learned about system design interviews

Without a firm foundation, tricks lose their magic. You can have all the tricks in the world, but if you don’t know [the material](https://interviewing.io/guides/system-design-interview), it won’t be enough. That said, just knowing the material isn’t enough either because doing well is also about managing your own psychology. The main mental obstacle is the discomfort of not knowing.

Set yourself up for the least amount of discomfort. Book an anonymous, mock system design interview with engineers from top companies. Keep your skills sharp.

Unlike engineering problems, design problems are more about not knowing. There are no optimal solutions. Progress is not quantifiable. There are no predetermined outcomes. **It’s not enough to sum them up as ambiguous; they test your ability to thrive in ambiguity. Luckily, this can be learned through practice.** It is a way to think, communicate, and solve problems. So, the skills you learn are transferable, even from practices that seem unrelated.

## Three ways to practice "not knowing"

They are: the "not knowing" game, improv class, and zen koans. Try one for 10 days. If your system design interview skills don’t improve, I’ll buy you a Coke. Share your experience, tell us what we should write about next, and add anything else in the Typeform at the end of this post.

### First way: The "not-knowing" game

The way to play is to notice who knows stuff you don’t and then follow that thread. It’s easier with a topic you’re currently passionate about. For me, that’s art. I bring it up and listen for it everywhere. If someone mentions something I didn’t know, that’s a point. If they explain something I didn’t know, that’s double points. And if someone scores enough points: treat them like an old friend and see where it goes.

The other day, a stranger and I began talking at a café. A half hour later, we were in their house looking at their original copy of Dante’s Divine Trilogy which is about hell, purgatory, and heaven. It is hundreds of years old and surprisingly massive. Before this experience which was steeped in "not knowing"–an impromptu visit to a stranger’s house–I hadn’t even heard of this extremely rare art piece. After it, I had turned its pages (and not in a museum!!!).

### Second way: Improv class

If unstructured conversation seems like hell, you might prefer an improv class such as clowning. There are two types of clowns: the high status (who “knows”) and the low status (who “doesn’t know”). Low status clowns do illogical things like jump when they’re told to sit. They are the rule breaking dopey sidekicks. When playing one, you tell yourself you have no idea what’s going on, and to be okay with that. This is similar to a meditative practice called “don’t know mind.”

### Third way: Meditation

If strangers and clowns are scary, try meditation. Zen koans are a particularly baffling flavor of meditation. One is, “What is the sound of one hand clapping?” Another one is, “What is your original face, before even your parents were born?” Full of paradoxes and non-sequiturs which can’t be figured out with a normal rational everyday way of thinking, zen koans can teach you to thrive in ambiguity by yourself on an app. Shout out to all the introverts out there.

To do well in system design interviews, you need to do two things: first and foremost, you need to learn the material. Then, you need to practice “not knowing” to get better at handling ambiguity. With the ability to thrive in ambiguity, nothing in life is impossible. Without that ability, you might never crack that code, have that adventure, or save the world.

Watch my full system design interview below:

If you have other topics you want us to write about, or feedback on this post, please leave it [here](https://iiosurveys.typeform.com/to/DvByB0ao).


# [Read nine chapters of Beyond Cracking the Coding Interview for free](https://interviewing.io/blog/nine-free-chapters-of-beyond-cracking-the-coding-interview)

By Aline Lerner | Published: March 1, 2025; Last updated: April 27, 2025

Along with Gayle Laakmann McDowell, Mike Mroczka, and Nil Mamano, I wrote the official sequel to *Cracking the Coding Interview*. It's fittingly called [*Beyond Cracking the Coding Interview.*](https://www.amazon.com/dp/195570600X).

Now, we're releasing [9 chapters of the book for free](https://bctci.co/free-chapters)! There are two PDFs in the linked folder:

- **The first seven chapters of the book**, covering topics such as why technical interviews are broken, what recruiters won't tell you, why not to spend a lot of time on resumes, and how to get in the door at companies without a referral.
- **Two technical chapters: Sliding Windows and Binary Search**. Our new take on Binary Search teaches one template that works for every binary search problem on LeetCode, with only a single-line change you need to remember. The Sliding Windows chapter features 6 unique sliding window templates that make off-by-one errors a thing of the past.

Take a look, and let me know what you think. You can reach me at [aline@interviewing.io](mailto:aline@interviewing.io).


# [Technical phone screen superforecasters](https://interviewing.io/blog/technical-phone-screen-superforecasters)

By Alexey Komissarouk | Published: November 24, 2020; Last updated: July 13, 2023

*Hey, Aline (founder of interviewing.io) here. This is the third post in our Guest Author series.*

*In this post, our latest Guest Author looks at interviews from the company’s perspective. So much engineering time goes into interviewing… [we know this firsthand](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should), but what can be done about it? Some companies solve this problem by introducing homework. In this post, Alexey digs into some historical data to unearth a really clever, elegant way to save eng time that’s also better for candidate experience!*

“The new VP wants us to double engineering’s headcount in the next six months. If we have a chance in hell to hit the hiring target, you seriously need to reconsider how fussy you’ve become.”

It’s never good to have a recruiter ask engineers to lower their hiring bar, but he had a point. It can take upwards of [100 engineering hours](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should) to hire a single candidate, and we had over 50 engineers to hire. Even with the majority of the team chipping in, engineers would often spend multiple hours a week in interviews. Folks began to complain about interview burnout. Also, fewer people were actually getting offers; the *onsite pass rate* had fallen by almost a third, from ~40% to under 30%. This meant we needed even more interviews for every hire.

[Visnu](https://twitter.com/visnup) and I were early engineers bothered most by the state of our hiring process. We dug in. Within a few months, the **onsite pass rate** went back up, and interviewing burnout receded. **We didn’t lower the hiring bar, though. There was a better way.**

## Introducing: the Phone Screen Team

We took the company’s best technical interviewers and organized them into a dedicated Phone Screen Team. No longer would engineers be assigned between onsite interviews and preliminary phone screens at recruiting coordinators’ whims. The Phone Screen Team specialized in phone screens; everybody else did onsites.

## Why did you think this would be a good idea?

Honestly, all I wanted at the start was to see if I was a higher-signal interviewer than my buddy Joe. So I graphed people’s phone screen pass rate against how those candidates performed in their onsite pass rate.

Joe turned out to be the better interviewer. More importantly, I stumbled into the fact that a number of engineers doing phone screens performed consistently better across the board. They both had more candidates pass their phone screens and then those candidates would get offers at a higher rate.

![Charts comparing expected vs actual pass rates](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F3e78e_alexeypost1_e6116fd5f4.webp&w=1920&q=75 "Onsite Pass Rate vs Phone Screen Rate")

Sample data, recreated for illustrative purposes

These numbers were consistent, quarter over quarter. As we compared the top quartile of phone screeners to everybody else, the difference was stark. Each group included a mix of strict and lenient phone screeners; on average, both groups had a phone screen pass rate of 40%.

The similarities ended there: the top quartile’s invitees were twice as likely to get an offer after the onsite (50% vs 25%). These results also were consistent across quarters.[1](#user-content-fn-1)

Armed with newfound knowledge of phone screen [superforecasters](https://en.wikipedia.org/wiki/Superforecaster), the obvious move was to have them do all the interviews. In retrospect, [it made a ton of sense](https://medium.com/@alexallain/what-ive-learned-interviewing-500-people-the-interviewer-skills-ladder-for-high-growth-software-37778d2aae85) that some interviewers were “just better” than others.

A quarter after implementing the new process, the “phone screen to onsite” rate stayed constant, but the “onsite pass rate” climbed from ~30% to ~40%, shaving more than 10 hours-per-hire [2](#user-content-fn-2). Opendoor was still running this process when I left several years later.

You should too. [3](#user-content-fn-3) [4](#user-content-fn-4)

## Starting your own Phone Screen Team

### 1. Identifying interviewers

Get your Lever or Greenhouse (or [ATS](https://en.wikipedia.org/wiki/Applicant_tracking_system) of choice) into an analyzable place somewhere, and then quantify how well interviewers perform.[5](#user-content-fn-5) There’s lots of ways to analyze performance; here’s a simple approach which favors folks who generated lots of offers from as few as possible onsites and phone screens.

![Phone screener quality equation](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fc21cb_screen_shot_2020_11_25_at_10_52_57_am_539a0e3740.webp&w=1920&q=75 "Phone screener quality")

You can adjust the constants to where zero would match a median interviewer. A score of zero, then, is good. Your query will look something like this:

| **Interviewer** | **Phone Screens** | **Onsites** | **Offers** | **Score** |
| --- | --- | --- | --- | --- |
| Accurate Alice | 20 | 5 | 3 | (45 – 20 – 20) / 20 = 0.25 |
| Friendly Fred | 20 | 9 | 4 | (60 – 36 – 20) / 20 = 0.2 |
| Strict Sally | 20 | 4 | 2 | (30 – 16 – 20) / 20 = -0.3 |
| Chaotic Chris | 20 | 10 | 3 | (45 – 40 – 20) / 20 = -0.75 |
| No Good Nick | 20 | 12 | 2 | (30 – 48 – 20) / 30 = -1.9 |

Ideally, hires would also be included in the funnel, since a great phone screen experience would make a candidate more likely to join. I tried including them; unfortunately, the numbers get too small and we start running out of statistical predictive power.

### 2. Logistics & Scheduling

Phone Screen interviewers no longer do onsite interviews (except as emergency backfills). The questions they ask are now retired from the onsite interview pool to avoid collisions.

Ask the engineers to identify and block off 4 hour-long weekly slots to make available to recruiting (recruiting coordinators will love you). Use a tool like [youcanbook.me](https://youcanbook.me/) or [calendly](https://calendly.com/) to create a unified availability calendar. Aim to have no more than ~2.5 interviews per interviewer per week. To minimize burnout, one thing we tried was to take 2 weeks off interviewing every 6 weeks.

To avoid conflict, ensure that interviewers’ managers are bought in to the time commitment and incorporate their participation during performance reviews.

### 3. Onboarding Interviewers

When new engineers join the company and start interviewing, they will initially conduct on-site interviews only. If they perform well, consider inviting them into the phone screen team as slots open up. Encourage new members to keep the same question they were already calibrated on, but adapt it to the phone format as needed. In general, it helps to [make the question easier and shorter](https://triplebyte.com/blog/how-to-interview-engineers) than if you were conducting the interview in person.

When onboarding a new engineer onto the team, have them shadow a current member twice, then be reverse-shadowed by that member twice. Discuss and offer feedback after each shadowing.

### 4. Continuous Improvement

Interviewing can get repetitive and lonely. Fight this head-on by having recruiting coordinators add a second interviewer (not necessarily from the team) to join 10% or so of interviews and discuss afterwards.

Hold a monthly retrospective with the team and recruiting, with three items on the agenda:

- discuss potential process improvements to the interviewing process
- review borderline interviews with the group to review together, if [your interviewing tool](https://coderpad.io) supports recording and playback
- have interviewers read through feedback their candidates got from onsite interviewers and look for consistent patterns

### 5. Retention

Eventually, interviewers may get burnt out and say things like “I’m interviewing way more people than others on my actual team – why? I could just go do onsite interviews.” This probably means it’s time to rotate them out. Six months feels about right for a typical “phone screen team” tour of duty, to give people a rest. Some folks may not mind and stay on the team for longer.

Buy exclusive swag for team members. Swag are cheap and these people are doing incredibly valuable work. Leaderboards (“Sarah interviewed 10 of the new hires this year”) help raise awareness. Appreciation goes a long way.

Also, people want to be on teams with cool names. Come up with a cooler name than “Phone Screen Team.” My best idea so far is “Ambassadors.”

There’s something very Dunder Mifflin about companies that create Growth Engineering organizations to micro-optimize conversion, only to have those very growth engineers struggle to focus due to interview thrash from an inefficient hiring process. These companies invest millions into hiring, coaching and retaining the very best sales people. Then they leave recruiting – selling the idea of working at the company – in the hands of an engineer that hasn’t gotten a lick of feedback on their interviewing since joining two years ago, with a tight project deadline on the back of her mind.

If you accept the simple truth that not all interviewers are created equal, that the same rigorous quantitative process with which you improve the business should also be used to improve your internal operations, and if you’re trying to hire quickly, you should consider creating a Technical Phone Screen Team.

## FAQs, Caveats, and Preemptive Defensiveness

1. **Was this statistically significant, or are you conducting pseudoscience?** Definitely pseudoscience. Folks in the sample were conducting about 10 interviews a month, ~25 per quarter. Perhaps not yet ready to [publish in Nature](https://idp.nature.com/authorize?response_type=cookie&client_id=grover&redirect_uri=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-00857-9) but meaningful enough to infer from, especially considering the relatively low cost of being wrong. [↩](#user-content-fnref-1)
2. **Why didn’t the on-site pass rate double, as predicted?** First, not all of the top folks ended up joining the team. Second, the best performers did well because of a combination of skill (great interviewers, friendly, high signal) and luck (got better candidates). Luck is fleeting, resulting in a [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean). [↩](#user-content-fnref-2)
3. **What size does this start to make sense at**? Early on, you should just identify who you believe your [best interviewers](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) are and have them (or yourself) do all the phone screens. Then, once you start hiring rapidly enough that you are doing about 5-10 phone screens a week, run the numbers and invite your best 2-3 onsite interviewers to join and create the team. [↩](#user-content-fnref-3)
4. **What did you do for specialized engineering roles?** They had their own dedicated processes. Data Science ran a take home, Front-End engineers had their own Phone Screen sub-team, and Data and ML Engineers went through the general full-stack engineer phone screen. [↩](#user-content-fnref-4)
5. **Didn’t shrinking your Phone Screener pool hurt your diversity?** In fact, the opposite happened. First, the phone screener pool had a higher percentage of women than the engineering organization at the time; second, a common interviewing anti-pattern is “hazing” – asking difficult questions and then rejecting somebody for “not even remembering about Kahn’s algorithm, lolz.” The best phone screeners don’t haze, bringing a more diverse group onsite. [↩](#user-content-fnref-5)

[We have the best technical interviewers on the market. Here's how we do it.](/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it)


# [People are still bad at gauging their own interview performance. Here’s the data.](https://interviewing.io/blog/own-interview-performance)

By Aline Lerner | Published: September 7, 2016; Last updated: May 1, 2023

[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because [resumes suck](https://blog.alinelerner.com/resumes-suck-heres-the-data/) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.

At the end of 2015, we published a [post about how people are terrible at gauging their own interview performance](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). At the time, we just had a few hundred interviews to draw on, so as you can imagine, we were quite eager to rerun the numbers with the advent of more data. **After drawing on roughly one thousand interviews, we were surprised to find that the numbers have *really* held up, and that people continue to be terrible at gauging their own interview performance.**

When an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (feel free to watch this process in action on our [recordings](https://interviewing.io/mocks) page). After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.

If you’re curious, you can see what the feedback forms look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of those questions is about how well they think they did. For context, a technical score of 3 or above seems to be the rough cut-off for hirability.

![Screenshot showing the interviewing.io interview feedback form for interviewers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F49613_interviewer_feedback_ae7debf69a.webp%3Fupdated_at%3D2022-12-13T17%3A27%3A22.302Z&w=1920&q=75 "Feedback form for interviewers")

![Screenshot showing the interviewing.io interview feedback form for interviewees](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F8ddad_interviewee_feedback_3f34ef7a0a.webp%3Fupdated_at%3D2022-12-13T17%3A30%3A40.759Z&w=1920&q=75 "Feedback form for interviewees")

## Perceived versus actual performance… revisited

Below are two heatmaps of perceived vs. actual performance per interview (for interviews where we had both pieces of data). In each heatmap, the darker areas represent higher interview concentration. For instance, the darkest square represents interviews where both perceived and actual performance was rated as a 3. You can hover over each square to see the exact interview count (denoted by “z”).

The first heatmap is our old data:

And the second heatmap is our data as of August 2016:

**As you can see, even with the advent of a lot more interviews, the heatmaps look remarkably similar.** The [R-squared](https://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) for a linear regression on the first data set is 0.24. And for the more recent data set, it’s dropped to 0.18. In both cases, even though some small positive relationship between actual and perceived performance does exist, it is not a strong, predictable correspondence.

You can also see there’s a non-trivial amount of impostor syndrome going on in the graph above, which probably comes as no surprise to anyone who’s been an engineer. Take a look at the graph below to see what I mean.

The x-axis is the difference between actual and perceived performance, i.e. actual minus perceived. In other words, a negative value means that you overestimated your performance, and a positive one means that you underestimated it. Therefore, every bar above 0 is impostor syndrome country, and every bar below zero belongs to its foulsome, overconfident cousin, the [Dunning-Kruger](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect) effect.[1](#user-content-fn-1)

On interviewing.io (though I wouldn’t be surprised if this finding extrapolated to the qualified engineering population at large), **impostor syndrome plagues interviewees roughly twice as often as Dunning-Kruger**. Which, I guess, is better than the alternative.

## Why people underestimate their performance

With all this data, I couldn’t resist digging into interviews where interviewees gave themselves 1’s and 2’s but where interviewers gave them 4’s to try to figure out if there were any common threads. And, indeed, a few trends emerged. **The interviews that tended to yield the most interviewee impostor syndrome were ones where question complexity was layered.** In other words, the interviewer would start with a fairly simple question and then, when the interviewee completed it successfully, they would change things up to make it harder. Lather, rinse, repeat. In some cases, an interviewer could get through up to 4 layered tiers in about an hour. Inevitably, even a good interviewee will hit a wall eventually, even if the place where it happens is way further out than the boundary for most people who attempt the same question.

**Another trend I observed had to do with interviewees beating themselves up for issues that mattered a lot to them but fundamentally didn’t matter much to their interviewer**: off-by-one errors, small syntax errors that made it impossible to compile their code (even though everything was semantically correct), getting big-O wrong the first time and then correcting themselves, and so on.

**Interestingly enough, how far off people were in gauging their own performance was independent of how highly rated (overall) their interviewer was or how strict their interviewer was.**

With that in mind, if I learned anything from watching these interviews, it was this. Interviewing is a flawed, human process. Both sides want to do a good job, but sometimes the things that matter to each side are vastly different. And sometimes the standards that both sides hold themselves to are vastly different as well.

## Why this (still) matters for hiring, and what you can do to make it better

Techniques like layered questions are important to sussing out just how good a potential candidate is and can make for a really engaging positive experience, so removing them isn’t a good solution. And there probably isn’t that much you can do directly to stop an engineer from beating themselves up over a small syntax error (especially if it’s one the interviewer didn’t care about). However, all is not lost!

As you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. **As it turns out, there’s a very statistically significant relationship between whether people think they did well and whether they’d want to work with the interviewer. This means that when people think they did poorly, they may be a lot less likely to want to work with you**. And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, *despite the fact that they actually did*.

How can one mitigate these losses? Give positive, actionable feedback immediately (or as soon as possible)! This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they *totally* didn’t want to work there anyway.

1. I’m always terrified of misspelling “Dunning-Kruger” and not double-checking it because of overconfidence in my own spelling abilities. [↩](#user-content-fnref-1)


# [Announcing the interviewing.io Technical Interview Practice Fellowship](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship)

By Aline Lerner | Published: July 19, 2020; Last updated: May 1, 2023

I started interviewing.io because I was frustrated with how inefficient and unfair hiring was and how much emphasis employers placed on resumes.

But the problem is bigger than resumes. We’ve come to learn that interview practice matters just as much. The resume gets you in the door, and your interview performance is what gets you the offer. But, even though technical interviews are hard and scary for everyone — many of our users are senior engineers from FAANG who are terrified of getting back out there and code up the kinds of problems they don’t usually see at work while someone breathes down their neck — interview prep isn’t equitably distributed.

This inequity never really sat right with me (that’s why interviewing.io exists), but when [we started charging for interview practice post-COVID](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), it *really* didn’t sit right with me.

As you may have read, if you follow interviewing.io news, COVID-19 turned our world upside down. In its wake, the pandemic left a deluge of hiring slowdowns and freezes. For a recruiting marketplace, this was an existential worst nightmare — in a matter of weeks, we found ourselves down from 7-figure revenue to literally nothing. Companies didn’t really want or need to pay for hiring anymore, and we were screwed.

Then, we pivoted and started charging our users, who had previously been able to practice on our platform completely for free (albeit with some strings, more on that in a moment). While this pivot was the right thing to do — without it, we would have had to shut down the company, unable to provide any practice at all — charging people, especially those from underrepresented backgrounds, didn’t sit right with us, and in our last post announcing our model, we made the following promises:

- We’d ALWAYS have a free tier
- **We’d immediately start working on a fellowship for engineers from underrepresented backgrounds or in a visa crisis/experiencing financial hardship ←** *That’s what this post is about!*
- We’d find a way to let people defer their payments

We launched with a free tier, and it’s still there and going strong. We’re still working on deferred payments and are in the thick of user research and price modeling.

But, the rest of this post is about the 2nd promise. To wit, I’m so proud to tell you that we’ve officially launched the first (pilot) cohort of the interviewing.io Technical Interview Practice Fellowship. This cohort will be focused on engineers from backgrounds that are underrepresented in tech. We are acutely aware, of course, that our first cohort couldn’t capture everyone who’s underrepresented, that gender and race isn’t enough, and that we need to do more for our users who can’t afford our price tags, regardless of who they are or where they come from.

Our hope is to expand this Fellowship to anyone who needs it.

We’re also working on the much harder problem of how to navigate the visa situation we’re in right now (different than when we wrote the first post, sadly… but especially important to me, given that I’m an immigrant myself).

### What is the Fellowship, and why does it exist?

Before we tell you a little bit about the Fellows in our inaugural cohort and what the Fellowship entails, a quick word about why this matters.

In order to get a job as a software engineer, it’s not enough to have a degree in the field from a top school. However you learned your coding skills, you also have to pass a series of rigorous technical interviews, focusing on analytical problem solving, algorithms, and data structures.

This interview style is controversial, in part because it’s not entirely similar to the work software engineers do every day but also because 1) like standardized testing, it’s a learned skill and 2) unlike standardized testing, interview results are not consistent or repeatable — the same candidate can do well in one interview and fail another one in the same day. According to our data, only about [25% of candidates are consistent in their performance from interview to interview](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary) and [women quit 7X more often than men after a poor performance](https://interviewing.io/blog/voice-modulation-gender-technical-interviews).

To account for both of these limitations, the best strategy to maximize your chances of success is to practice a lot so you can 1) get better and 2) accept that the results of a single interview are not the be-all and end-all of your future aptitude as a software engineer and that it’s ok to keep trying.

The main problem created by modern interview techniques is that, despite interview practice being such a critical prerequisite to success in this field, access to practice isn’t equitably distributed. We want to fix this, and we’re well equipped to do so. Based on our data, engineers are twice as likely to pass a real interview after they’ve done 3-5 practice sessions on our platform.

Our Fellows will get these practice sessions completely for free. These will be 1:1 hour-long sessions with senior engineers from a top company who have graciously volunteered their time and expertise. Huge thank you and a big shout-out to them all.

After each session, Fellows will get actionable feedback that will help them in their upcoming job search, and we will be helping Fellows connect with top companies as well.

Note: We’d like to be able to offer even more support – and are actively seeking more partners to do so. Please see the *How you can help* section below if you or your organization would like to get involved!

### Why now?

The world seems to be in a place, now more than ever, to have the conversation about race, gender, socioeconomic, and other kinds of equity, in hiring. This is our small part of that conversation.

### Who are the Fellows?

After opening up our application process, we close to 1,000 submissions in a week, and (though it was really, really hard) we culled those down to 56 Fellows.

Our first cohort is:

- **82%** Black, Latinx, and/or Indigenous
- **53%** women
- **55%** senior (4+ years of experience) & **45%** junior (0-3 years of experience)

Here are some of their (anonymized) stories. There were a lot of stories like these.

> *My goal is to keep pressing as well as to share and give to underrepresented communities because the journey in tech can be isolating. Often I am the only one. It is critical that there are more people that look like me that are engineers \*and\* ascend the leadership ladder.*

> *My parents immigrated from [redacted] to The Bronx without a formal education. I’m the first individual in my household to graduate from college and I’m the only Software Engineer in my family. I grew up in a poor neighborhood where many individuals had limited economic and educational opportunity. I aim to make the path to become a Software Engineer easier for those who were in my situation.*

> *My journey to becoming a software engineer almost never happened. Throughout my undergraduate studies I was faced with having to drop out multiple times, due to the immigration status of my parents…. I was tasked with assisting in my family’s living situation and paying for school. I worked full time and started my own construction company in order to take care of my family and studies. It was always tough having to work 8-10 hours a day and then going to class or doing homework… Becoming a software engineer was always a goal of mine, and realizing that goal was well worth the struggle, given the struggle my parents went through to bring us here in the first place.*

> *I spent 5 years in public education working directly with marginalized communities in the struggle for equity. My journey through software engineering is a continuation of this spirit of advocacy and changemaking. Software engineering is a tool to be put at the service of advocacy.*

### What can I do to help?

There are a number of ways you can help and get involved!

#### Help sponsor future Fellowship cohorts & create scholarships for underrepresented engineers!

Every Fellow in this first cohort represents at least 100X who are not. We have the tech to scale the hell out of this program, and all we need is backing and resources from people or organizations who recognize there’s a need (donations are tax-deductible). **Please email [fellowship-sponsors@interviewing.io](mailto:fellowship-sponsors@interviewing.io) if you’d like to get involved or want more information.**

#### Hire through us!

Despite mounting evidence that resumes are poor predictors of aptitude, companies were obsessed with where people had gone to school and worked previously. On interviewing.io, software engineers, no matter where they come from or where they’re starting, can book anonymous mock interviews with senior interviewers from top companies. We use data from these interviews to identify top performers much more reliably than a resume, and fast-track them to real job interviews with employers on our platform through the same anonymous, fair process. Because we use data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Twitch, Lyft, Dropbox, and many others, **and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds. Many of our candidates have literally been rejected based on their resumes by the same employer who later hired them when they came through our anonymous platform** (one notable candidate was rejected 3 times from a top-tier public company based on his resume before he got hired at that same company through our anonymous interview format).

**Please email [sales@interviewing.io](mailto:sales@interviewing.io) to get rolling.**

#### Buy an individual practice session for someone who can’t afford it

**If you know individual engineers who need interview practice but can’t afford it, use our [handy interview gifting feature](https://interviewing.io/gift-practice-interviews).** Interviews are $100 each. They’re not cheap, but we have to price them that way to pay for interviewer time (interviewers are senior FAANG engineers) and cover our costs. Sadly that means practice interviews are not affordable to everyone. Even if you can’t get involved to help us fund interviews at scale, if you know someone who needs practice but can’t afford it, you can buy them an anonymous mock interview or two individually. It’s the best gift you can give to an engineer who’s starting their job search.


# [What do the best interviewers have in common? We looked at thousands of real interviews to find out.](https://interviewing.io/blog/best-technical-interviews-common)

By Aline Lerner | Published: November 28, 2017; Last updated: July 13, 2023

At interviewing.io, we’ve analyzed and written at some depth about what makes for a good interview from the perspective of an interviewee. However, despite the inherent power imbalance, interviewing is a two-way street. I wrote a while ago about how, in this market, [recruiting isn’t about vetting as much as it is about selling](http://blog.alinelerner.com/building-a-product-in-the-technical-recruiting-space-read-this-first/), and not engaging candidates in the course of talking to them for an hour is a woefully missed opportunity. But, just like solving interview questions is a learned skill that takes time and practice, so, too, is the other side of the table. Being a good interviewer takes time and effort and a fundamental willingness to get out of autopilot and engage meaningfully with the other person.

Of course, everyone and their uncle has strong opinions about what makes someone a good interviewer, so instead of waxing philosophical, we’ll present some data and focus on analytically answering questions like… Does it matter how strong of an engineering brand your company has, for instance? Do the questions you ask actually help get candidates excited? How important is it to give good hints to your candidate? How much should you talk about yourself? And is it true that, at the end of the day, what you say is way less important than how you make people feel?[1](#user-content-fn-1) And so on.

Before I delve into our findings, I’ll say a few words about interviewing.io and the data we collect.

[interviewing.io](https://interviewing.io/) is an anonymous technical interviewing platform. On interviewing.io, people can practice technical interviewing anonymously, and if things go well, unlock real (still anonymous) interviews with companies like Lyft, Twitch, Quora, and more.

The cool thing is that both practice and real interviews with companies take place within the interviewing.io ecosystem. As a result, we’re able to collect quite a bit of interview data and analyze it to better understand technical interviewing. One of the most important pieces of data we collect is feedback from both the interviewer and interviewee about how they thought the interview went and what they thought of each other. If you’re curious, you can watch a real interview on our [recordings](https://interviewing.io/mocks) page, and see what the feedback forms for interviewers and interviewees look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, one of which is their own take on how they thought they did.

![Screenshot of the interviewing.io interview feedback form for interviewers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_6712d25e2a.png&w=1200&q=75 "Interviewing.io interview feedback form for interviewers")

Interviewer feedback form

![Screenshot of the interviewing.io interview feedback form for interviewees](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fb48_screenshot_2017_11_29_09_02_07_7d4a011c3d.png&w=1080&q=75 "Interviewing.io interview feedback form for interviewees")

Interviewee feedback form

In this post, we’ll be analyzing feedback and outcomes of thousands of real interviews with companies to figure out what traits [the best interviewers](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) have in common.

Before we get into the nitty-gritty of individual interviewer behaviors, let’s first put the value of a good interviewer in context by looking at the impact of a company’s brand on the outcome. After all, if brand matters a lot, then maybe being a good interviewer isn’t as important as we might think.

## Brand strength

So, does brand really matter for interview outcomes? One quick caveat before we get into the data: every interview on the platform is user-initiated. In other words, once you unlock our jobs portal (you have to do really well in practice interviews to do so), you decide who you talk to. So, candidates talking to companies on our platform will be predisposed to move forward because they’ve chosen the company in the first place. And, as should come as no surprise to anyone, companies with a very strong brand have an easier time pulling candidates (on our platform and out in the world at large) than their lesser-known counterparts. Moreover, many of the companies we work with do have a pretty strong brand, so our pool isn’t representative of the entire branding landscape. However, all is not lost — in addition to working with very recognizable brands, we work with a number of small, up-and-coming startups, so we hope that if you, the reader, are coming from a company that’s doing cool stuff but that hasn’t yet become a household name, our findings likely apply to you. And, as you’ll see, getting candidates in the door isn’t the same as keeping them.

To try to quantify brand strength, we used three different measures: the company’s [Klout Score](https://klout.com/corp/score) (yes, that still exists), its Mattermark [Mindshare Score](https://mattermark.com/first-official-company-rankings-update/#:~:text=The%20Mindshare%20Score%20is%20the,gain%20and%20retain%20attention%20online), and its score on [Glassdoor](https://www.glassdoor.com/Reviews/index.htm) (under general reviews).[2](#user-content-fn-2)

When we looked at interview outcomes relative to brand strength, its impact was not statistically significant. In other words, **we found that brand strength didn’t matter at all when it came to either whether the candidate wanted to move forward or how excited the candidate was to work at the company**.

This was a bit surprising, so I decided to dig deeper. Maybe brand strength doesn’t matter overall but matters when the interviewer or the questions they asked aren’t highly rated? In other words, can brand buttress less-than-stellar interviewers? Not so, according to our data. Brand didn’t matter even when you corrected for interviewer quality. In fact, of the top 10 best-rated companies on our platform, half have no brand to speak of, 3 are mid-sized YC companies that command respect in Bay Area circles but are definitely not universally recognizable, and only 2 have anything approaching household name status.

So, what’s the takeaway here? **Maybe the most realistic thing we can say is that while brand likely matters *a lot* for getting candidates in the door, once they’re in, no matter how well-branded you are, they’re yours to lose.**

## Choosing the interview question

If brand doesn’t matter once you’ve actually gotten a candidate in the door, then what does? Turns out, the questions you ask matter a TON. As you recall, feedback on interviewing.io is symmetric, which means that in addition to the interviewer rating the candidate, the candidate also rates the interviewer, and one of the things we ask candidates is how good the question(s) they got asked were.

**Question quality was extremely significant (p < 0.002 with an effect size of 1.25) when it came to whether the candidate wanted to move forward with the company. This held both when candidates did well and when they did poorly.**

While we obviously can’t share the best questions (these are company interviews, after all), we can look at what candidates had to say about the best and worst-rated questions on the platform.

> *Always nice to get questions that are more than just plain algorithms.*

> *Really good asking of a classic question, opened my mind up to edge cases and considerations that I never contemplated the couple of times I’ve been exposed to the internals of this data structure.*

> *This was the longest interviewing.io interview I have ever done, and it is also the most enjoyable one! I really like how we started with a simple data structure and implemented algorithms on top of it. It felt like working on a simple small-scale project and was fun.*

> *He chose an interesting and challenging interview problem that made me feel like I was learning while I was solving it. I can’t think of any improvements. He would be great to work with.*

> *I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted] which made it feel like less like a toy problem and more like a pared-down version of a real problem.*

> *This is my favorite question that I’ve encountered on this site. it was one of the only ones that seem like it had actual real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.*

> *Question wasn’t straightforward and it required a lot of thinking/understanding since functions/data structures weren’t defined until a lot later. [Redacted] is definitely a cool company to work for, but some form of structure in interviews would have been a lot more helpful. Spent a long time figuring out what the question is even asking, and interviewer was not language-agnostic.*

> *I was expecting a more technical/design question that showcases the ability to think about a problem. Having a domain-specific question (regex) limits the ability to show one’s problem-solving skills. I am sure with enough research one could come up with a beautiful regex expression but unless this is something one does often, I don’t think it [makes for] a very good assessment.*

> *This is not a good general interview question. A good interview question should have more than one solution with simplified constraints.*

### Anatomy of a good technical interview question

1. Layer complexity (including asking a warmup)
2. No trivia
3. Real-world components/relevance to the work the company is doing are preferable to textbook algorithmic problems
4. If you’re asking a classic algorithmic question, that’s ok, but you ought to bring some nuance and depth to the table, and if you can teach the interviewee something interesting in the process, even better!

## Asking the question

One of the other things we ask candidates after their interviews is how helpful their interviewer was in guiding them to the solution. Providing your candidate with well-timed hints that get them out of the weeds without giving away too much is a delicate art that takes a lot of practice (and a lot of repetition), but how much does it matter?

As it turns out, being able to do this well matters a ton. **Being good at providing hints was extremely significant** (p < 0.00001 with an effect size of 2.95) **when it came to whether the candidate wanted to move forward with the company (as before, we corrected for whether the interview went well)**.

You can see for yourself what candidates thought of their interviewers when it came to their helpfulness and engagement below. Though this attribute is a bit harder to quantify, it seems that hint quality is actually a specific instance of something bigger, namely the notion of turning something inherently adversarial into a collaborative exercise that leaves both people in a better place than where they started.[3](#user-content-fn-3)

And if you can’t do that every time, then at the very least, be present and engaged during the interview. And no matter what the devil on your shoulder tells you, no good will ever come of opening Reddit in another tab.[4](#user-content-fn-4)

One of the most memorable, pithy conversations I ever had about interviewing was with a seasoned engineer who had spent years as a very senior software architect at a huge tech company before going back to what he’d always liked in the first place, writing code. He’d conducted a lot of interviews over a career spanning several decades, and after trying out a number of different interview styles, what he settled on was elegant, simple, and satisfying. **According to him, the purpose of any interview is to “see if we can be smart together.”** I like that so much, and it’s advice I repeat whenever anyone will listen.

> *I liked that you laid out the structure of the interview at the outset and mentioned that the first question did not have any tricks. That helped set the pace of the interview so I didn’t spend an inordinate amount of time on the first one.*

> *The interview wasn’t easy, but it was really fun. It felt more like making a design discussion with a colleague than an interview. I think the question was designed/prepared to fill the 45 minute slot perfectly.*

> *I’m impressed by how quickly he identified the issue (typo) in my hash computation code and how gently he led me to locating it myself with two very high-level hints (“what other tests cases would you try?” and “would your code always work if you look for the the pattern that’s just there at the beginning of the string?”). Great job!*

> *He never corrected me, instead asked questions and for me to elaborate in areas where I was incorrect – I very much appreciate this.*

> *The question seemed very overwhelming at first but the interviewer was good at helping to break it down into smaller problems and suggest we focus on one of those first.*

> *[It] was a little nerve-wracking hearing you yawn while I was coding.*

> *What I found much more difficult about this interview was the lack of back and forth as I went along, even if it was simple affirmation that “yes, that code you just wrote looks good”. There were times when it seemed like I was the only one who had talked in the past five minutes (I’m sure that’s an exaggeration). This made it feel much more like a performance than like a collaboration, and my heart was racing at the end as a result.*

> *While the question was very straightforward, and [he] was likely looking for me to blow through it with no prompting whatsoever in order to consider moving forward in an interview process, it would have been helpful to get a discussion or even mild hinting from him when I was obviously stuck thinking about an approach to solve the the problem. While I did get to the answer in the end, having a conversation about it would have made it feel more like a journey and learning experience. That would have also been a strong demonstration of the collaborative culture that exists while working with teams of people at a tech company, and would have sold me more vis-a-vis my excitement level.*

> *If an interview is set to 45 minutes, the questions should fit this time frame, because people plan accordingly. I think that if you plan to have a longer interview you should notify the interviewee beforehand, so he can be ready for it.*

> *One issue I had with the question though is what exactly he was trying to evaluate from me with the question. At points we talking about very nitty-gritty details about python linked list or array iteration, but it was unclear at any point if that was what he was judging me on. I think in the future he could outline at the beginning what exactly he was looking for with the problem in order to keep the conversation focused and ensure he is well calibrated judging candidates.*

> *Try to be more familiar with all the possible solutions to the problem you choose to pose to the candidate. Try to work on communicating more clearly with the candidate.*

### Anatomy of a good interview

1. Set expectations, and control timing/pacing
2. Be engaged!
3. Familiarity with the problem and its associated rabbit holes/garden paths
4. Good balance of hints and letting candidate think
5. Turn the interview into a collaborative exercise where both people are free to be smart together

## The art of storytelling… and the importance of being human

Beyond choosing and crafting good questions and being engaged (but not overbearing) during the interview, what else do top-rated interviewers have in common?

The pervasive common thread I noticed among the best interviewers on our platform is, as above, a bit hard to quantify but dovetails well with the notion of being engaged and creating a collaborative experience. It’s taking a dehumanizing process and elevating it to an organic experience between two capable, thinking humans. Many times, that translates into revealing something real about yourself and telling a story. It can be sharing a bit about the company you work at and why, out of all the places you could have landed, you ended up there. Or some aspect of the company’s mission that resonated with you specifically. Or how the projects you’ve worked on tie into your own, personal goals.

> *I like the interview format, in particular how it was primarily a discussion about cool tech, as well as an honest description of the company… the discussion section was valuable, and may be a better gauge of fit anyway. It’s nice to see a company which places value on that 🙂*

> *The interviewer was helpful throughout the interview. He didn’t mind any questions on their company’s internal technology decisions, or how it’s structured. I liked that the interviewer gave me a good insight of how the company functions.*

> *Extremely kind and very generous with explaining everything they do at [redacted]. Really interested in the technical challenges they’re working on. Great!*

> *Interesting questions but the most valuable and interesting thing were the insights he gave me about [redacted]. He sounded very passionate about engineering in general, particularly about the challenges they are facing at [redacted]. Would love to work with him.*

> *[A] little bit of friendly banter (even if it’s just “how are you doing”?) at the very beginning of the interview would probably help a bit with keeping the candidate calm and comfortable.*

> *I thought the interview was very impersonal, [and] I could not get a good read on the goal or mission of the company.*

And, as we wrote about in a previous post, one of the most genuine, human things of all is [giving people immediate, actionable feedback](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). As you recall, during the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. As it turns out, there’s a very statistically significant relationship (p < 0.00005) between whether people think they did well and whether they’d want to work with the interviewer.[5](#user-content-fn-5) This means that when people think they did poorly, they may be a lot less likely to want to work with you. And by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, despite the fact that they actually did.

How can one mitigate these losses? **Give positive, actionable feedback immediately (or as soon as possible)! This way people don’t have time to go through the self-flagellation gauntlet that happens after a perceived poor performance, followed by the inevitable rationalization that they totally didn’t want to work there anyway.**

### How to be human

1. Talk about what your company does… and what specifically about it appealed to you and made you want to join
2. Talk about what you’re currently working on and how that fits in with what you’re passionate about
3. When you like a candidate, give positive feedback as quickly as you can to save them from the self-flagellation that they’ll likely go through otherwise… and which might make them rationalize away wanting to work with you
4. And, you know, be friendly. A little bit of warmth can go a long way.

## Becoming a better interviewer

Interviewing people is hard. It’s hard to come up with good [interview questions](https://interviewing.io/questions), it’s hard to give a good interview, and it’s especially hard to be human in the face of conducting a never-ending parade of interviews. But, being a good interviewer is massively important. As we saw, while your company’s brand will get people in the door, once they’ve reached the technical interview, the playing field is effectively level, and you can no longer use your brand as a crutch to mask poor questions or a lack of engagement. And in this market, where the best candidates have a ton of options, when wielded properly, a good interview that elevates a potentially cold, transactional interaction into something real and genuine can become the selling point that gets great engineers to work for you, whether you’re a household name or a startup that just got its first users.

Given how important it is to do interviews well, what are some things you can do to get better right away? **One thing I found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note.** These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune out a lot of the details and distill the essence of the problem into something it doesn’t take the candidate a lot of work/setup to grok, and you’ll likely have to iterate on the question a few times before you get it right — but they payoff can be huge.

Another thing you can do to get actionable feedback like the kind you saw in this post (and then immediately level up) is to get on interviewing.io as an interviewer. **If you interview people in our double-blind practice pool, no one will know who you are or which company you represent, which means that you get a truly unbiased take on your interviewing ability, which includes your question quality, how excited people would be to work with you, and how good you are at helping people along without giving away too much.** It’s also a great way to go beyond your team, which can be pretty awkward, and try out new questions on a very engaged, high-quality user base. You’ll also get to keep replays of your interviews so you can revisit crucial moments and figure out exactly what you need to do to get better next time.

![Screenshot of the interviewer overall performance report](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffb028_screenshot_2017_11_29_11_52_16_528b62c8c9.png&w=1080&q=75 "Overall performance")

*Want to hone your skills as an interviewer? Want to help new interviewers at your company warm up before they officially get added to your interview loops? You can [sign up to our platform as an interviewer](https://interviewing.io/signup), or (especially for groups) ping us at [interviewers@interviewing.io](mailto:interviewers@interviewing.io).*

1. “People will forget what you said, people will forget what you did, but people will never forget how you made them feel.” - Maya Angelou [↩](#user-content-fnref-1)
2. It’s important to call out that brand and *engineering* brand are two separate things that can diverge pretty wildly. For instance, Target has a strong brand overall but probably not the best engineering brand (sorry). Heap, on the other hand, is one of the better-respected places to work among engineers (both on interviewing.io and off), but it doesn’t have a huge overall brand. Both the Klout and Mattermark Mindshare scores aren’t terrible for quantifying brand strength, but they’re not amazing at engineering brand strength (they’re high for Target and low for Heap). The Glassdoor score is a bit better because reviewers tend to skew engineering-heavy, but it’s still not that great of a measure. So, if anyone has a better way to quantify this stuff, let me know. If I were doing it, I’d probably look at GitHub repos of the company and its employees, who their investors are, and so on and so forth. But that’s a project that’s out of scope for this post. [↩](#user-content-fnref-2)
3. If you’re familiar with [Dan Savage’s campsite rule](https://en.wikipedia.org/wiki/Savage_Love#Campsite_rule) for relationships, I think there should be a similar for interviewing… leave your candidates in better shape than when you found them. [↩](#user-content-fnref-3)
4. Let us save you the time: Trump is bad, dogs are cute, someone ate something. [↩](#user-content-fnref-4)
5. This time with even more significance! [↩](#user-content-fnref-5)


# [Does communication matter in technical interviewing? We looked at 100K interviews to find out.](https://interviewing.io/blog/does-communication-matter-in-technical-interviewing-we-looked-at-100k-interviews-to-find-out)

By Dima Korolev | Published: May 17, 2022; Last updated: May 1, 2023

*Hey, Aline (founder of interviewing.io) here. This is the 5th post in our Guest Author series.*

*One of the things I’m most excited about with the Guest Author series is the diversity of opinions it’s bringing to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my personal opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I still don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*

*If you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*

![Author avatar](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FDima_Korolev_61fbf691c1.png&w=384&q=75 "Dima Korolev")

Dima Korolev

Dima Korolev finds joy in helping established companies on a large-scale architecture level, previously at PokerStars, now at Miro. His career includes roles at Google (from 2007–2011), along with other large companies and several successful startups. While in school, Dima once earned Coder of the Month and was consistently in the top 100 globally on Topcoder. He particularly enjoys mentoring engineers and finds it emotionally rewarding to help others develop stronger skills and expand their horizons. His love of helping others inspired Dima to become one of the first interviewers on interviewing.io, completing more than 100 interviews to date. You can reach him on [*LinkedIn*](http://www.linkedin.com/in/dimakorolev)*,* [*GitHub*](https://github.com/dkorolev)*,* [*Twitter*](https://twitter.com/UniqueDima)*,* and his [*website*](http://dima.ai/).

The interviewing.io platform has hosted and collected feedback from over 100K technical interviews, split between mock interviews and real ones. It’s generally accepted that to pass a technical interview, you have to not only come up with a solution to the problem (or at least make good headway), but you also have to do a good job of articulating your thoughts, explaining to your interviewer what you’re doing as you’re doing it, and coherently discussing tradeoffs and concepts like time and space complexity.

But how important is communication in technical interviews, really? We looked at the data, and it turns out that *talk is cheap*. Read on to find out how and why.

On interviewing.io, engineers can practice technical interviewing anonymously. When an interviewer and an interviewee match on interviewing.io, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.

Here’s the feedback form that interviewers fill out:

![Sample interview feedback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_1ca42f5eec.png&w=1920&q=75 "Interview feedback form")

As you can see above, there’s one yes/no question: *Would you advance this person to the next round?* There are also three other questions, each graded on a scale from 1 to 4 (4 is best):

- Technical ability, which we’ll call ***Code***
- Problem-solving ability, which we’ll call ***Solve***
- Communication skill, which we’ll call ***Communicate***

Do the numeric scores predict whether a candidate gets advanced to the next round? You bet! Here are the top buckets, ranked by how often the interviewers say “yes”:

![Success-Rate.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Success_Rate_f6934e5141.png)

Right away, three pieces of anecdotal evidence suggest that talk is cheap:

1. The only “2” that made it to the list is communication score (see 4-4-2 above).
2. The least valuable “4” coupled with two “3”-s is also the communication score (see 3-3-4 above), with a major drop in hire rate.
3. And, ironically, 4-3-3, a scenario where the candidate got 3s in both communication and problem solving, has a [negligibly] higher success rate than 4-3-4, a scenario where the candidate got a 4 in communication and a 3 in problem solving!

In fact, the 4-4-2 bucket is the third best one can get, losing only to 4-4-4 and 4-4-3. Interviewers are happy to advance 4-4-2 candidates to the onsite round a whopping 96% of the time. Notably, a candidate is 3X more likely to get rejected with 3-3-4 than with 4-4-2.

So it seems that talk is cheap. But to learn exactly *how* cheap, we have to go deeper, and this is where data science helps.

## Into the data

It’s data science time! Let’s do a thought experiment. Imagine a candidate walks into an interview, impaired by the moral equivalent of 1 point, on either their ability to Code, to Solve, or to Communicate.[1](#user-content-fn-1)

To minimize damage, from which ability should our imaginary candidate have that 1 point deducted?

To answer this question, let’s take a look at how much worse this rejection rate would be with a 1-point deduction.

![Rejection-with-minus.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_minus_804e06854a.png)

Whoa!

For a good third of all interviews, **dropping 1 point from the *Code* or *Solve* scores would lead to the rejection rate skyrocketing 6X**! Clearly, that last point in *Code* or *Solve* is extremely valuable.

At the same time, in all categories, the negative impact of dropping 1 point from the *Communicate* category is by far the smallest.

The opposite direction is interesting as well. Say our imaginary candidate is fond of Heroes of Might and Magic and was up all night playing. *This is not recommended either, but imaginary experiments really help with science.* Just before the interview, this candidate meditates, their inner interviewee hero visits a Temple, and their Morale improves by 1. They now have a +1 score point. How should our hero best use it? The rejection rate would now decrease with the score increasing. Hence it’s green in the table.

![Rejection-with-plus.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_plus_30cde165c3.png)

The TL;DR is that +1 to *Code* and +1 to *Solve* have comparable effects, and their effect is always significantly higher than the effect of +1 to *Communicate*.

The “top 25%” row is not very representative, as the 4s can not improve, and plenty of scores there already are the 4s.

For other rows, boosting *Communicate* by 1 turns out to barely affect the rejection rate. Boosting *Code* and/or *Solve*, on the other hand, can help flip up to 4 of 10 negative outcomes!

## Pragmatically speaking

Unfortunately, in real life we don’t have magic potions that +1 your interview performance. But we do have considerable control over selecting which score to improve. Of course, 4-4-4 is the holy grail, but during an interview it is often up to you to decide which score to focus on.

Here is a more realistic example. What if you could trade one score for another?

Again, the table shows the difference in rejection rates, the numbers in red are bad, and the numbers in green are good. The top row is removed because, as mentioned earlier, it is not representative.

![Rejection-with-swap.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Rejection_with_swap_398980dc53.png)

Nothing could be more clear. When in doubt, **trade *Communicate* for *Solve* or *Code*.** Period.

## Conclusions, and why communication still matters

The bottom line: If your objective is to get a “yes,” it is far less damaging to score 1 rating point lower on *Communicate*, and it is much more beneficial to score 1 rating point higher on *Code* and/or on *Solve*. Talk is indeed cheap, and indeed, coding and problem solving is what you need to show in a coding interview.

Keep in mind that, obviously, these insights are valid for coding interviews. In behavioral interviews, the technical skill signal is the least valuable, while problem solving is king. And in systems design interviews, extra points on communication skills, while still the least valuable of three, are not worth exchanging for additional points in tech and problem-solving skills.

It is worth noting that practice coding interviews, while highly useful, are not 100% representative of real-life interviews. A real-life interview, even if explicitly focused on coding, is a mixture of everything.

**The more senior the position, the more vital systems design and behavioral skills become. This statement is harder to prove with data, but every interviewer I spoke with agrees—at least within the standard L3 to L6 range, from a junior to staff.**

Thus, we can conclude that:

- For L3 and L4, you are better off ignoring the communication score entirely, *as long as you are consistently scoring 2+ on it*.
- For L5+ interviews, where communication skills are far more important, standard coding practice sessions are not the best way to improve on them. Systems design practice, as well as the behavioral interviews, are there to help.

Good luck, and have fun on interviewing.io!

1. Of course, it’s best to postpone the interview in such a situation—or even if you’re just feeling unprepared. Seriously, official advice endorsed by the interviewing.io team here: Do not take your interviews when you’re not in the best shape. Those companies can wait, and in our experience, recruiters are very supportive of you taking the time you need to prepare and put your best foot forward. [↩](#user-content-fnref-1)


# [Why engineers don’t like take-homes – and how companies can fix them](https://interviewing.io/blog/why-engineers-dont-like-take-homes-and-how-companies-can-fix-them)

By Aline Lerner | Published: July 16, 2024; Last updated: July 23, 2024

> *[My experiences with take-homes] drive home the idea that this employer doesn't care if you are a carbon-based life form, as long as code comes out of one or more of your orifices.*

Take-home assignments *could*, in theory, be great for both companies and candidates. What better, fairer way to evaluate someone’s ability to do the work… than to have them do the work?

Unfortunately, in practice, take-homes typically suck for engineers.

We surveyed almost 700 of our users about their experiences with take-homes and interviewed a handful more for deeper insights. We learned a lot—mostly about candidates' poor experiences and negative feelings toward take-homes. They take a lot of time. They don’t respect candidates’ time. Candidates often get no feedback. And candidates are almost never compensated.

The good news? Turns out there are some pretty simple things companies can do to vastly improve their take-home assignments. But before we dive into that…

## Why do companies use take-home assignments?

Take-homes vary a ton by role and company in terms of the types of questions, subject matter, length, and intensity. At their simplest, take-homes can be the same questions as in an algorithmic interview, except done asynchronously. The other extreme is asking candidates to build an entire app and deploy it.

We were surprised to see how often companies use take-homes. About 85% of our users got one at some point in their career, independent of their experience level. Of the users who encountered them, they tended to see them as part of the process about 20% of the time, again, independent of their experience level[1](#user-content-fn-1).

Why are take-homes relatively popular among employers? They mostly use them to save time in the hiring process. There are, however, some more noble reasons a company might use a take-home assignment:

- Get better signal during the interview process, as a take-home can be more indicative of actual work
- Get a candidate's best work out of them in a lower-stress environment than a live algorithmic interview and/or attract candidates who don’t like algorithmic interviews (of which there are [plenty](https://hn.algolia.com/?q=coding+interviews))
- Broaden the candidate pool by offering a way in for candidates from non-traditional backgrounds—in lieu of a resume screen, which they’d likely fail, candidates can do choose to do an assignment

Accordingly, here’s the relevant part of a great conversation between Vincent Woo of CoderPad and Patrick McKenzie (known to Hacker News folks as patio11) of Stripe, formerly of [Starfighter](https://www.kalzumeus.com/2015/03/09/announcing-starfighter/).

Vincent: *What general sort of high level change do you think that recruiters at tech
companies that are roughly Stripe’s size or bigger ought to make?*

Patrick: *If I could wave a magic wand and sell the world on one concept, it would be selling the world on the desirability of work sample testing… where the way to tell if someone is good at something is to have them do the something.*

Despite enthusiasm for the theory of take-homes and some very well-intentioned reasons, candidates overwhelmingly don’t like take-homes. Here’s why.

## Why don’t candidates like take-homes? It’s about value asymmetry.

**Though users expressed a lot of frustration with take-homes, we were surprised to see very few take a hard-line stance and refuse to do them. Only 6% outright refuse, and 66% of people complete take-homes all or most of the time.** Surprisingly, these stats didn’t really change when we looked only at senior engineers. I was expecting that experienced engineers would do them almost never, if at all, but that’s not what the data shows. It’s possible that seniors are just louder in their disapproval.

![How often people do take-homes.png](https://strapi-iio.s3.us-west-2.amazonaws.com/How_often_people_do_take_homes_e85d712b9b.png)

Nevertheless, the more desirable a company, the more likely candidates will do the take-home and feel OK about it—70% told us they completed them because they “Really wanted to work at the company and were willing to do what it took.”

> *I found Weedmaps [to be] a very interesting company. They were the first marijuana related company to IPO. So you'd be on the frontier working for them. I found that exciting. So I applied and they had [a] take-home. I was like, sure, of course I’ll do this.*

Other reasons our users gave for completion included: “Because the take-home would be discussed at the onsite” (38%) and “Interesting/cool assignment” (37%). However, many of those who did finish them had such a poor experience that they said they’d never apply to certain companies ever again. We’ll talk about what makes the experience poor in a little bit.

Of the people who refused to do at least one take-home at some point in their career, here were their reasons.

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FWhy_people_don_t_do_take_homes_f138c44568.png&w=1200&q=75)

Numbers don’t add up to 100 because it was a multi-select poll. In addition, we obviously don’t know how often take-homes are boring or unclear in the wild (probably often!), nor do we know how often companies pay people to do them (our guess, based on some napkin math, is >10% of the time). So we don’t have the true denominator for these stats. Nevertheless, we found these results insightful.

**The common thread among all of these reasons is value asymmetry.** The worst take-homes feel unrewarding to candidates. Even exploitative. Take-home assignments ask a lot of candidates: a significant investment of their time, with an often unclear scope, no guarantee of progressing to the next round, often without feedback, and almost always without compensation. Meanwhile, the company has basically invested nothing, except to send the task. We heard this a lot.

> *When I'm interviewing, I look for things that are proxies for valuing team members… or not. If they want me to do a take-home test, and they haven't even spent 30 minutes on a phone screen, I begin to sense an asymmetry in our relationship, with their time and resources being very valuable, and mine not being valuable at all.*

> *A divergence between how much effort they want me to put in, and how much they want to put in themselves. It signals that they are more worried about their time than mine, their costs than mine. It also means they underestimate how much effort it takes to write code, so that if I go to work for them, I am likely to face demands to work uncompensated overtime to meet their optimistic estimates.*

> *Spending five hours on their one-hour test for nothing leaves hard feelings. And if they ghost me after the test, I will happily tell every developer who asks what cheapskates they are.*

## Is there any relationship between who candidates are and their willingness to do take-homes?

Perhaps surprisingly, our data says no. We ran a regression to compare our survey respondents’ interview performance on our platform to how likely they were to do take-homes. The relationship was so weak as to be negligible.

Similarly, we ran a regression to see if people who *look* good on paper are more or less likely to do take-homes. The relationship there was negligible as well.

In other words, contrary to some popular opinions, you’re not necessarily weeding out your best candidates by doing take-homes, whether you define “best” in terms of how their resume looks or how they perform in interviews.

## How companies can make take-homes better (and why they probably should)

After reading the stats above, you might think that, despite their grumblings, candidates generally do take-homes, and the best candidates won’t be weeded out. So, if you’re one of the many companies that uses them, it may not make sense to invest your limited time into making them better. It’s not that simple. For some companies, like the FAANGs, who have extremely strong brands and are known to pay well, changes are probably not worth it, especially in this market, where junior and senior engineers are willing to jump through more hoops than ever before.

If you’re not a FAANG, though, listen up. Here are some questions you can ask yourself to figure out if you should indeed make some changes to your take-homes.

**First, take an honest look in the mirror and ask yourself about your brand strength. Are you a household name? Does having you on candidates’ resumes give them automatic prestige? Are you known to pay above market?** If the answer to *all* of these questions isn’t a resounding yes, your brand strength is probably not strong enough to make people jump through hoops.

Here’s a sketch to drive that point home. Unless you have a ton of brand strength, candidates’ willingness to jump through hoops drops off sharply.

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbrand_strength_vs_value_asymmetry_4bc96b83d4.png&w=1200&q=75)

In fairness, I drew this sketch in a boom market. Maybe it’s a bit less steep now, but honestly, I’m not sure.

If you aren’t maxing out on brand strength, there are two questions you should ask yourself:

1. What is my take-home completion rate?
2. What is my offer acceptance rate?

**If your completion rate is below, say, 60% (our data shows that candidates complete take-homes around 62% of the time), then it’s DEFINITELY time to make a change.** Honestly, regardless of what our data says, if people aren’t completing them at least 85% of the time, it’s *probably* time to make a change—losing more than 15% of your candidates to attrition at any given stage in the funnel is bad.

**What about your offer acceptance rate? If it’s less than 50% and you’re using a take-home already, there’s an opportunity to make some improvements.** You might be wondering what this has to do with take-homes in the first place. To answer that, let’s change how we think about different parts of the interview process. At face value, every part of the process is there to vet candidates, to determine if they’re the right fit for your organization. However, when used correctly, every part of your process should become a selling vehicle as well. This is *especially* important for companies who do not have a strong, well-known brand. The FAANGs can get away with using their interview processes primarily as vetting exercises because candidates are already sold on the pay or prestige or sometimes on the work and the product. When you don’t have an established brand, the candidates who come to you are, at best, open to learning more, and the interview process is the instrument that teaches them. Wield it accordingly.

Though we strongly advocate coming up with [great, unique interview questions](https://interviewing.io/blog/best-technical-interviews-common)[2](#user-content-fn-2) and making sure you have [great interviewers](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it), if your process does have a take-home component, it is one of the more overlooked parts of the process when it comes to selling. You have the opportunity to have someone *do the actual work that you do*! This is your chance to pick the coolest stuff you’ve worked on and serve it up to someone on a platter and make it stick in their brains and make them imagine what it’d be like to work on these kinds of problems every day! Why wouldn’t you jump at this opportunity?

You may think you don’t need to sell in this market. But just because employers have all the power right now, it doesn’t mean that will always be the case. And great senior engineers still have a lot of leverage.

If, after considering your take-home completion rate and your offer acceptance rate, it looks like you *do* need to make some changes, here are some practical tips, based on what we’ve learned from talking to our users (overwhelmingly senior engineers who are targeting top-tier companies—probably the people you want). Let’s start with what we just talked about: using the take-home as a selling vehicle.

### Make them interesting and relevant to the actual work

You're getting a chance to spend a couple of hours with somebody in a take-home (metaphorically). Why wouldn't you do everything you can to get them excited? **Pick a problem that you've worked on, and get them hooked on it. Pick the kernel of an interesting problem that you've solved, and build something around it that will challenge candidates. Something that gets them thinking, “I could have done that better” or “This is a different or more efficient way to do it.” That's going to be more effective than the standard perks many companies offer.**

> *[Best take-home I’ve seen was an] open-ended system design question on the type of system I would work on, was meant to simulate a team discussion on the system we needed to build, and was a great way for me to start thinking about what I'd be working on there.*

One way you could do this, is to have your engineering team keep a shared doc of ‘cool’ solutions they've found, or new things they've tried. These can serve as jumping-off points for creating your take-homes.

> *The… challenge was for an internal tooling team that specialized in incident response tools; their challenge was to create a scaled down version of a tool already in use at the company. The focus was more on understanding the domain and customer than wiring up a bunch of complicated stuff, and it was a delight. It being a greenfield also gave you an opportunity to showcase some software design skills. I did not get this job, but enjoyed the experience and still feel connected to the team.*

> *It was conceptually related to the sort of work the team was performing, but it was simplified and stand-alone enough to clearly not be unpaid labor for their product.*

Just be sure that when you come up with a practical problem that you strip out the annoying parts and focus only on the juicy kernel of the problem, the part that’s actually cool and lets the candidate be smart and creative. Don’t make them do grunt work or wrestle with their dev environment!

> *[This] was for a tooling team in Support, where they didn't have a lot of experience creating challenges or interviewing. One of the engineers took a difficult task that he had accomplished recently and just made that the take home challenge. It involved a lot of Ruby version conflict debugging. It was completely demoralizing and felt like hazing.*

### Keep them short

Candidates overwhelmingly favor take-homes that respect their time, i.e., short ones.

> *The best were short and brief, took no more than 2 hours and were directly related to what I would be doing on the job.*

> *Best are realistic and time bound, i.e., low time investment required.*

> *Short and quick take-homes are great.*

**Over 80% of survey respondents said that take-homes should take 4 hours or less, and a plurality thought that they should take 2 hours.**

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FHow_long_engineers_think_take_home_assignments_should_take_2a361d8e29.png&w=1200&q=75)

The outliers are interesting. Unfortunately, we missed an opportunity to ask about those in the survey.

Take-homes being short in theory is one thing… but we got a lot of feedback that take-homes often go far beyond how long companies tell candidates that take-homes should take.

#### Have someone on your team QA the actual length of the assignment

This disconnect between actual time and expected time is another instance of value asymmetry: companies didn't even bother to have someone on their team do the take-home themselves and time it.

> *I've done a couple of algorithm questions as take-homes that the interviewer said should take an hour. They routinely take longer than that.*

> *Companies will say, ‘don’t spend more than X hours on it,’ but then it actually takes 5X that amount. It's just too much time. And most of the time they didn’t give feedback at all even though I spent so much time on it.*

Companies should clearly communicate the expected time commitment for a take-home. And they should be realistic about how much time it should take.

> *Best was an interesting problem with a realistic time frame. They estimated 2 hours and it took me exactly that, which was refreshing and made the process seem fair.*

> *I've done a couple of exercises where they were truly respectful of my time. The tasks were small, clearly defined, and they gave me a realistic timeframe to complete them.*

> *For the best take-home I did, the company told me: ‘Tell us when you're going to start, and you'll have 4 hours. We'll be available over email that whole time, and we'll answer your questions within minutes.’ That felt respectful of my time, and was a more reliable signal of my capabilities than a 20-minute algorithms exercise.*

**To get a realistic estimate, companies could simply QA their own take-homes—just by having someone on your team take it, and time them.**

Still, some candidates will spend more time anyway, because they really want to work for a certain company. And it can be easy for candidates to go down a rabbit hole and get most of a take-home done in the first few hours, then spend another day or more, perfecting it to give themselves a better chance.

> *Do you want me to literally only work two hours on something that's clearly going to take 10 to really be able to have a conversation with you about how I think about things? Because two hours of decisions isn't as good as 10 hours of decisions.*

It’s okay if candidates want to put in extra time, but it should be made clear that that's not the expectation.

### Set a clear scope, related to the role

> *The scope was poorly defined, which made the assignment not only tedious, but also seemed like a waste of time.*

**Having clear, straightforward instructions can significantly enhance the take-home experience for candidates, making them feel purposeful and relevant. The best take-homes are those that directly align with the actual job.** When candidates are asked to solve a simplified version of a real problem your company is facing, it not only tests relevant skills, but also gives a candidate insight into what work at your company would be like, which is really what selling is. (This is, of course, assuming that you believe what you’re working on is cool… If you don’t, how will you ever convince others?)

> *One was a great learning experience because I had to solve a problem similar to what I would face in the job. It was concise and focused.*

Being able to ask questions before or during the take-home, e.g., having a direct contact point who’s an engineer at the company, can also be a big plus.

### Be thoughtful about where in your process you insert the take-home, and give candidates a choice between a take-home and something else

#### For take-homes early in the process, before speaking to a human

We expect that many of the companies reading this piece have take-homes as the first step in
their process, before ever talking to a human. This decision isn’t very popular.

> *If a company calls me for a screening interview, it means they've read my resume and cover letter, and not immediately dismissed my application as inappropriate. It also means they're putting a person on a phone call with me for the duration of the screening interview. This is symmetric, my effort matches theirs.*

> *[I] did a take home that was automatically sent to me after applying, before even speaking to a human. The feedback was 'you are too expensive,' no feedback on the coding.*

> *Company needs to build an investment with the candidate first—they shouldn't ask for it before meeting the candidate at least once.*

**That said, take-homes can be a great way to let candidates who don’t look good on paper show what they can do, and those candidates are more likely to complete them.** To prevent candidate unhappiness/attrition, give them a choice. Either submit a resume or do the take-home assignment or both. If you go this route, though, design a take-home that you trust. We’ve seen companies take this approach and then throw out perfect-scoring take-homes when they didn’t like the resume. If you use a take-home, then respect your candidates enough to follow up with the ones who’ve done well, even if they don’t look good on paper.

#### For take-homes later in the process

**Another way to make take-homes purposeful is to give your candidates an explicit choice about whether they’d rather do a technical interview or a take-home.** Engineers are, in fact, split on which they’d prefer. Giving them a choice allows them to showcase their skills in the format they prefer and feel best prepared for. If you have good questions, you should be able to get good signal from either.

Only 10% of respondents told us that they were given the choice of take-home versus technical interview. So there is an opportunity for more companies to do this. It's a candidate-friendly gesture that shows empathy and can help candidates shine.

Many candidates have spent months preparing for standard technical interviews. So it can feel anticlimactic when they find out that a company they're excited about doesn't do them. Others get so nervous in a live interview that they don’t perform.

> *I rarely don't pass take-home assessments, but I often fail to pass live interviews.*

Interestingly, one user we interviewed told us that they spend far less time on a take-home compared to preparing for a live technical interview. And because of that they prefer take-homes.

> *I prefer take-homes over all other assessments because I find I spend far less time on take-homes then I do preparing for live interviews. It's hard to overestimate the amount of extra time I spend preparing for a technical interview, compared to doing a take-home—for me it’s maybe 10 times as much. I've spent hundreds of hours, maybe 500 hours, over the course of my career preparing for technical interviews. And if I have one pop up, I can't just drop everything and do it right away. I have to spend a lot of extra time just ramping up for a particular interview, in addition to the hundreds of hours that I've done.*

### Give candidates a good, rational reason to do the take-home

Clear communication about the purpose of a take-home in the hiring process, as well as why it’s rational to spend time on it, is important for candidates. It’s a way to make sure it feels purposeful and not like a random task.

One way to make the take-home feel deliberate is to replace some parts of your process with it. A standard process without a take-home has a recruiter call followed by a technical phone screen followed by an onsite (virtual or otherwise). The technical phone screen usually lasts about an hour. The onsite usually lasts 6 hours.

Let’s say your take-home takes 2 hours to do. You can make it replace the phone screen and one of the onsite rounds, which nets out to the same number of hours spent. If you go this route, we recommend doing the math *explicitly* for candidates and showing them that the time they spend on the take-home is equivalent to the time they’d be spending on a process without it.

Another way to make the take-home feel deliberate is to incorporate it into the onsite, where at least one of the rounds, if not more, will include a code review and/or thoughtful discussion about tradeoffs and choices made. This should be standard practice, but isn’t always. 32% of our users said companies had told them this, and it was the reason they decided to do a take-home.

> *The best take-homes were ones that we discussed in the first rounds of interviews. The worst ones were ones that I submitted and we never talked about them again.*

**Ideally, you do both of these things together, and very clearly explain to candidates both the math and how the take-home informs the content of the onsite.**

### Compensate candidates

Probably the most striking result of our survey was that 58% of candidates think that they deserve compensation for completing take-homes. Yet only 4% reported ever receiving it. Compensation can shift candidate perceptions of the hiring process and of the company:

> *They compensated me for my time, which made the process feel very professional and respectful.*

> *It was a completely open source codebase and so their process was the exact same as someone that was an employee: here's the ticket with the information to do it, set up the environment, download all the code, get everything running. They gave me a few different tasks I could choose from, I could pick two, and if I completed them I would be compensated a fixed rate, which was $100 for each task. Which in terms of the time I spent on it, is still really cheap for them.*

If candidates know they’re going to be paid for their work on a take-home, they’ll be more likely to complete it as well.
Compensating candidates is a clear gesture that shows you value their time and effort, that there’s more symmetry in the relationship. It also goes hand-in-hand with time: paying also forces a company to scope the take-home to a reasonable number of hours. So it's a forcing function for good behavior—if you can’t afford much, then don't make the take-home too long!

#### How much to pay?

Responses about how much companies should pay for take-home assignments were split.

1. **Fixed amounts**: Just over half of respondents (52%) suggested specific and reasonable fixed amounts, ranging from $50 to $500.
2. **Hourly rates**: The other almost half (47%) favored an hourly rate, with suggestions ranging from $50 an hour and upward (average of $217 an hour). Some suggested that the rate should correspond to the salary of the position being applied for, or be comparable to what an employee at that level and company would earn. As one user put it:

> *Maybe just pay market?*

One antipattern when it comes to comp is NOT having a set rate, asking the candidate to name their price, and thereby putting the candidate in a position where they feel like they need to negotiate. In this scenario, the candidate has to negotiate twice: once on the take-home and once on their actual comp, with the worry that negotiating too aggressively on the take-home might count them out… or not aggressively enough anchoring the company to lower compensation down the line. No one needs these mind-games in an already stressful process. Just have a set rate, for god’s sake.
3. **Symbolic compensation**: This is probably not the best option, but a handful of survey respondents (1%) did mention that a minimal symbolic compensation would do. While 1% is small, we found this interesting to include because, when we interviewed people, two of them mentioned this.

> *I think any compensation at all has symbolic value. A $100 Amazon card would impress me. A $50 Amazon card and a company t-shirt would at least not insult me. It’s kind of a consolation prize to say, ‘No hard feelings.’*

> *I feel like a couple meal vouchers would do it these days.*

**Of these options, we’d recommend a reasonable fixed amount based on the task, and the actual time it’s supposed to take.** And of course you know how long it takes because you had one of your engineers do it themselves, right? Right??

### Give feedback

**Lack of feedback was the primary reason our survey respondents said their experience with a given company was bad.** Regardless of interview type, we’re always [pro feedback](https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire), but feedback is especially important for take-homes, because in a way they ask more of a candidate. Offering constructive feedback, regardless of the hiring decision, respects the candidate's effort on the take-home.

Despite the time and effort they invested in completing take-homes, many of our survey respondents said they received no feedback at all. This was seen as demoralizing, and it deterred candidates from applying to future opportunities at those companies.

> *Getting rejected without having a chance to discuss the code with anyone is a terrible experience.*

> *It is really discouraging spending a large amount of time to find out you are rejected without explanations.*

> *They provided no feedback after submission, which made the whole effort feel unappreciated and one-sided.*

Incidentally, the main reason companies don’t give feedback is fear of getting sued. As it turns out, [literally ZERO companies (at least in the US) have ever been sued by an engineer](https://interviewing.io/blog/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it) who received constructive post-interview feedback.

*Thanks to [Dan Fennessy](https://www.linkedin.com/in/danielfennessy/) for all the behind-the-scenes work on this post.*

1. Some users told us they’re seeing take-homes more recently, likely a function of worsening market conditions—the less leverage talent has, the more hoops companies can ask them to jump through. [↩](#user-content-fnref-1)
2. You might have to do this soon anyway, in all your interviews, to [ward off against AI-driven cheating](https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews). [↩](#user-content-fnref-2)


# [Does posting Open To Work on LinkedIn help or hurt? A tale of two labor markets.](https://interviewing.io/blog/whos-open-to-work-a-tale-of-two-labor-markets)

By Aline Lerner and Maxim Massenkoff | Published: April 10, 2023; Last updated: May 28, 2023

*Note: This post originally appeared in [TechCrunch on April 3, 2023](https://techcrunch.com/2023/04/03/should-you-post-that-youre-opentowork-a-tale-of-two-labor-markets/).*

LinkedIn members face a dilemma when choosing whether to mark themselves as #OpenToWork. On the one hand, it sends a clear and useful signal to recruiters and hiring managers that you’re open to new job opportunities. On the other hand, the badge can lump you with a crowd of people who are more likely to be unemployed or unhappy with their jobs, which could be seen as a negative sign – you know, the whole Groucho Marx thing:\* I don’t want to belong to any club that would accept me as one of its members.\*

If you’re job hunting, should you list yourself as Open To Work? Does doing so carry a negative signal? And with the recent deluge of layoffs at tech companies, has the meaning of #OpenToWork changed? We decided to find out!

## The correlation between LinkedIn #OpenToWork and interviewing.io interviews

interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. In our lifetime, we’ve hosted over 100k technical interviews, split between mocks and real ones. To test whether listing yourself as #OpenToWork on the profile section of your LinkedIn is a good thing to do, we aggregated pass/fail rates in the interviews our users did and cross-referenced them with whether users marked themselves as #OpenToWork on their LinkedIn profiles. We also made sure to check their LinkedIns twice: once in early 2021, when there were practically [no tech layoffs](https://layoffs.fyi/), and again in early 2023, in the wake of the worst round of tech layoffs since 2001.

Why did we check twice? Economic theory suggests that the people laid off or searching for a job in boom times are different from those laid off in a recession. If you are let go or cannot find a job when companies are flush, it might be that you were laid off for performance reasons or can never get past a screening call. On the other hand, someone who suffered from an across-the-board layoff in a time of economic crisis could be perfectly-capable: just a casualty of macroeconomic forces.

Worried about layoffs, or just want to be prepared? Sign up for anonymous mock interviews with engineers from top companies.

We found that being Open To Work is far more common now. Among the over ten thousand people that we had LinkedIn data for, **only 1.4% had the badge in 2021 compared to 4.2% in the first quarter of 2023**.

## Is posting #OpenToWork on LinkedIn good or bad?

We found that being Open To Work was a *negative* signal for those who had it up in 2021, a boom time for tech hiring. The chart below shows the percentage of people who passed their interviews—our summary measure of candidate performance. On average, about 51 percent of candidates pass their interviews. In contrast, those with #OpenToWork badges in 2021 were fully 7 percentage points below that, at 44%.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/percent_hirable_and_open_to_work_f863aa6a5b.png)

The finding confirms that there can be negative selection among job seekers. Being upfront about looking for a job—at least in 2021—was indeed a bad sign.

**But strikingly, this result flips in present times. Engineers who are currently Open To Work are actually positively selected relative to everyone else. Using the LinkedIn Open To Work feature is a good sign in these rough times: 56% of engineers tagged as OpenToWork passed their interviews, 5 percentage points more than average. The difference in these effects is highly statistically significant.** And we get the same results when we leave out anyone who has worked at a FAANG company—in case the recent [engineering layoffs](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments) simply flooded the market with engineers from those top-tier companies.

## Before using the Open To Work feature, consider the job market

Anecdotally, we’ve heard that layoffs in 2022 and 2023, especially at large companies, were not performance-based. In many cases, cuts were based on an employee's start date—those who hired last were the first on the chopping block. In some cases, entire teams were cut – if a team’s product wasn’t close enough to revenue, they ended up on the chopping block, independently of how strong the engineers on that team actually were.
![](https://strapi-iio.s3.us-west-2.amazonaws.com/tweet_google_layoffs_81e3f27bb4.png)

![](https://strapi-iio.s3.us-west-2.amazonaws.com/blind_layoffs_aren_t_about_performance_d0b2a7db8e.png)

We were excited to see that this anecdotal perception held up in the data.

That said, people who are OpenToWork have not necessarily been laid off. In uncertain times, it’s more common for workers to be openly searching for jobs. The macroeconomic forces could normalize a more aggressive on-the job search, which might also pull more talented—but still employed—people into checking OpenToWork.

Regardless of layoff composition, in boom times, openly searching for a job can be a negative signal. But during downturns, the rules change, and openly looking for work becomes much more “normal”.

The signal to hiring managers is clear: because of the rocky economic times, good workers are out there looking for jobs, and displaying #OpenToWork on your profile photo is no longer a negative signal. While hiring has slowed across many tech companies, those with the budget will find a better pool to choose from, and those who were laid off in late 2022/early 2023 might be among the most impressive candidates.

If you use LinkedIn and are looking for a new job, we wouldn’t go as far as recommending that you list yourself as Open To Work because of the potential residual bias associated with that tag. We would, however, encourage you to feel no shame about having been impacted by layoffs.

The data is clear, after all—the pool of engineering job seekers has never been more talented.


# [Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews)

By Aline Lerner | Published: December 27, 2016; Last updated: May 1, 2023

The first blog post I published that got any real attention was called “[Lessons from a year’s worth of hiring data](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/)“. It was my attempt to understand what attributes of someone’s resume actually mattered for getting a software engineering job. Surprisingly, as it turned out, where someone went to school didn’t matter at all, and by far and away, the strongest signal came from the number of typos and grammatical errors on their resume.

Since then, I’ve discovered (and written about) how [useless resumes are](https://blog.alinelerner.com/resumes-suck-heres-the-data/), but ever since writing that first post, I’ve been itching to do something similar with interviewing.io’s data. For context, interviewing.io is a platform where people can practice technical interviewing anonymously and, in the process, find jobs — do well in practice, and you get guaranteed (and anonymous!) technical interviews at companies like Uber, Twitch, Lyft, and more. Over the course of our existence, we’ve amassed performance data from thousands of real and practice interviews. **Data from these interviews sets us up nicely to look at what signals from an interviewee’s background might matter when it comes to performance.**

As often happens, what we found was surprising, and some of it runs counter to things I’ve said and written on the subject. More on that in a bit.

When an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (check out our [recordings](https://interviewing.io/mocks) page to see this in action). Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, and Uber, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more.

![Screenshot of the interview feedback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffeedback_circled_40095dd6e4.png&w=1200&q=75 "Example feedback form")

**To run the analysis for this post, we cross-referenced interviewees’ average technical scores** (circled in red in the feedback form above) **with the attributes below to see which ones mattered most**.[1](#user-content-fn-1)

- Attended a top computer science school
- Worked at a top company
- Took classes on Udacity/Coursera[2](#user-content-fn-2)
- Founded a startup
- Master’s degree
- Years of experience

**Of all of these, only 3 attributes emerged as statistically significant: top school, top company, and classes on Udacity/Coursera.** Apparently, as the fine gentlemen of Metallica once said, nothing else matters. In the graph below, you can see the [effect size](https://en.wikipedia.org/wiki/Effect_size) of each of the significant attributes (attributes that didn’t achieve significance don’t have bars).

As I said at the outset, these results were quite surprising, and I’ll take a stab at explaining each of the outcomes below.

## Top school & top company

Going into this, I expected top company to matter but not top school. The company thing makes sense — you’re selecting people who’ve successfully been through at least one interview gauntlet, so the odds of them succeeding at future ones should be higher.

Top school is a bit more muddy, and it was indeed the least impactful of the significant attributes. Why did schooling matter in this iteration of the data but didn’t matter when I was looking at resumes? I expect the answer lies in the disparity between performance in an isolated technical phone screen versus what happens when a candidate actually goes on site. With the right preparation, the technical phone interview is manageable, and top schools often have rigorous algorithms classes and a culture of preparing for technical phone screens (to see why this culture matters and how it might create an unfair advantage for those immersed in it, see my post about how we need to [rethink the technical interview](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview)). Whether passing an algorithmic technical phone screen means you’re a great engineer is another matter entirely and hopefully the subject of a future post.

## Udacity/Coursera

MOOC participation (Udacity and Coursera in particular, as those were the ones interviewing.io users gravitated to most) mattering as much as it did (and mattering way more than pedigree) was probably the most surprising finding here, and so it merited some additional digging.

In particular, I was curious about the interplay between MOOCs and top schools, so I partitioned MOOC participants into people who had attended top schools vs. people who hadn’t. When I did that, something startling emerged. **For people who attended top schools, completing Udacity or Coursera courses didn’t appear to matter. However, for people who did not, the effect was huge, so huge, in fact, that it dominated the board.** Moreover, interviewees who attended top schools performed significantly worse than interviewees who had not attended top schools but HAD taken a Udacity or Coursera course.

So, what does this mean? Of course (as you’re probably thinking to yourself while you read this), correlation doesn’t imply causation. As such, rather than MOOCs being a magic pill, I expect that people who gravitate toward online courses (and especially those who might have a chip on their shoulder about their undergrad pedigree and end up drinking from the MOOC firehose) already tend to be abnormally driven. But, even with that, I’d be hard pressed to say that completing great online CS classes isn’t going to help you become a better interviewee, especially if you didn’t have the benefit of a rigorous algorithms class up until then. Indeed, a lot of the courses we saw people take focused around algorithms, so it’s no surprise that supplementing your preparation with courses like this could be tremendously useful. Some of the most popular courses we saw were:

**Udacity**

- [Design of Computer Programs](https://www.udacity.com/course/design-of-computer-programs--cs212)
- [Intro to Algorithms](https://www.udacity.com/course/intro-to-algorithms--cs215)
- [Computability, Complexity & Algorithms](https://www.udacity.com/course/computability-complexity-algorithms--ud061)

**Coursera**

- [Algorithms Specialization](https://www.coursera.org/specializations/algorithms)
- [Functional Programming Principles in Scala](https://www.coursera.org/learn/progfun1)
- [Machine Learning](https://www.coursera.org/learn/machine-learning)
- [Algorithms on Graphs](https://www.coursera.org/learn/algorithms-on-graphs)

## Founder status

Having been a founder didn’t matter at all when it came to technical interview performance. This, too, isn’t that surprising. The things that make one a good founder are not necessarily the things that make one a good engineer, and if you just came out of running a startup and are looking to get back into an individual contributor role, odds are, your interview skills will be a bit rusty. This is, of course, true of folks who’ve been in industry but out of interviewing for some time, as you’ll see below.

## Master’s degree & years of experience

No surprises here. I’ve ranted quite a bit about the [disutility of master’s degrees](https://blog.alinelerner.com/how-different-is-a-b-s-in-computer-science-from-a-m-s-in-computer-science-when-it-comes-to-recruiting/), so I won’t belabor the point.

Years of experience, too, shouldn’t be that surprising. For context, our average user has about 5 years of experience, with most having between 2 and 10. I think we’ve all anecdotally observed that the time spent away from your schooling doesn’t do you any favors when it comes to interview prep. You can see a scatter plot of interview performance vs. years of experience below as well as my attempt to fit a line through it (as you can see, the R^2 is piss poor, meaning that there isn’t a relationship to speak of).

## Closing Thoughts

If you know me, or even if you’ve read some of my writing, you know that, in the past, I’ve been quite loudly opposed to the concept of pedigree as a useful hiring signal. With that in mind, I feel like I owe clearly acknowledge, up front, that we found this time runs counter to my stance. But that’s the whole point, isn’t it? You live, you get some data, you make some graphs, you learn, you make new graphs, and you adjust. **Even with this new data, I’m excited to see that what mattered way more than pedigree was the actions people took to better themselves (in this case, rounding out their existing knowledge with MOOCs), regardless of their background.**

Most importantly, these findings have done nothing to change interviewing.io’s core mission. We’re creating an efficient and meritocratic way for candidates and companies to find each other, and as long as you can code, we couldn’t care less about who you are or where you come from. In our ideal world, all these conversations about which proxies matter more than others would be moot non-starters because coding ability would stand for, well, coding ability. And that’s the world we’re building.

*Thanks to Roman Rivilis for his help with data annotation for this post.*

1. For fun, we tried relating browser and operating system choice to interview performance, (smugly) expecting Chrome users to dominate. Not so. Browser choice didn’t matter, nor did what OS people used while interviewing. We got this data from looking at interviewees’ LinkedIn profiles. [↩](#user-content-fnref-1)
2. We got this data from looking at interviewees' LinkedIn profiles. [↩](#user-content-fnref-2)

[Technical interview performance is kind of arbitrary. Here’s the data.](/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data)


# [The definitive list of companies who are hiring engineers right now](https://interviewing.io/blog/companies-hiring-engineers-2022)

By Aline Lerner | Published: September 19, 2022; Last updated: June 18, 2024

A little over a month ago, we conducted a survey among our users to get to the bottom of what was happening with hiring freezes at Google and Facebook and [published the results](https://interviewing.io/blog/google-facebook-hiring-freeze). The post went live on August 1st, 3 days before Google’s two-week freeze was due to come to an end, and I even called out that the post might soon be rendered moot. It looks like that did not come to pass – now, a month later, the giants still appear to be largely frozen (with some specific exceptions… see the post about hiring freezes for details).

interviewing.io is both a mock interview platform and an eng hiring marketplace (engineers use us for technical interview practice, and top performers get fast-tracked at companies), so we have some unique insights into how these freezes have affected engineers’ behavior. In the wake of most FAANGs freezing hiring, we’ve seen a not entirely unexpected uptick in our users’ appetites for interviewing at non-FAANG companies. What *was* surprising, however, was engineers’ appetite for technical interview practice – though we’ve seen a marked decrease in purchases of Google and Facebook-specific mock interviews, people are practicing for technical interviews *in general* at about the same clip as before the freezes.

Armed with these indicators, and in the spirit of being useful during a hard and uncertain time, we thought it’d be interesting to survey our users once again to figure out 1) whether engineers are battening down the hatches and laying low for a while or if, despite the looming recession, they’re still actively considering new jobs and 2) which companies are actually hiring engineers right now.

**TL;DR** There are lots of engineers actively looking. There are also lots of companies who are actively hiring. Scroll down to see a massive list.

## What we asked

We were curious about 2 things:

1. Are you looking right now, and if so, where are you interviewing?
2. Is your current employer actively hiring engineers?

Below are the actual survey questions:

![Screenshot of the survey question: which companies are you interviewing right now](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fwhere_interviewing_b5ef2cf127.png&w=1920&q=75 "Survey question #1")

![Screenshot of the survey question: is the company you currently work at actively hiring engineers?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fwhere_hiring_d0172e4456.png&w=1920&q=75 "Survey question #2")

## Are engineers actively looking for work, and are companies actively hiring engineers?

By all accounts, yes, it looks like they are, on both counts. Of survey respondents, 49% said they were in the middle of an active job search, and 51% said that their employer was actively hiring.

## Big list of companies who are hiring engineers right now

We collated responses to both questions above and came up with a list of employers who are hiring. Because we are based in the U.S. and most of our users are here, we confined this list to either companies who are based in the U.S. or are actively hiring here. Note that the logos, descriptions, locations, and sizes below are enriched via Clearbit. We spot-checked the list, and made some corrections/additions here and there, but it’s possible we missed something. If your company is listed here and something is incorrect, let us know by emailing [hello@interviewing.io](mailto:hello@interviewing.io).

## How interviewing.io can help you

If you’re a company who’s hiring engineers and want access to the best engineers in the world, *we have them*. Our candidates convert about 3X better than other sources (including internal referrals) because we use technical interview data, instead of resumes, to identify top performers. Our candidates are the two things you want: they’re really good, and they’re looking right now. Sign up, and [start talking to actually great candidates](https://interviewing.io/employers/). We’ve hired for FAANGs and startups alike and have helped thousands of engineers find great jobs.


# [If you care about diversity, don't just hire from the same five schools](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools)

By Meena Boppana | Published: October 23, 2017; Last updated: June 20, 2023

**EDIT:** Our university hiring platform is now on [Product Hunt](https://www.producthunt.com/products/interviewing-io#university-hiring-by-interviewing-io)!

If you’re a software engineer, you probably believe that, despite some glitches here and there, folks who have the technical chops can get hired as software engineers. We regularly hear stories about college dropouts, who, through hard work and sheer determination, bootstrapped themselves into millionaires. These stories appeal to our sense of wonder and our desire for fairness in the world, but the reality is very different. For many students looking for their first job, the odds of breaking into a top company are slim because they will likely never even have the chance to show their skills in an interview. For these students (typically ones without a top school on their resume), their first job is often a watershed moment where success or failure can determine which opportunities will be open to them from that point forward and ultimately define the course of their entire career. In other words, having the right skills as a student is nowhere near enough to get you a job at a top-tier tech company.

To make this point concrete, **consider three** (fictitious, yet indicative) **student personas, similar in smarts and skills but attending vastly different colleges. All are seeking jobs as software engineers at top companies upon graduation**.

Mason goes to Harvard. He has a mediocre GPA but knows that doesn’t matter to tech companies, where some of his friends already work. Come September, recent graduates and alums fly back to campus on their company’s dime in order to recruit him. While enjoying a nice free meal in Harvard Square, he has the opportunity to ask these successful engineers questions about their current work. If he likes the company, all he has to do is accept the company’s standing invitation to interview on campus the next morning.

[Emily](https://blog.andrewhoang.me/how-to-be-emily/) is a computer science student at a mid-sized school ranked in the top 30 for computer science. She has solid coursework in algorithms under her belt, a good GPA, and experience as an engineering intern at a local bank. On the day of her campus’s career fair, she works up the courage to approach companies – this will be her only chance to interact with companies where she dreams of working. Despite the tech industry being casual, the attire of this career fair is business formal with a tinge of sweaty. So after awkwardly putting together an outfit she would never wear again[1](#user-content-fn-1), she locates an ancient printer on the far side of campus and prints 50 copies of her resume. After pushing through the lines in order to line up at the booths of tech companies, she gives her resume to every single tech company at the fair over the course of several hours. She won’t find out for two more weeks if she got any interviews.

Anthony goes to a state school near the town where he grew up. He is top in his class, as well as a self-taught programmer, having gone above and beyond his coursework to hack together some apps. His school’s career fair has a bunch of local, non-tech employers. He has no means of connecting with tech companies face-to-face and doesn’t know anyone who works in tech. So, he applies to nearly a hundred tech companies indiscriminately through their website online, uploading his resume and carefully crafted cover letter. He will probably never hear from them.

## Career fair mania

The status quo in university recruiting revolves around career fairs and in-person campus recruiting, which have serious limitations. For one, they are extremely expensive, especially at elite schools. Prime real estate at the MIT career fair will run you a steep $18,000, for entry alone. That’s not counting the price of swag (which gets more exorbitant each year), travel, and, most importantly, the opportunity cost of attending engineers’ time. **While college students command the lowest salaries, it’s not uncommon for tech companies to spend 50% more on recruiting a student than a senior engineer.**

At elite schools, the lengths to which companies go to differentiate themselves is becoming more exorbitant with each passing year. In fact, students at elite colleges suffer from company overload because every major tech company, big and small, is trying to recruit them. All of this, while students at non-elite colleges are scrambling to get their foot in the door without any recruiters, let alone VPs of high-profile companies, visiting their campus.

Of course, due to this cost, companies are limited in their ability to visit colleges in person, and even large companies can visit around 15 or 20 colleges at most. This strategy overlooks top students at solid CS programs that are out of physical reach.

In an effort to overcome this, companies are attending conferences and hackathons out of desperation to reach students at other colleges. The sponsorship tier for the Grace Hopper Conference, the premier gathering for women in tech, tops out at $100,000, with the sponsorship tier to get a single interview booth starting at $30,000. Additionally, larger companies send representatives (usually engineers) to large hackathons in an effort to recruit students in the midst of a 48-hour all-nighter. However, the nature of in-person career fairs and events are that not all students will be present. Grace Hopper is famously expensive to attend as a student, especially when factoring in airfare and hotel.

This cost is inefficient at best, and prohibitive at worst, especially for small startups with low budget and brand. Career fairs serve a tiny portion of companies and a tiny portion of students, and the rest are caught in the pecuniary crossfire. **Demand for talented engineers out of college who bring a different lived experience to tech has never been higher, yet companies are passing on precisely these students via traditional methods.** Confounding the issue even further is the fundamental question of [whether having attended a top school has much bearing on candidate quality](https://interviewing.io/blog/lessons-from-3000-technical-interviews) in the first place (more on that in the section on technical screening below).

![Chart showing how career fairs suck](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1896f_threshold_4_f34775ee84.webp%3Fupdated_at%3D2022-11-21T19%3A55%3A16.652Z&w=1920&q=75 "Career fairs suck")

## Homogeneity of hires

The focus of companies on elite schools has notable, negative implications for the diversity of their applicants. In particular, many schools that companies traditionally visit are notably lacking in diversity, especially when it comes to race and socioeconomic status. According to a [survey of computer science students at Stanford](https://medium.com/@jcueto/race-and-gender-among-computer-science-majors-at-stanford-3824c4062e3a), there were just fifteen Hispanic female and fifteen black female computer science majors in the 2015 graduating class *total*. In this analysis, the Stanford 2015 CS major was 9% Hispanic and 6% black. According to a [2015 analysis](https://medium.com/@winniewu/race-and-gender-among-computer-science-concentrators-at-harvard-1c1943a20457), the Harvard CS major was just 3% black and 5 percent Hispanic. Companies that are diversity-forward and constrained to recruiting at the same few schools end up competing over this small pool of diverse students. Meanwhile, there is an entire ocean of qualified, racially diverse students from less traditional backgrounds whom companies are overlooking.

The focus on elite schools also has meaningful implications on socioeconomic diversity. According to a detailed [New York Times infographic](https://www.nytimes.com/interactive/2017/01/18/upshot/some-colleges-have-more-students-from-the-top-1-percent-than-the-bottom-60.html?mcubz=0&_r=0), “four in 10 students from the top 0.1 percent attend an Ivy League or elite university, roughly equivalent to the share of students from poor families who attend any two- or four-year college.” The infographic highlights the rigid segmentation of students by class background in college matriculation.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Where_Todays_25_year_olds_went_to_college_b8f73e7688.webp?updated_at=2022-11-21T20:14:07.383Z)

Source: [New York Times](https://www.nytimes.com/interactive/2017/01/18/upshot/some-colleges-have-more-students-from-the-top-1-percent-than-the-bottom-60.html)

The article finds that the few lower-income students who end up at elite colleges do about as well as their more affluent classmates but that attending an elite versus non-elite college makes a huge difference in future income.

The focus of tech companies on elite schools lends credence to this statistic, codifying the rigidity with which students at elite college are catapulted into the 1 percent, while others are left behind. Career-wise, it’s that first job or internship you get while you’re still in school that can determine what opportunities you have access to in the future. And yet, students at non-elite colleges have trouble accessing these very internships and jobs, or even getting a meager first round interview, contributing to the lack of social mobility in our society not for lack of skills but for lack of connections. This sucks. A lot.

## The technical screen

Let’s return to our three students. Let’s say that Emily, the student who attended her college’s career fair, gets called back by one or two companies for a first round interview if her resume meets the criteria that companies are looking for. Not having an internship at a top tech company already — quite the catch-22 — puts her at a disadvantage. Anthony has little to no chance of hearing back from employers via his applications online, but let’s say that by some miracle lands a phone screen with one of the tech giants (his best shot, as there are more recruiters to look through the resume dump on the other end).

What are their experiences when it comes to prepping for upcoming technical interviews?

Mason, the Harvard student, [attends an event](https://www.facebook.com/events/138642833412845/) on campus with Facebook engineers teaching him how to pass the technical interview. He also accepts a few interviews at companies he’s less excited with for practice, and just in case. While he of course needs be sharp and prepare in order to get good at these sorts of algorithmic problems, he has all of the resources he could ask for and more at his disposal. Unsurprisingly, his Facebook interview goes well.

Emily’s school has an informal, undergraduate computer science club in which they are collectively reading technical interviewing guides and trying to figure out what tech companies want from them. She has a couple interviews lined up, but all of which are for jobs she’s desperate to get. They trade tips after interviews but ultimately have a shaky understanding of they did right and wrong in the absence of post-interview feedback from companies. Only a couple of alumni from their school have made it to top tech companies in the past, and so they lack the kinds of information that Mason has on what companies are looking for. (E.g. Don’t be afraid to take hints, make sure to explain your thought process, what the heck is this CoderPad thing anyway…)

Anthony doesn’t know anyone who has a tech job like the one he’s interviewing for, and only one of his friends is also interviewing. He doesn’t know where to start when it comes to getting ready for his upcoming interview at GoogFaceSoft. He only has one shot at it with no practice interviews lined up. He prepares by googling “tech interview questions” and stumbles upon a bunch of unrealistic interview questions, many of them behavioral or outdated. He might be offered the interview and be fit for the job, but he sure doesn’t know how to pass the interview.

For students who may be unfamiliar with the art of the technical interview, algorithmic interviews can be mystifying, [leading to an imbalance of information on how to succeed](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview). Given that [technical interviewing is a game](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary), it is important that everyone knows the rules, spoken and unspoken. There are many practice resources available, but no amount of reading and re-reading *Cracking the Coding Interview* can prepare you for that moment when you are suddenly in a live, technical phone screen with another human.

## We built a better way to hire

Ultimately, as long as university hiring relies on a campus-by-campus approach, the status quo will continue to be fundamentally inefficient and unmeritocratic. No company, not even the tech giants, can cover every school or every resume submitted online. And, in the absence of any meaningful information on a student’s resume, companies default to their university as the only proxy. This approach is inefficient at best and, at worst, it’s the first in a series of watershed moments that derail the promise of social mobility for the non-elite, taking with them any hope of promoting diversity among computer science students.

Because this level of inequity, placed for maximum damage right at the start of people’s careers, really pissed us off, we decided to do something about it. interviewing.io’s answer to the unfortunate status quo is a university-specific hiring platform. If you’re already familiar with how core interviewing.io works, you’ll see that the premise is exactly the same. **We give out free practice to students, and use their performance in practice to identify top performers, completely independently of their pedigree. Those top performers then get to interview with companies like Lyft and Quora on our platform. In other words, we’re excited to provide students with pathways into tech that don’t involve going to an elite school or knowing someone on the inside.** So far, we’ve been very pleased with the results. You can see our student demographics and where they’re coming from below. Students from all walks of life, whether they’re from MIT or a school you’d never visit, are flocking to the platform, and we couldn’t be prouder.

interviewing.io evaluates students based on their coding skills, not their resume. We are open to students regardless of their university affiliation, college major, and pretty much anything else (we ask for your class year to make sure you’re available when companies want you and that’s about it). Unlike traditional campus recruiting, we attract students organically (getting free practice with engineers from top companies is a pretty big draw) from schools big and small from across the country.

We’re also proud that **almost 40 percent of our university candidates come from backgrounds that are underrepresented in tech**.

![Chart showing underrepresented status of students who are using interviewing.io](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUnderrepresented_status_of_students_on_interviewing_io_a00a8396af.webp%3Fupdated_at%3D2022-11-21T19%3A57%3A53.399Z&w=1920&q=75 "Underrepresented status of students on interviewing.io")

Because of our completely blind, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when a student unmasks at the end of a successful interview, the company in question realizes that the student who just aced their technical phone screen was one whose resume was sitting at the bottom of the pile all along.

In addition to identifying top students who bring a different lived experience to tech, we’re excited about the economics of our model. **With interviewing.io, a mid-sized startup can staff their entire intern class for the same cost as attending 1-2 career fairs at top schools… with a good chunk of those interns coming from underrepresented backgrounds.**

*Meena runs interviewing.io’s university hiring program. We help companies hire college students from all over the US, with a focus on diversity. Prior to joining interviewing.io, Meena was a software engineer at Clever, and before that, Meena was in college on the other side of the engineer interviewing equation.*

1. At least her school didn’t send out [this](https://twitter.com/kaybeezee/status/907291100238532609). [↩](#user-content-fnref-1)


# [We’ve raised our Series A!](https://interviewing.io/blog/weve-raised-our-series-a)

By Aline Lerner | Published: October 18, 2021; Last updated: May 1, 2023

I’m really excited to announce that we raised a $10M Series A, led by the fine people at [M13](https://m13.co/portfolio). This round of funding is a long time coming. I don’t know what the average company age is when successfully raising an A, but it took us 6 years to get here. It was a long and windy path, and for a little while, we almost died (more on that below), but we’re still here, and we’re so grateful and excited to finally have the resources to do what we’ve always wanted to do: fix hiring, for real.

What does “fixing hiring” mean? We believe that the only way to really effect change, is to make hiring more efficient—meaning that the right people are talking to the right companies at the right time. *Make hiring efficient* is actually our company mission. “Efficient? But, that’s so uninspiring,” one might say. And to that, we say false. For us, fairness follows from efficiency… whereas the reverse isn’t necessarily true.

How do you make hiring efficient? Here’s our todo list:

1. Kill resumes
2. Democratize access to interview practice: make it so everyone who needs practice can get it and can enter the technical interview process on a level playing field
3. Democratize access to jobs and put power back in the hands of individual engineers. Make it so people who are good, regardless of who they are or how they look on paper, can talk directly to hiring managers at any company they want, whenever they want

## Killing resumes

Many years ago, when I was working as a recruiter, I wrote a program that took hundreds of candidate resumes who had previously interviewed at the company, and looked at a bunch of different traits, including GPA, years of experience, university, highest degree earned, previous employer, and so on.

I found that, by far and away, the thing that mattered most was [how many typos and grammatical errors a resume ha](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/)d. Where people went to school didn’t matter at all, and neither did any of the other traits above (except their previous employer, which mattered a bit, but much less than the typos).

I also did an experiment where I showed anonymized resumes to recruiters and hiring managers, asking them if they’d interview that candidate. Not only could they not tell who the good candidates were, they all [disagreed with each other about what a good candidate looked like](https://blog.alinelerner.com/resumes-suck-heres-the-data/) in the first place.

It became abundantly clear to me that resumes sucked, but if we’re honest, they’re not the only problem. Once you’re in the door, you have to pass the technical interview.

## Democratizing access to interview practice

Technical interviews are hard for all engineers, regardless of background or seniority, but they’re not going away for a while, and, perhaps worst of all, access to practice isn’t equitably distributed. At schools like MIT, where I went for undergrad, students have boundless access to interview practice. When I was there, not only was there a multi-week course during the month-long break between Fall and Spring semesters that was dedicated exclusively to passing technical interviews, but there was a social component to it as well —everyone at these schools is interviewing at FAANG for internships and new grad positions, which means that students can practice with each other, share their successes and failures, and, over time, internalize just how much of technical interviewing is a numbers game. Once you “get it”, you know that bombing a Google interview doesn’t mean that you weren’t meant to be an engineer. It just means that you need to work some more problems, do more mock interviews, and try again at Facebook.

Students not coming from these schools, or any engineers without an existing network of people with whom they could practice, would miss out not only on the interview prep, but also on the more subtle change in perception where failure is a temporary state, not a death sentence.

And so, interviewing.io was born. The idea was simple. Any software engineer could use the platform to prepare for technical interviewing via *anonymous* mock interviews with senior engineers from top companies like Google, Facebook and others. Junior engineers could get a taste of what real interviews were like and grok that these interviews are a learned skill. Senior engineers could de-rust and warm back up without anyone knowing who they were.

Interviews suck for everyone. **Many engineers, regardless of seniority or pedigree, perform poorly in their first mock interview, but after about 5 practice interviews, their odds of passing a real interview double.** **In fact, after getting the requisite amount of practice interviews in and warming up, there is NO difference in interview performance among engineers who graduated from a top-tier CS school and those who did not.**

## Democratizing access to jobs and putting engineers in the driver’s seat of their job search

If you do well in practice interviews, no matter who you are or how you look on paper, you unlock our jobs portal. With 2 clicks, you can book real interviews with engineers at top companies, bypassing online applications, resume screens, and recruiters.

In other words, you don’t have to have a friend refer you in or wait for recruiters to contact you and then spend weeks scheduling calls. You’re in control, and you can book or not book whenever you like.

These interviews are also anonymous, just like practice, which means that companies don’t know who you are until *after* the interview, so you’re judged on what you can do, not how you look on paper.

For employers who hire through us, this is a win. End-to-end, from first touch to hire, a top-tier tech company will hire about 0.5% of its candidates. With interviewing.io, that number is closer to 10%, and companies who use it save an average of 220 hours per hire, split between engineering and recruiting. **Close to half of our hires are candidates the company would not have considered if they had seen just their resume, and many of these engineers had previously been rejected by literally the same company where they later got hired via interviewing.io.** Over our lifetime, we’ve made hires for companies like Facebook, Amazon, Lyft, Uber, Snap, Dropbox, and dozens of others.

**The other, subtle value-add is that while, to the rest of the world, interviewing.io users look like passive candidates who aren’t on the market — they don’t update their LinkedIn or start blogging or tweeting — interviewing.io knows they’re about to look because they’ve started practicing. In other words, we know the 2 things that actually matter: who’s good and who’s actually looking right now. No one else, not even LinkedIn, has that information.**

And for those engineers, we roll out the red carpet.

## A brief interlude: The bit where we almost died but then didn’t

This all sounds pretty good, right? It kind of was, but then things got bad. You see, for most of our existence, we made all of our money from employers, which meant that we could offer interview practice for free. When hiring ground to a halt in the wake of COVID, we lost almost all of our revenue and started charging individual engineers for interview practice to survive. You can see the [blog post where we announced our pivot](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), and while the post’s title is kind of peppy, it was a very hard post to write and an even harder decision to make. If you want to hear the whole sordid tale of woe, you can listen to my 2020 interview on Indie Hackers:

After we made the switch, we promised our users that we would find a way to make it so people who couldn’t afford to pay us could still practice on our platform—if we didn’t do that, we’d be in gross violation of our goal to democratize access to practice. That promise is a large part of why we raised this round. More on that in a moment.

Here’s the shape of our revenue before and after COVID (things started to go south a few months before quarantine… there’s always a seasonal dip in our business, but this time it never recovered).

![It looks cool now, but it wasn’t fun to live through.](https://strapi-iio.s3.us-west-2.amazonaws.com/Screen_Shot_2021_10_17_at_2_25_22_PM_bebee560fb.webp)

It looks cool now, but it wasn’t fun to live through.

The hockey-stick growth we had after our switch is what enabled us to raise this A. And that hockey-stick growth is due to the herculean efforts of the interviewing.io team to build out and test a brand new product in a little over a month, while the future loomed more and more uncertain. A special thank you here to the longest-standing members of the team who were here before COVID and through it: Liz Graves, Sam Jordan, Eamonn MacConville, and Dawid Kluszczynski.

![This is our awesome team. Thanks guys.](https://strapi-iio.s3.us-west-2.amazonaws.com/team_photo_2021_final_46a24a9101.webp)

This is our awesome team. Thanks guys.

It’s due to our intrepid interviewer community, who stuck by us and continued to give hours of their time to helping people practice and get better, even when they were unsure if we’d be around tomorrow.

It’s due to the short list customers who stayed on even when hiring slowed down, those who came back when the economy started to recover, and the new ones who’ve taken a bet on us since.

And it’s due to our users who could have rightfully rage quit when we started charging them for something that used to be free… but instead chose to stay, answered our myriad user research emails about pricing and offerings, and were willing not only to to pay us, but pay us through a series of rather suspicious-looking PayPal links (before we actually integrated payments into our platform).

Today, an engineer signs up for interviewing.io every 8 minutes, we have over a third of Bay Area engineers in our ecosystem, and **over our lifetime, interviewing.io has hosted close to 100K technical interviews. In the process, we’ve observed, over and over, that where someone went to school has no bearing on how they’ll perform in an interview.**

## Why M13 (including the bit where I was pitching while 7 months pregnant)?

We chose M13 to lead our Series A because both Brent Murri and Matt Hoffman immediately understood our vision for what good hiring looks like and both cared deeply about fixing it for engineers in particular. Perhaps most importantly, they not only understood, but encouraged me to invest time and resources in the power of our community. Our one core value as a company is to put candidate (read: engineer) experience first, regardless of where the money is coming from. We want to be the place where smart people hang out and come back to. If we have that right, everything else will come. Building for engineers, smart driven creative engineers, is a bit of a double-edged sword (because they, too, are brashly opinionated), but it’s also a gift. Not everyone gets to love their users, and most companies don’t get to tangibly change people’s lives and give people stuff they can’t get anywhere else. We’re able to do that, and we’re fortunate to have found partners who believe in this value as fervently as we do.

And, look, I have no idea if this is the right place to put this, but there’s one other reason we went with M13. Most people don’t know this (though I guess they will now), but I started pitching our Series A when I was 7 months pregnant. I had been hoping to start earlier, but there was a lot of deck work and pitch practice that preceded it, and I’m a solo founder, so all the pitch practice and deck work had to be slotted in between continuing to run the company. (As an aside, my team is awesome. I didn’t realize exactly how awesome until I had to give birth 3 weeks early and ended up being on unplanned leave for several weeks afterwards with no plans in place yet… in retrospect, I could have stepped back and focused on pitching much earlier!)

So there I am, on a bunch of Zoom calls with investors, where they see me from the neck up, while fielding all manners of conflicting advice about when to reveal that I’m in the family way. Of course, in an ideal world, you reveal it immediately, and it doesn’t change anything. And there are those who would advise leading with the pregnancy because if investors aren’t immediately supportive, fuck them, right? After hearing all sorts of horror stories, I took the middle road and decided to wait until prospective investors had at least a few meetings with me and had the chance to get what interviewing.io was before dropping the news. Not only were the folks at M13 completely gracious about the big reveal, but the way they handled the rest of the process was exemplary, even when I went dark at the most fraught part of the deal—right after I got their term sheet—because, well, I went into the hospital for a routine checkup in between investor calls and came out with a baby. (Don’t worry, everything worked out alright, and we have a beautiful baby girl.)

I hope every founder who’s pregnant or about to be or has small children has the kind of experience that I had with M13. Full stop. In that world, worrying about when to break the news that we’re pregnant will be an unfortunate, anachronistic blip.

## What’s next? (Spoiler: We’re going to make it so anyone can practice now and pay later, and we’ll get *all* the companies you want onto our platform)

Now that we have the resources, we want to make good on the promises we made to our users back when things got real hard. It’s not possible to democratize access to interview prep if you’re charging for it, so that we’d create a Fellowship program where engineers from traditionally underrepresented backgrounds can practice for free and that we’d create a program where people who needed practice now could defer payment until they got a job.

We’ve already rolled out the first two things (and will be running a 3rd Fellowship cohort soon). Our deferred payment program has been in pilot mode for the last few months, and so far we’ve seen an amazing 94% payback rate. **With our Series A funding, we’ll be able to take this program out of pilot mode and roll it out to every engineer on our platform— don’t pay us a dime until you get a job and are in a position to pay.**

The other big thing we’re going to focus on is getting more employers on interviewing.io. Hiring is back, post-COVID, in a big way, and as we regrow the employer side of our business, we’ll be able to make our platform more affordable, even without deferred payments. We also want to fulfill our promise of putting power back in the hands of engineers. Hiring is bad. Resumes don’t tell you much, scheduling is painful, getting in front of companies takes weeks of emailing if you’re lucky and infinite time if you’re not. We’re rethinking hiring from the ground up, automating away the cruft, and removing friction for the part that matters: *smart people talking to each other about the actual work.* **So, if you’re a top performer on our platform, no matter who you are, our goal is to make it so you can speak to a hiring manager at any company you want, anonymously, as early as the next day, and get fast-tracked through the rest of the process.**

*This post wouldn’t be complete without a thank you to all of the new investors who joined in this round in addition to M13. A special thank you to our existing investors, many of whom invested in multiple rounds. Of those, I’d like to mention the following people individually: Leo Polovets at Susa Ventures, Freada Kapor Klein at Kapor Capital (who’s been with us since the very, very beginning… thank you)*, *David Waxman at TenOneTen Ventures, and Peter Livingston at Unpopular Ventures. Even when things got really bad and we were sitting at the very nadir in the graph above, they were with us every step of the way and continued to believe in our vision for better hiring.*


# [Hamtips, or why I still run the Technical Phone Screen as the Hiring Manager](https://interviewing.io/blog/hamtips-or-why-i-still-run-the-technical-phone-screen-as-the-hiring-manager)

By Alexey Komissarouk | Published: October 5, 2021; Last updated: May 1, 2023

*Hey, Aline (founder of [interviewing.io](https://interviewing.io/)) here. This is the fourth post in our Guest Author series.*

*In this post, our Guest Author, Alexey Komissarouk, talks about how advantageous it is to have hiring managers (rather than individual members of the team) conduct the first technical screen, effectively combining the technical interview and the hiring manager sell call. At [interviewing.io](https://interviewing.io/), where the first interaction an engineering candidate will have with a company is always with another engineer (and often with a hiring manager), we’re strong advocates of this approach. Why? Because selling matters at every stage of the process — in this market, interviews aren’t just for vetting anymore — and hiring managers can sell in a way that a peer cannot. And because hiring managers are going to be among the best-calibrated interviewers, which means that they’ll save the rest of the team time downstream.*

*Caveat: Yes, yes, almost everything about the interviewing / recruiting process is broken. Sometimes though, you just have to play the hand you’re dealt and settle for minor improvements.*

The 75-minute HMTPS is my proposed minor improvement.

![Cartoon: where do you see yourself in five years?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Foatmeal_interviewing_1_97fd522520.webp&w=1080&q=75 "A serious question")

Hat tip to The Oatmeal

## What is the HMTPS?

It stands for “Hiring Manager Technical Phone Screen.” Since you asked, I’ve been pronouncing it “ham-tips.” It’s the call a candidate will have after their RPS (Recruiter Phone Screen), but before their onsite.

This combines two calls: the Technical Phone Screen (TPS), which is a coding exercise that usually happens before the onsite, and the HMS call, which is a call with the Hiring Manager (your would-be manager) that can be done before an onsite, or after, or not at all.

Instead of leaving the HMS call to languish, I combine them into one – the HMTPS. It takes 75 minutes.

## Why combine the two interviews?

An ideal interview loop has as few steps as possible, and gets to a decision ASAP. By combining these two steps you shorten the intro-to-offer by ~1 week and reduce candidate dropoff by 5-10%.

It’s also a lot less work for recruiters playing scheduling battleship.[1](#user-content-fn-1)

Finally, Hiring Managers will, on average, be better at selling working at the company – it’s kind of their job.

## Why 75 minutes?

We’re combining a 30-minute call and a 60-minute call, and combining the 15-minute Q&A at the end of each into one, like so:

![Chart: Consolidating pre-onsite interviews to improve hiring velocity](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FConsolidating_pre_onsite_interviews_to_improve_hiring_velocity_3_3fd8cff5d1.webp&w=1920&q=75 "Improving hiring velocity")

I’m also more comfortable shortening the ~50 minute technical question into 30 minutes because (a) I’m pretty calibrated on my question, having run it 200+ times at this point, and so can get most of the signal I’m looking for within the first 30 minutes.

I’ve tried doing this call in 60 minutes and it ends up feeling pretty rushed; not to say somebody else couldn’t pull that off, but I’ve appreciated the bit of space. Also, since most candidates don’t schedule in 15-minute increments, we can always go a little long (up to the 90 minute mark) if we need to.

## Why is this good for the hiring manager?

First, it’s easier to schedule (usually towards the end of the day). Second, it usually gives me enough time with the candidate so that I end up being pretty confident about how they’ll do both at the job and on the onsite. I haven’t quantified this yet, but anecdotally I have been surprised by onsite interviewer feedback much more rarely when I do this.

## Why is this good for the candidate?

It’s one fewer hoop to jump through. Also, whether or not they get along with me as their future manager – both technically and interpersonally – can, and should, be a pretty strong determinant as to whether they should continue with the process. This gives a more accurate and realistic signal, since we are both coding together and talking about work.

## When is this a bad idea?

This makes the Hiring Manager a bit of a bottleneck in interviewing; once a company gets to the point where you are interviewing for titles like “Senior Software Engineer, Team TBD” you have to round robin TPS-es to the rest of your [Phone Screen Team](https://interviewing.io/blog/technical-phone-screen-superforecasters).

Also, as the HM I likely have some unreasonable biases (Golang engineers, I’m looking at you), and making me the bottleneck in interviewing exacerbates those. That said, the HM’s bias is going to be applied sooner or later in the interview process, and my take is that between the more effective selling and the time savings (both for the HMTPS and downstream), the benefits outlined are worth it.

1. Tuesday at 4? You sunk my Grooming Session!" [↩](#user-content-fnref-1)

[My manager is not promoting me. What should I do? Advice from an eng manager at Amazon, Meta, and Microsoft.](/blog/my-manager-is-not-promoting-me-advice-from-amazon-meta-microsoft-eng-mgr)


# [Why AI can’t do hiring](https://interviewing.io/blog/why-ai-cant-do-hiring)

By Aline Lerner | Published: May 15, 2023; Last updated: March 7, 2024

The recent exciting and somewhat horrifying inflection point in AI capability, which many of us got to experience firsthand when playing with OpenAI’s ChatGPT, tipped me into finally writing this post.

I’m the founder of interviewing.io, a mock interview platform and eng hiring marketplace. Engineers use us for mock interviews, and we use the data from those interviews to surface top performers, in a much fairer and more predictive way than a resume. If you’re a top performer on interviewing.io, we fast-track you at the world’s best companies.

We’re venture backed and have raised 4 rounds of funding in the last 7 years, totaling over $15M, which means that I’ve done a lot of VC pitches. I don’t know how many exactly (and a lady should never tell), but it’s in the hundreds. Once you’ve done that many pitches, you start to hear the same feedback over and over. They range from questions about whether the eng hiring is big enough (it is) to how objections about human-on-human interviews don’t scale (if 2 humans doing a thing together didn’t scale, our species would be extinct) to polite suggestions about how we’d be a much more attractive investment if we used ML/AI to match candidates to companies.

I’ve heard the latter a LOT over the years, but despite the well-intentioned advice, I’m convinced that building an AI matcher is a fool’s errand. My argument is this: **It’s not that AI doing hiring is technically impossible – ChatGPT has shown us that the ceiling on what’s possible is higher than many of us had ever imagined – but that it’s impossible because you don’t have the data.** In other words, the hard part about hiring isn’t the tech. It’s having the data to make good hiring decisions in the first place.

**For the purposes of this piece, I define “hiring” as being able to find candidates for specific roles and fill those roles. I am NOT referring to automating tasks like resume parsing, writing sourcing emails, or scheduling, i.e., tasks that human recruiters, sourcers, and coordinators do as part of their job. Surely those can be automated. The more interesting question is whether an AI can do the job of a recruiter better than a human.[1](#user-content-fn-1)** In other words, can it take a list of candidates, a list of job descriptions, and then use publicly available (NOT proprietary) data to match them up successfully and fill roles? After all, the reality is that neither recruiters nor burgeoning AI recruiting startups have a proprietary data set to work with. They usually have job descriptions, LinkedIn Recruiter (the granular search functionality of which isn’t publicly accessible… but the LinkedIn profiles of candidates actually are), and whatever else they can find on the internet.

To wit, this post isn’t about how AI can’t be used for hiring if you have all the data. Rather, it’s about how you can’t get access to all the data you’d need to do hiring, thereby making the training of an AI impossible.

This post also isn't about how human are great at hiring. There's nothing special about our humanity when it comes to recruiting, and humans actually suck at hiring for the same reasons as AI — we don't have the data either.

## A few caveats: the case for Microsoft and the question of bias

At this point, you’re probably thinking, “Well, surely Microsoft can do this, given that they own both LinkedIn and GitHub.” In this post, you’ll see why LinkedIn and GitHub are not enough. Perhaps if Microsoft chose to buy a bunch of applicant tracking systems (ATSs) in order to get access to interview performance data, coupled with data from LinkedIn and GitHub, they’d have a fighting chance, but honestly that’s probably not enough either.

Moreover, the tenuous Microsoft edge aside, the reality is that most of us do NOT have access to the kind of training data we’d need, but we still see startup after startup claiming to do AI hiring in their marketing materials.

Finally, before we get into why AI can’t do hiring, I want to call out the important question of bias that results from training AI on hiring data where decisions were previously made by humans. To keep this (already long) post on task, I will not touch on the subject of the bias. To be sure, it’s a real problem, and there’s already a lot of good writing on the subject.

That caveat aside, if we’re trying to build a solution that takes candidates and jobs as inputs and produces a set of matches as output, let’s start by considering what that matcher does and how it’s trained.

## How do we train an AI matcher?

Let’s pretend for a moment that we have built the platonic ideal of an AI matcher. It takes 2 inputs: a list of candidates and a list of companies, and a sorted list of company/role matches come out, like so:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/AI_matcher_d024945e8d.png)

To train this matcher, we need 3 distinct pieces of data:

1. Publicly available job descriptions from a bunch of tech companies
2. Publicly available data about engineers, i.e., LinkedIn profiles, GitHub profiles & contributions, and engineers’ social graph across a bunch of different platforms
3. A list of successful company/candidate matches, taken from the public domain, e.g., from scraping LinkedIn to see where people worked/for how long, and cross-referencing that with (1) and (2).

You may notice that in all 3 data requirements above, I called out that they’re publicly available. That might seem odd at first — after all, if you’re starting a company that’s building this matcher, your secret sauce might be your proprietary data about candidates or companies or both, and you might first run a different business model, completely divorced from AI, to collect this data, at which point, boom, you flip a switch, and all of a sudden you’re an AI company.

That’s a good strategy, but it’s actually really hard to acquire detailed, proprietary data about candidates, companies, or how well people do at companies once they’re hired, let alone all 3 at once. Most startups that try to build an AI matcher don’t start with a bunch of proprietary data. Rather, they start with the public domain. The thesis of this piece is that getting the data is the hard part, not the AI, so to reason through it, let’s assume that we have the AI already but that the only data we have is publicly available.

So now that we have all this training data, let’s get to work. We’ll go through the set of successful matches and then find more data about those candidates and those companies and see which traits carry the most signal for a good match.

We now have a trained and working matcher. So far so good. But wait, not so fast!

## What is hiring, really?

Let’s switch gears and forget about our matcher for a moment. Broadly speaking, regardless of how we get there, what needs to be true for someone to be a good fit for a job? There are three things:

1. *Good*: They’re good enough to do the job
2. *Looking*: They’re open to taking a new job
3. *Interested*: They’re interested in the job/company

**Let’s succinctly call those “good, looking, and interested”. These three criteria are necessary to make a hire.**

The first two items are largely independent of the company. The third is about candidate/company fit, and we’ll come back to it when we talk about matching. Before we do that, though, can we actually deduce which engineers are good and looking?

I would argue that we can’t. No matter how the matcher ended up getting trained or what patterns or artifacts it detected and assigned value to, the data to tell whether someone is a good engineer (even if the definition is elastic, depending on a given company’s “bar”) simply does not exist in the public domain.\*\* An AI is very good at finding patterns in existing data. It is not good at magicking data out of thin air. That means that before we even get to the question of matching, we’re dead in the water.\*\* Let me try to convince you.

### “Good”

Generally, you have 3 pieces of public data available to you for a given engineer:

1. Their public-facing LinkedIn
2. Their public-facing GitHub
3. Their public-facing social graph

Surely you can tell if someone is a good engineer from some combo of these 3?

### LinkedIn

Let’s look at each of the 3 data sources above, starting with LinkedIn. What data is available on engineers’ public-facing LinkedIns? Given that a LinkedIn profile is a glorified resume, it’s usually these 3 things:

1. Where they’ve previously worked
2. Where they went to school
3. Any certifications/endorsements/skills that they have

I run a hiring marketplace, and having any kind of edge in predicting which of our users are good is material to our business, so we’ve spent a good amount of time and effort trying to tie these attributes to how good an engineer is. As it turns out, an engineer’s employment history carries some signal, school carries very little to none, and LinkedIn [certifications](https://interviewing.io/blog/why-you-shouldnt-list-certifications-on-linkedIn) and [endorsements](https://interviewing.io/blog/linkedin-endorsements-useless) carry a negative signal.[2](#user-content-fn-2)

Knowing which programming languages or frameworks an engineer knows can be useful, but most eng roles are either language/framework agnostic OR the language/framework is not the most significant bit when determining whether an engineer is a good fit or not — knowing the language is usually a nice to have, but it won’t get you hired if you’re not a good coder, first and foremost.

Often, in the absence of being able to search for whether an engineer is good, recruiters will search for programming languages as a proxy for fit, but that’s all it is, a weak proxy.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/linkedin_filter_c6d9fc88df.png)

Basically, if you’re relying on a combo of school, work history, and specific skills, you’re doing exactly the same work that armies of recruiters have been doing for decades, with very limited success. It’s really easy to search LinkedIn for past employers, schools, and skills/endorsements. They’ve built a whole business on it. However, humans are terrible at predicting engineer quality from resumes — [only about as good as a coinflip](https://interviewing.io/blog/resumes-suck-heres-the-data). **And it’s not because humans are bad and that an AI would do better. It’s because there’s minimal signal in a resume in the first place, and try hard as you might, extracting signal in this case is like squeezing water from a stone. Having a robot hand will not save you.**

AI or not, we’re in a hard market. Keep your skills sharp. Sign up for anonymous mock interviews with engineers from top companies.

### GitHub

GitHub is an interesting one. Surely if you have access to a bunch of code someone has written, you can suss out if they’re a good engineer. The GitHub approach is also appealing because it’s much more meritocratic than a resume — your good code can stand on its own, regardless of who you are or where you come from. Wouldn’t it be great if GitHub could help you surface the odd diamond in the rough or an upstart with no job experience?

This is of course, the ethical question of whether GitHub should be used for hiring in the first place and whether it’s fair to create a bias toward hobbyist engineers and/or open-source contributors.[3](#user-content-fn-3) For the purposes of this piece, I will put aside that question and focus just on whether GitHub *can* be used for hiring.

The short answer is that it can’t because most engineers don’t have public commits. Senior engineers at large tech companies don’t work on open-source projects for the most part. This is why [programmers on Reddit laugh](https://www.reddit.com/r/ProgrammerHumor/comments/11g13i6/he_is_not_qualified) at the idea of screening out candidates with unused GitHub accounts. Bjarne Stroustrup, the inventor of C++, would look unimpressive to an algorithm obsessed with GitHub activity.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/bjarne_68a4633cd6.png)

This same message is clear from data on GitHub’s users. The website [reported 100 million active developers in early 2023](https://techcrunch.com/2023/01/26/github-says-it-now-has-100m-active-users), but data on all commits in 2022 shows that only about 11 million users had any public code commits and only 8 million had more than two commits. That means that only 8-10% of engineers actually have publicly available GitHub data that our AI could use, in the absolute best case, and this counts millions of repositories with names like “hello world” and “test.”

Ben Frederickson did some of his digging about the utility of GitHub in hiring and published a [stellar, highly detailed report](https://www.benfrederickson.com/github-wont-help-with-hiring) in 2018. According to Frederickson, only 1.4% of GitHub users pushed more than 100 times, and only 0.15% of GitHub users pushed more than 500 times. Frederickson’s findings from 2018 roughly corroborate ours from late 2022, and in both cases, there is a clear power law – most of the commits are being done by a tiny fraction of GitHub’s users.

A paper called “[The Promises and Perils of Mining GitHub](https://dl.acm.org/doi/10.1145/2597073.2597074)”, published in 2014, looked at GitHub activity as well, through the lens of projects rather than users. They found a similar power-law relationship – the most active 2.5% of projects account for the same number of commits as the remaining 97.5% projects. Moreover, this paper found that about 37% of projects on GitHub were not being used for software development, and the rest were used for other purposes (e.g., storage).

**Given these limitations, the reality is that the portion of GitHub accounts that could actually be useful for hiring is likely under 1%.**

### Social Graph

Finally, we have the social graph. The hypothesis here is that great engineers follow other great engineers on platforms like GitHub and Twitter, so if we can identify a set of great engineers somehow, and dig deeply enough through the tangled web of whom they follow and who follows them, we’ll be able to create a reliable talent map.

Let’s assume for a moment that we can seed this approach with enough great engineers (e.g., by scraping the GitHub profiles that do have enough code to extract signal). What happens next?

To get a feel for the “following” behavior among software engineers and whether they tend to follow the best people they work with, we surveyed our user base, like so:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/social_network_survey_3a890590ac.png)

Based on almost 1000 responses from our engineers (average years of experience = 8, median = 7), the social graph approach doesn’t hold water. First, engineers that we surveyed only rarely followed their most impressive colleagues on GitHub or Twitter. **The average engineer reported following just one of their top five coworkers in these sites.** The social graph was just not that connected to people’s perceptions of talent.

What about Linkedin? \*\*Although many of our users did indeed follow the best engineers they’ve ever worked with, they also followed everyone else: the majority of engineers we surveyed reported that their connections had no rhyme or reason, or they just connected with anyone who tried to connect with them. \*\*

These muddle the graph-based approaches. Twitter and GitHub follows are still too uncommon to be a reliable signal. And LinkedIn connections capture a haphazard mix of networking approaches — the most dominant of which is either random or just driven by workplace, which any recruiter would have known in the first place.

### “Looking”

Let’s assume for a moment we can figure out who the good engineers are (or rate them on some kind of scale, at least). Next, we have to figure out which of these engineers are on the market right now.

This is arguably an easier problem to solve than whether an engineer is good because publicly available cues do exist. That said, a number of startups have tried to tackle this problem (most notably [Entelo, with their Sonar product](https://www.recruiter.com/recruiting/new-entelo-technology-lets-recruiters-know-which-employees-will-most-likely-quit)), though so far none have been particularly successful. The kinds of inputs that typically go into trying to figure out if an engineer is active are split between candidate-level attributes (e.g., how recently they updated their LinkedIn) and company-level attributes (e.g., how long since the company’s last round of funding). Here’s what that list could look like:

Candidate-level attributes:

- When did they last update their LinkedIn/have they recently started being more active on LinkedIn?
- How long have they been in their current role? And at their current company?
- Have they recently started being more active on social media (e.g., tweeting about engineering topics)?
- Have they recently started a blog?
- Have they recently started contributing to open source?

Company-level attributes:

- How long has it been since the company last raised money (if not public)?
- How has the stock price been doing (if public)?
- Have other people been leaving the company, especially management?

Some of these attributes are easier to pull than others (e.g., to track LinkedIn updates, one has to be logged in and has to repeatedly cache candidate activity, which likely violates LinkedIn’s terms of service), but I imagine that there’s enough publicly accessible data to make some guesses about who’s moving. Of course, these guesses will be fairly primitive for candidates who don’t do stuff publicly and loudly — going just off of stock price and/or fundraising history gives you a very crude first pass, but it’s not enough.

When asked their favorite social media platforms, half of engineers in a [StackOverflow survey](https://insights.stackoverflow.com/survey/2019) reported Reddit or YouTube — which it’s hard to imagine could ever be leveraged for predictive algorithms — or not using social media at all.

As such, even if this data is easier to get than candidate quality data, figuring out who’s looking is still a data problem and not an AI tech problem.

## Matching: Figuring out what engineers want and whether companies have it

Let’s say that we’re somehow able to surmise from public-facing candidate data whether and engineer is both “good” and “looking”. Now we need to figure out whether they’re actually going to be interested in a given company.

To do that, we need to have a list of company attributes that engineers could care about, and then we need to figure out 1) how each company stacks up against this list and 2) which attributes a given engineer cares about. I’ve been a recruiter for about a decade, and below is a decently representative list (in no particular order):

- Compensation
- Company mission/whether the company is mission-driven
- Vertical
- Size of whole team and the eng team
- Tech stack
- Young vs. established
- Prestige of the brand/social proof
- What problems are being solved/what’s coming up on the roadmap
- Chemistry with manager and with the immediate team
- The company’s culture and values

Now that we have this list, how do we figure out how each company stacks up? Without a proprietary data set, the main resource you have is a company’s job descriptions. How much of this information can you pull from those descriptions?

### Figuring out what companies have to offer

Compensation is increasingly easier to get, especially given recent legislation in some states that makes it mandatory for employers to include salary ranges in their job descriptions. Of course, these ranges tend to be wide, and many aren’t super useful (see the examples below), but it’s something. The screenshot below comes from [comprehensive.io](https://comprehensive.io), a site that aggregates comp ranges from job descriptions in NY and California, where companies are legally required to disclose them. As you can see, the ranges are quite wide.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/comprehensive_io_comp_ranges_8b165c53d3.png)

Below is an example for a specific company and role: Dropbox’s open Mobile Software Engineer role in the US. As you can see, the ranges are pretty wide (a 66K spread for SF, NYC, and Seattle for instance). In my mind, like many of these ranges, all this tells you is “you’re going to get paid market for the location that you’re in”. Of course, if a company is paying below market, that’s something you need to know, but that’s the exception rather than the rule.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/dropbox_comp_0c43fb6d38.png)

Some attributes like vertical, tech stack, total size, and brand prestige, are more consistent than compensation and are possible to figure out from publicly available data.

Whether the company is mission-driven can sometimes be deduced from the vertical, the company’s B-corp status, and the company’s job descriptions. However, given every company’s penchant for sounding mission-driven, even when they do something as dryly mercenary as ad-serving infrastructure, this may be a bit tricky. But I’m sure the tech is there to figure this one out.

Properties like eng team size, upcoming projects and roadmap, and the company’s culture/values are much harder. Sussing these out is not a technological problem but rather a data one, just like determining whether engineers are good or not. Don’t believe me? Read any job description. How do you begin to figure out from this wall of mush what it’s actually like to work at a company? Maybe some great job descriptions exist out there, and maybe you can get somewhere by scraping sites like Glassdoor, but given how low-signal both tend to be, AI will likely get just as hamstrung as the humans trying to parse these documents.

If you don’t buy that, check out [keyvalues.io](http://keyvalues.io) and look for literally any company. You’ll notice that every company seems to have pulled randomly from the same-grab bag of 20 or so lofty-sounding values that tell you nothing about what actually happens at that company day to day. Instead, you find yourself smack in the middle of a vague virtue signaling arms race.

So, some of these attributes you can figure out from the public domain, and some you can’t. In a nutshell, the data available to you on both sides looks like this:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/candidates_and_companies_53617907f9.png)

Now we have to tackle the second part of the matching problem: how do we figure out what each engineer on our list is looking for? For instance, which engineers in our database care about compensation, and what are their salary requirements? And what types of problems are they most interested in? And so on…

The reality is that there isn’t a good way to intuit most of these things from publicly available data. If you want to know what engineers care about, you have to ask them. And even when you ask them, they will very likely, albeit through no fault of their own, be lying.

## Figuring out what engineers value and what they’re looking for

Because, to the best of my knowledge, there isn’t a dataset out there that compares and contrasts what engineers said they were looking for versus what jobs they ended up taking, I’ll fall back to an anecdote from earlier in my recruiting career. TL;DR Dr. House was right; everybody lies.

Many years ago, I was interim head of talent at Udacity. Many of my candidates told me that it was their dream to work in ed-tech, but one in particular stood out. In addition to extolling his passion about education, he told me that one of his deal-breakers was working in advertising and that he'd never do it. He did great in our first-round technical screen, and I set him up with an onsite interview. Then, while he was in town, he interviewed at an advertising startup where one of his friends was working. That's the place he ended up choosing… because he really hit it off with the team. Though this particular example was the most stark (the one-letter difference between “ad-tech” and “ed-tech” belies the massive gulf between those verticals), instances like this, where a candidate claimed to strongly want one thing but then ended up choosing something completely different after meeting the team, are the rule rather than the exception.

The truth is, people will tell you all manner of lies about where they want to work, what they want to work on, and what's important to them. But, of course, they're not lying deliberately. It likely means you're not asking the right questions, but sometimes knowing what to ask is really hard. For many of the questions you can think of, people will have all sorts of rehearsed answers about what they want to do, but those answers are framed to the specific audience and may not reflect reality at all. Or, a lot of the time, people simply don't know what they want until they see it.

It’s hard enough sussing this stuff out when you’re talking to candidates 1:1. Imagine trying to gather this kind of nuanced information from what they say on social media or in their public blogs.

Perhaps the most important thing I've learned is that, at the end of the day, one of the biggest parts of a recruiter's job is to get the right two people in a room together. Regardless of industry, or domain, or stack, or money (within reason of course), chemistry is king. Get the right two people to have the right conversation, and everything else goes out the window. Everybody lies. It’s not malicious. It’s just that chemistry is the thing that matters most, and all the rest of the attributes above are poor proxies for the magic that sometimes happens when two smart people have a good conversation.

How do you predict chemistry between people? Can an AI do it? Possibly, if that AI has access to a ton of data about candidates and companies, i.e., everything we’ve discussed in this post thus far… AND past candidate/company interactions and their outcomes.

Even if you can’t get all the candidate and company data you’d need, you CAN get a history of candidate/company interactions and their outcomes from an Applicant Tracking System (ATS). But ATS data is not public. It’s the opposite — for ATSs, their data is their moat, which is what drives retention, and ATS switching costs are painful and often prohibitive.

In the absence of rich candidate and company data and the interactions between them, an AI predicting chemistry is impossible. Hell, humans can’t do it either.

## But what if you do have the data?

If you have proprietary data, then you don’t need an AI. A simple non-AI program (e.g., a regression) or a human can do the job well enough. In fact, Arvind Narayan from Princeton gave an excellent talk called “[How to recognize AI snake oil](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)”, whose crux is that, for complex questions where you need to predict social outcomes (e.g., recidivism, job performance), *no matter how much data you have*, because “AI is not substantially better than manual scoring using just a few features”.

Arguably, if you do have the data, you could still build out AI hiring to increase efficiency, but remember that it’s your possession of proprietary data that made an AI approach viable.

So what does this all mean for the future of recruiting? As I said in the beginning, AI is really well-suited to automating a bunch of recruiting tasks that humans do now. For instance, an AI can take the pain out of stuff like this:

- Interview scheduling
- Composing first-draft sourcing emails (though you’d need a human to truly make them sing)
- Enriching candidate profiles with “first-order” data (years of experience, location, figuring out what programming languages they’ve used in some cases, etc.)
- Tracking candidate progress through a funnel
- Some amount of assessment[4](#user-content-fn-4)
- Creating beautiful dashboards that track key recruiting metrics (time to hire, cost per hire, etc.)

As AI gets more and more sophisticated, the list above will get longer and longer, and given that most recruiters aren’t particularly good at their jobs, over time, AI will take over more and more, and there will be progressively less for human recruiters to do.

However, the stuff above isn’t what makes recruiting hard; these are the trappings of recruiting, but not the essence. The hard thing about recruiting is figuring out who’s good and who’s looking right now, and bulldozing the way for those candidates to have as many conversations with companies as they have appetite for, in order to see if they have chemistry with that company and potentially their future team.

Until we have access to all the data that reliably predicts whether someone is a good engineer, whether they’re looking right now, and what a company offers, and whether they’re interested in that thing, having an AI will not be enough.

**At the end of the day, you can’t use AI for hiring if you don’t have the data. And if you have the data, then you don’t strictly need AI.**

And finally, because we don't have the data, humans will also continue to be bad at hiring. The difference is that good human recruiters can make some meaningful warm intros, let 2 engineers get in a room together to see if there's chemistry, and get the hell out of the way.

*Huge thank you to Maxim Massenkoff, who did the data analysis for this post. Also thank you to everyone who read drafts of this behemoth and gave their feedback.*

1. If I were you, I’d be justifiably skeptical at this point. Wow, a recruiter is writing about how AI can’t do recruiting. Classic Luddite trope, right? Let’s burn down the shoe factory because it can never compete with the artisanal shoes we make in our homes. In this case, though, you’d be wrong. I walked away from a very lucrative recruiting agency that I built to start interviewing.io, precisely because I wanted to be the guy who owned the shoe factory, not the guy setting it on fire out of spite. Recruiting needs to change, it needs disintermediation, and it needs more data. It’s the only way hiring will ever become efficient and fair. I just don’t think AI is going to be that change. If I’m wrong, I’ll be the first in line to pivot interviewing.io to an AI-first solution. It’s also worth calling out that in this piece, I focus specifically on hiring engineers, as my whole career and all of interviewing.io is dedicated exclusively to technical recruiting, but I expect that my reasoning holds for many other verticals. [↩](#user-content-fnref-1)
2. In all my years of trying to figure this out, the thing that carried the most signal is [how many typos engineers had on their resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data), which I think highlights the absurdity of this whole undertaking. See [this post](https://interviewing.io/blog/lessons-from-3000-technical-interviews) and [this post](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt) for previous data/discussion about the relative importance of employment history and pedigree in hiring. [↩](#user-content-fnref-2)
3. See [this piece by Ashe Dryden for some good discussion about the ethics of open source labor](https://www.ashedryden.com/blog/the-ethics-of-unpaid-labor-and-the-oss-community). [↩](#user-content-fnref-3)
4. You might be wondering at this point why I didn’t list assessments here. Arguably, AI will be able to do some amount of assessment, probably somewhere between a HackerRank/Codility and a real human interviewer. I wrote many years ago about how [recruiting engineers is fundamentally a sourcing rather than a vetting problem](https://blog.alinelerner.com/building-a-product-in-the-technical-recruiting-space-read-this-first/), so I’m very skeptical that AI-based assessments are meaningfully going to change the game – in order for assessments to be effective, engineers have to be willing to do them, which means there has to be value for both sides. This is why senior engineers typically balk at asynchronous coding tests, and the biggest attrition rates are usually at that part of the funnel. The future of technical assessments, in the age of generative AI is a noble and difficult topic, though, and it’s something we’ll tackle in a future post. [↩](#user-content-fnref-4)


# [You now need to do 15% better in technical interviews than you did at the start of 2022 (and the bar will keep rising).](https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews)

By Aline Lerner | Published: November 30, 2022; Last updated: May 6, 2024

interviewing.io is a technical mock interview platform and technical recruiting marketplace, so we have a *ton* of useful data around technical interviewing and hiring. One of the most useful pieces of data in the current climate is the ever-changing technical interview bar – throughout 2022, it’s gotten progressively harder to pass technical interviews, and it’s only going to keep getting harder… because employers are gaining more leverage in the market.

One might say that because we’re a mock interview platform, publishing this post is self-serving. The reality is that we take no joy in this content. Ever since hiring freezes and layoffs have started dominating the news cycle, we’ve lost about half of our interview volume, many of the prominent employers who were hiring through us as recently as Q2 of this year have paused. So, trust us, we’d much rather be writing about something else while living in a hiring boom. And look, whether we want to see it or not or whether we have vested interest in publishing it or not, the data is the data, and the data doesn’t care. Realistically, neither will the employers who are still interviewing and hiring.

We hope that what we’re about to share will help you enter an increasingly unforgiving labor market, the likes of which engineers haven’t seen since the early 2000s, with your eyes wide open.

## The state of the world

Despite not wanting to believe it (as Upton Sinclair said, “It is difficult to get a man to understand something, when his salary depends on his not understanding it.”), I’m coming around to the idea that engineers are going to lose significant leverage in the market.

![Chart showing the number of jobs is shrinking and the number of unemployed engineers is growing](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FTech_jobs_are_shrinking_and_the_number_of_unemployed_engineers_is_growing_1_50a288a21d.png&w=1920&q=75 "Chart: Jobs are shrinking, and the number of unemployed engineers is growing")

In the graph above, the blue line is the number of open tech jobs, according to [TrueUp](https://trueup.io). TrueUp is a tech job index and layoff tracker, started by [Amit Taylor](https://www.linkedin.com/in/amittaylor/). One of its limitations is that it tracks jobs at somewhat arbitrarily defined “tech” companies and excludes jobs (even if they’re technical) at non-tech companies (e.g. Walmart… their example, not ours). However, it’s the best aggregation of open jobs over time that I’ve been able to find, and I think it’s as good a proxy as any.

The red line comes from our [own analysis of tech layoffs](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments) (it’s on its own axis because the blue line represents the number of open tech jobs across all disciplines, whereas the red line is engineers only). We recently looked at layoff lists on layoffs.fyi, tagged them by function, and did some corrections for how likely people in different functions were to opt in to those lists. We ultimately learned that engineers comprised about 5% of total layoffs overall (though eng departments were hit harder than we expected, and at companies that did layoffs, about 12% of the eng team was let go). As such, we counted up total layoffs in the US by month and took 5% of that.

**TL;DR The number of open jobs is shrinking, and the number of unemployed engineers is growing.**

How much will this actually affect hiring practices and how high the bar will actually get?

It’s hard to say, and I’m not an economist, but I do run an eng hiring marketplace, and we have some proprietary data I’d like to share with you all.

## The rising eng bar

interviewing.io is both a mock interview platform and an eng hiring marketplace – engineers use us for [technical interview practice](https://interviewing.io/mocks), and top performers get fast-tracked at companies.

Companies can actually interview our top performers anonymously, right on our platform, and leave feedback after each interview. If the candidate passes, they unmask and move to the next step (typically an onsite). Feedback is both quantitative and qualitative, and in addition to telling us if the candidate passed, companies also rate them on technical ability, communication ability, and problem solving ability. Technical ability is the most predictive and is therefore weighed the most heavily in our scoring system.

While we’re not contractually allowed to list publicly which companies hire through us, it’s historically been a mix of FAANGs, FAANG-adjacent top tier companies (e.g. ride sharing, file sharing), and up and coming startups. That is to say that we’re confident that our data about the eng bar is indeed representative of what’s happening at the top end of the market.

To figure out where the bar is and how it’s changed, we averaged the technical scores for successful interviews among senior engineers[1](#user-content-fn-1) over the last few quarters and graphed it against trueup.io’s number of open tech jobs (same as the blue line in the graph above). Below, you can see the results. The shape of the 2 curves surprised us in its uncanniness. **Since January of 2022, after a brief rise in Q2, tech jobs have contracted by 40%. At the same time, after a brief dip in Q2, the bar for a successful technical interview has increased by 10 percentile points (or by about 15%).[2](#user-content-fn-2)**

![The engineering bar keeps rising, as the number of open tech jobs shrinks](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FThe_engineering_bar_keeps_rising_as_the_number_of_open_tech_jobs_shrinks_1_01da313f30.png&w=1920&q=75 "Chart: Engineering bar keeps rising; as the number of open tech jobs shrinks")

## The interviewing.io technical interview bar index

Here’s our quantification of where the “eng bar” is. We’ll call it the **interviewing.io technical interview bar index** (ITIB index). We used historical data to measure the link between the number of open tech jobs and the typical performance of people who cleared a real technical interview on our platform. As the plot above shows, it’s a negative relationship: when the job pool shrinks, candidates have to perform better in technical interviews.

**Our regression estimates show that for every 50K drop in tech jobs, candidates need to perform about 3 percentiles higher in interviews.** Given the almost halving of jobs that has happened so far in 2022, candidates went from needing to beat about two thirds of candidates who had also gotten to the phone screen stage to needing to beat more than three quarters.

One caveat to this approach is that interviewing.io probably doesn’t represent the whole eng market for 2 reasons: selection bias among employers and selection bias among candidates. As I mentioned earlier, the employers who hire through us tend to have a high bar and conduct a specific kind of interview (typically algorithmic in the phone screen). Moreover, only people who do really well in mock interviews get to the point where they can talk to real companies, so these percentiles are comparing people who are already very strong performers to one another rather than to everyone in the pool. **However, because they’re relative rather than absolute, we feel reasonably confident that these numbers can be generalized to the overall hiring market.**

The ITIB index captures this in a single number, measuring the current technical bar in percentile terms. If it’s at 40, you need to be better than 40% of engineers to get a Yes. **Right now, it’s at 78, up from 68 in Q1 of 2022.**

We’ll be tracking this index on a monthly basis on Twitter. Follow us at [@interviewingio](https://twitter.com/interviewingio).

1. We just looked at senior engineers here because our average user is senior (~7 years of experience), and so that’s where we have the most consistent data. [↩](#user-content-fnref-1)
2. In the graph called, “The eng bar keeps rising…”, we chose to show raw scores rather than percentiles so that our users could benchmark their scores easily. However, because our raw scores likely don’t mean anything to the general public, we recomputed them as percentiles for the technical interview bar index. Also note that if you try to compute the % difference from the raw scores, you’ll get a lower number, but that’s not the right way to approach it because small differences in code scores amount to big differences in engineer ranking. [↩](#user-content-fnref-2)


# [Stop trying to make recruiters think, or, why your resume is bad and how to fix it](https://interviewing.io/blog/stop-trying-to-make-recruiters-think-or-why-your-resume-is-bad-and-how-to-fix-it)

By Aline Lerner | Published: April 29, 2025

Years ago, Steve Krug wrote a book about web design called *Don’t Make Me Think*. It’s a classic, and the main point is that good design should make everything painfully obvious to users without demanding anything of them.

Resumes are just the same. Your resume shouldn’t make recruiters think. It should serve up the most important things about you on a platter that they can digest in 30 seconds or less.

Before I share some resume tips, there’s something important I want to reiterate: **Don’t spend a lot of time on your resume**. You can read my [piece about how resume writing is snake oil](https://interviewing.io/blog/why-resume-writing-is-snake-oil), but the TL;DR is that recruiters spend a median of 30 seconds looking at resumes, and most of that is spent looking for top-tier companies. If you don’t have top-tier companies (and in some cases niche skills), wordsmithing your bullets or rearranging your sections or changing your layout won’t help. If you do have top-tier companies, sometimes doing some wordsmithing and rearrangement will help… *if your top company experience or niche skills are buried.*

If you don’t have top-tier brands, the best bang for your buck is to switch from online applications to hiring manager outreach. [Here’s how to do it.](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2)

With that said, I know that no matter what I say, people will still grind on their resumes instead of doing outreach. Grinding on resumes is safe. Outreach is scary and opens you up to personal (rather than impersonal) rejection. So, look, if you’re going to do *something* to your resume, let’s make sure that that something is low-effort and high-return. Unlike the endless resume tweaking that most candidates do, these changes directly address how recruiters actually read resumes.

Here we go.

## Stop putting filler buzzwords in your "About" section. Use it to spell out the most impressive things about you.

Your "About" or "Summary" section is prime real estate. Yet so many candidates fill this section with meaningless jargon like "passionate self-starter" or "detail-oriented team player."
Instead, use this section to explicitly tell recruiters the 2-3 most impressive things about you in plain English. This is your chance to control the narrative. *Want recruiters to take something away from reading your resume?* Don’t assume they’ll figure it out. They’re not reading it long enough to intuit anything. Spell it out for them verbatim in this section.
Do this, not that:

❌ Results-driven full-stack engineer with a passion for scalable systems and user-centric design  
✅ Senior engineer with 3 years at Amazon, promoted twice in 3 years (2X the company average)[1](#user-content-fn-1)

## Don’t include your GPA if it’s under 3.8

This is simple but effective: only include your GPA if it's 3.8 or higher[2](#user-content-fn-2). A middling GPA doesn't help your case and might inadvertently signal academic mediocrity.

If your GPA isn't stellar, focus on other academic achievements: hackathons, technical competitions, fellowships or scholarships. These provide better signals about your capabilities than a so-so GPA.

## Context matters for lesser-known companies

If you've worked at Google or Facebook, recruiters instantly get what kind of company you're coming from. But when you have "TechStartup123" on your resume, they have no idea what they're looking at or how impressive it might be.

For lesser-known companies, include a one-line description explaining what the company does, along with any impressive metrics or investors:

❌ "Software Engineer, DevTools Inc."  
✅ "Software Engineer, DevTools Inc. ($50M Series B from Sequoia, 2M+ active users)"

This simple addition provides crucial context that helps recruiters evaluate your experience properly. Without it, they might discount valuable experience simply because they don't recognize the company name.

## Avoid the "job-hopper" misperception

Here's a common mistake: listing each role at the same company as if they were separate jobs. This can make recruiters think you've job-hopped, which is often seen as a red flag.

Instead, group different roles under the same company heading:

❌ Listing separate entries for "Junior Developer at XYZ" and "Senior Developer at XYZ"  
✅ "XYZ Company - Senior Developer (2021-Present) - Junior Developer (2019-2021) *Promoted in 2 years vs. company average of 3.5 years*"

The second format clearly shows growth within a single company and explicitly highlights faster-than-average promotion, which is a strong positive signal. (You may also want to carry over your promotion cadence into your “About” section, as you saw above.)

## Be crystal clear about your work authorization status

This one is particularly crucial if you have a foreign-sounding name and/or education outside the US. I've seen many qualified candidates get passed over because recruiters assumed they needed visa sponsorship when they actually didn't. Don't leave this to chance.

Make your work status explicit in your header or summary section:

❌ No mention of work authorization (leaving recruiters to guess)  
✅ "US Citizen" or "Green Card Holder" or "Authorized to work in the US without visa sponsorship"

## Career changers: provide context about the change

If you've switched careers, your resume can look confusing without proper context. Recruiters might struggle to understand why someone with your background is applying for this role, or they might not recognize how your previous experience translates to your current trajectory.

Address this head-on in your “About” section.

❌ Listing previous career experience with no explanation of your transition  
✅ "Transitioned from marketing to software engineering in 2021 after completing a bootcamp" or "Former accountant who pivoted to data science through self-study and online courses while continuing full-time work"

This context helps recruiters understand your timeline and puts your current title and achievements in perspective. Without it, you risk serious misinterpretation:

1. Recruiters might think you're far more junior than you actually are in your new field (potentially ruling you out for appropriate-level positions)
2. Or conversely, they might assume you have years of relevant experience in your new field (and then wonder why you haven't achieved more in that time)

Both misinterpretations can be fatal to your application. By providing a clear timeline of your transition, you help recruiters accurately gauge your experience level and set appropriate expectations. This transparency also demonstrates valuable traits like adaptability and determination.

And here's another key point for career changers: you don't need to list all your previous positions before the transition... unless they're impressive. Be selective about what pre-transition experience you include:

❌ DON'T include mundane or irrelevant details from your previous career that add nothing to your current narrative. Your three years as a retail associate before becoming a developer probably won't strengthen your software engineering application.  
✅ DO highlight prestigious achievements from your previous career. If you were, say, a concert pianist, a lawyer who graduated from a top-tier law school, or a management consultant at McKinsey, absolutely include that. These signal that you're smart and high-achieving, regardless of domain.

## In conclusion

If you do all these things, you may or may not see a return. After all, even the impact of these tweaks pales in comparison to having top brands on your resume. But, given that these will take you a few minutes to do, it doesn’t hurt. Here’s the TL;DR:

![A list of high-ROI resume tweaks](https://strapi-iio.s3.us-west-2.amazonaws.com/high_roi_resume_tweaks_f23aa49f90.png)

1. I have no idea what the average promotion cadence is at Amazon, and this example is meant to be illustrative rather than accurate, though maybe my readers will tell me the cadence now. [↩](#user-content-fnref-1)
2. I realize this diverges from the advice in *Beyond Cracking the Coding Interview*, where Gayle recommends including it if it’s 3.0 or more. This is one of the cases where the authors had differing opinions. We’re (mostly) human. [↩](#user-content-fnref-2)


# [The 3 things that diversity hiring initiatives get wrong](https://interviewing.io/blog/diversity-hiring-initiatives-wrong)

By Aline Lerner | Published: March 17, 2021; Last updated: June 18, 2024

I’ve been hiring engineers in some capacity for the past decade. Five years ago I founded interviewing.io, a technical recruiting marketplace that provides engineers with anonymous [mock interviews](https://interviewing.io/mocks) and then fast-tracks top performers—regardless of who they are or how they look on paper—at top companies.

We’ve hosted close to 100K technical interviews on our platform and have helped thousands of engineers find jobs. For the last year or so, we’ve also been running a [Fellowship program](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship) specifically for engineers from underrepresented backgrounds.

That’s all to say that even though I have developed some strong opinions about “diversity hiring” initiatives, my opinions are based not on anecdotes but on cold, hard data. And the data points to the following 3 problems with these initiatives:

1. **Over-indexing on unconscious bias, which ignores the much bigger problem of discrimination based on pedigree (i.e., conscious bias):** Too many organizations invest in resume blinding and other attempts to mitigate unconscious bias at the top of the funnel. But they continue to ignore the much more problematic, ever-present *conscious* bias against engineers who didn’t attend a top school or work at a top company, which is especially troubling given that there are many more underrepresented engineers who didn’t attend top schools.
2. **Focusing on bias in the middle of the funnel rather than the top:** The top of the funnel is orders of magnitude larger than the middle. By the time you get to the middle, you’ve weeded out most of your candidates from underrepresented backgrounds, and the damage has been done. Investing in bias training during interviews, therefore, gives you diminishing returns and is more of an exercise in optics than an attempt to fix anything.
3. **Ignoring the technical interview practice gap:** Technical interviews are a learned skill, but access to practice is not equitably distributed. As a result, marginalized groups often underperform—not because they’re worse at engineering but simply because they haven’t had access to the same level of preparation.

All three of these problems are somewhat subtle and require a bit of explanation, but until they’re addressed, we’re not going to make meaningful progress toward equitable representation in software engineering.

## Problem 1: Over-indexing on unconscious bias at the top of the funnel (while ignoring conscious bias)

First off, what is the top of the funnel? In the case of hiring, this refers to the stage where we decide whether we want to move a candidate into our process. If a candidate applies online (inbound), this is the step where we look at their resume and decide whether or not to move forward. And if we’re actively trying to bring candidates into our process, this is when we look at their public profile (usually LinkedIn) and decide if we want to reach out to them (outbound).

Whether the candidate is inbound or outbound, we have limited information available to help us make a decision. The information we have available typically consists of a resume (or a resume-like profile) and contains biographical info like name, where they went to school and their graduation date, and their work history. From these few bits of data, we can glean—intentionally or not—a candidate’s gender and sometimes their race, their age, and their pedigree (i.e., did they attend an elite school and/or work at a top company?).

## Unconscious vs. conscious bias

In this context, unconscious bias refers to making quick judgments, often without being aware of it, against someone based on the biographical info and other traits gleaned from their resume. In the context of engineering hiring, we tend to focus specifically on reducing bias around attributes like gender and race (though of course discrimination based on other traits like sexual orientation, age, beliefs, disability, etc., is also unacceptable).

Conscious bias, on the other hand, is the very *intentional* decision not to move forward with candidates who lack pedigree, i.e., those who didn’t attend an elite computer science school or work at a company with a well-established brand. \*\*Unlike unconscious bias, organizations don’t typically condemn this, and more often than not, conscious bias is actually codified into the hiring process, with explicit rules for the recruiting team that say something like, “\*\****These are the schools we hire from, and these are the companies we look for on a resume. If a candidate doesn’t have either, let’s not move forward.”***

Either way, at the top of the funnel, before we ever interact with a candidate, we must decide if we want to bring them in for an interview based on very limited information, and that’s where things get complicated.

## Why anonymizing resumes isn’t enough

At the top of the funnel, the canonical way to mitigate unconscious bias is to anonymize resumes. The idea is that if we can remove all signals that might help us uncover someone’s race and gender, then we, by definition, can’t be biased, right?

Here’s the problem. Though anonymizing resumes isn’t inherently bad, it’s not nearly enough. In a nutshell, as long as we keep over-indexing on top schools and top companies, it’s not mathematically possible to hit our goals, no matter how much unconscious bias mitigation and resume anonymization we perform.

It’s a fact: there are far more women and people of color who didn’t attend one of a handful of well-known universities than there are who did. To make matters worse, filtering based on pedigree doesn’t really work! **As a result, the pedigree-based tradeoffs we currently make (in the name of keeping the bar high) exclude people from underrepresented backgrounds… without actually getting us the best candidates.**

## Over-indexing on resumes and pedigree

Let’s talk about resumes first. The resume is a demonstrably terrible way to judge whether someone is a good engineer. People often talk about how, in the spirit of fairness, it’s important to consider non-pedigreed candidates, but there’s a tacit agreement that pedigree is useful and that we’re doing something charitable by broadening the pool. As a result, we accept the premise that we’re going to be either pragmatic or fair, but not both. However, everything I’ve learned from being in this business more than a decade has shown me that pitting fairness against pedigree creates a false dichotomy.

In other words, resumes inherently don’t carry enough signal about a candidate’s engineering ability to be useful. I’ve written extensively (and angrily) on this subject, and below I’ve linked the TL;DRs for 3 studies that all point to resumes being largely useless:

- [The number of typos and grammatical errors on someone’s resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data) matters much more than where they worked or went to school (the latter, in fact, didn’t matter at all)
- [Neither recruiters nor engineers can figure out who the strong candidates are based on their resumes](https://interviewing.io/blog/resumes-suck-heres-the-data), and, more importantly, they all disagree with each other about what a good candidate looks like
- Data from interviewing.io (based on thousands of interviews) has shown that [where someone went to school has no bearing on their performance](https://interviewing.io/blog/lessons-from-3000-technical-interviews) in technical interviews

**Simply put, even if you don’t care one iota about inclusion and just pragmatically want to hire the best candidates with the fewest amount of interviews, you should STILL stop focusing on resumes and pedigree.**

So we’re making decisions based on an incorrect heuristic and bad data. That’s not great, but what does it have to do with diversity? Here’s the thing. If you make hiring decisions based on pedigree, the numbers simply aren’t there, and it’s mathematically impossible to reach your hiring goals.

Here is some napkin math. Every year, about 60,000 students graduate in the US with a computer science degree. If you only hire from top schools, your pool narrows to about 6,000. Of those, about 1,500 are women and fewer than 1,000 are people of color. With the U.S. Bureau of Labor Statistics (BLS) expecting nearly 140,000 new engineering jobs over the 2016–26 decade, it’s laughable to believe that any given company will be able to meet its diversity goals by hiring this way.

## What happens when you mitigate conscious bias

But what happens if you look beyond elite schools and start mitigating *conscious* bias? That’s when you start to have a real shot. Below is a model we built to capture the size of the candidate pool (for gender, specifically) that shows what will happen if we keep hiring the way we’re hiring now vs. expanding our pool beyond elite schools and focusing on what people can actually do, instead of how they look on paper.

![Chart showing the change in gender parity gap over time](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgender_parity_hiring_3653357b54.png&w=1080&q=75 "Gender Parity in Hiring")

You can read more about [how we built this model](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring), but we looked at two alternative sources of candidates: MOOCs and bootcamps. In 2015 alone, over 35 million people signed up for at least one MOOC course, and in 2018 MOOCs collectively had over 100M students.

Because they’re cheaper and much faster than a four-year program, bootcamps seem like a rational choice when compared to the price of attending a top university. Since 2013, bootcamp enrollment has grown 9X, with a total of 20,316 graduates in 2018. **Though these numbers represent enrollment across all genders, and the raw number of grads lags behind CS programs, the portion of women graduating from bootcamps is also on the rise and graduation from online programs has actually reached gender parity (as compared to only 20% in traditional CS programs).**

But, even if lower-tier schools and alternative programs have their fair share of talent, how do we surface the most qualified candidates in the absence of the signals we’re used to? Good news. In this brave new world, where we have the technology to write code together remotely, and where we can collect data and reason about it, technology has the power to free us from relying on proxies.

At interviewing.io, we make it possible to move away from proxies by providing engineers with hyperrealistic, rigorous mock interviews and connecting top performers with employers, regardless of how the candidates look on paper.

Or if you don’t use us, there’s a slew of asynchronous coding assessments like CodeSignal, HackerRank, and Triplebyte, that can help you filter down your candidates before you invest precious engineering time into interviewing them.

So, despite the numbers and the bevy of available solutions, why do we still latch on to mitigating unconscious bias? Sadly, it’s easier. Removing names from a resume is eminently doable. Redefining credentialing in software engineering is really hard (I know, the interviewing.io team and I have been at this for over five years). But unless we can talk openly about this problem and stop pretending that unconscious bias mitigation will solve our problems, we can’t make meaningful progress toward equitable representation.

So far, we’ve just talked about bias at the top of the funnel. What about the bias we encounter when, instead of looking at a piece of paper, we’re interacting with another human, especially a human who doesn’t look like us?

## Problem 2: Focusing on bias in the middle of the funnel rather than the top

The second problem I’ve observed occurs during the middle of the funnel, once candidates have made it through the door. Unconscious bias training tries to make us better at interacting with (and avoiding making judgments about) people who don’t look like us. It’s a well-intentioned attempt, but [sadly it’s been shown not to work](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), and it can even be responsible for people digging their heels in on the prejudices they hold.

Most importantly, and this is perhaps the more subtle point, by the time a candidate gets to an interviewer, numerically speaking, it’s too late—you’ve already filtered out most of the candidates who would have moved your numbers, and any effort you put in at this step in the funnel gives you diminishing returns.

![Diagram showing where bias exists vs where most training is focussed](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdiversity_hiring_funnel_fde8c49b99.png&w=1080&q=75 "Diversity in Hiring")

Put another way, there are often 100X the number of candidates at the top of the funnel than at the next stage. Most get cut before they ever talk to a human, simply based on their resumes. Despite that, employers are investing in training interviewers, which unfortunately is only relevant at a stage that most of the non-traditional talent won’t reach. Any gains we can make in the middle of the funnel are mathematically negligible compared to the gains we can make at the top (where, as you saw in the previous section, we persist in our conscious bias and call it a best practice).  
Especially when you have limited resources to mitigate bias in your process, and as more and more of your team fall victim to [diversity fatigue](https://www.forbes.com/sites/janicegassam/2020/05/27/has-diversity-become-a-dirty-word/?sh=9881c585a9bd), investing so much effort into a part of the funnel that has a very real cap on its ROI makes no sense—even less so given that the thing you’re doing doesn’t even work.

## Problem 3: Ignoring the technical interview practice gap

Of all the problems with diversity hiring, this one is the most subtle and perhaps the most damaging. Because it’s such an important (and tricky) topic that requires going in depth to fully understand, I encourage you to read the [detailed post that I wrote last week](https://interviewing.io/blog/technical-interview-practice-gap) focusing just on this topic. .

Attending an elite computer science institution (like MIT, where I went) provides students with a number of benefits, but perhaps the most significant is boundless access to technical interview practice.

MIT offered a multi-week course dedicated to passing technical interviews, and I got support from my peers who were interviewing at FAANG for internships and new grad positions. This allowed us to practice with each other, share our successes and failures, and recognize just how much of technical interviewing is a numbers game. **And we learned that bombing a [Google interview](https://interviewing.io/guides/hiring-process/google#google) did not mean that you weren’t meant to be an engineer. It just meant that you needed to work some more problems, do more mock interviews, and try again at Facebook.**

But let’s put the anecdotal experience aside and examine the data that we at interviewing.io have collected. We’ve hosted close to 100K interviews on our platform, which has taught us two things about the technical interview: 1) like standardized testing, it’s a learned skill, and 2) unlike standardized testing, interview results are not consistent or repeatable—the same candidate can ace one interview and fail another one the same day.

## The importance of interview practice

In a recent study, [we looked at how people who got jobs at FAANG performed](https://interviewing.io/blog/how-know-ready-interview-faang) in practice vs. those who did not. We discovered that technical ability did not obviously associate with interview success, but the number of practice interviews people completed (either on interviewing.io or elsewhere) did. Surprisingly, no other factors we included in our model (seniority, gender, degree, etc.) mattered at all.

**Secondly, technical interview performance from interview to interview is fairly inconsistent, even among strong candidates.** (Notably, consistency appears to have nothing to do with seniority, pedigree, or anything else.) **In fact,** [**only about 20% of interviewees perform consistently**](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary) **from interview to interview. Why does this matter? Once you’ve done a few traditional technical interviews, you learn to account for and accept the volatility and lack of determinism in the process.** If you happen to have the benefit of speaking with friends who’ve also been through it, it only gets easier. But what if you don’t?

## How the interview practice gap hurts underrepresented groups the most

In an earlier post, we wrote about how [women quit interview practice 7 times more often than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) after just one bad interview. Unfortunately, this is likely affecting other underrepresented and underserved groups as well.

**This is a broken process for everyone, but the flaws within the system hit these groups the hardest—and simply because they haven’t had the opportunity to internalize exactly how much of technical interviewing is a game.**

So what does this have to do with the practice gap? The key takeaway from our research has shown that there is **a meaningful bump in performance** (almost 2X as likely to pass!) **for candidates who have completed at least 5 practice interviews.** This isn’t a lot of interviews for someone who’s actively practicing and knows how the interview prep game works. But it’s a far cry from what companies who are looking to boost their diversity numbers offer their candidates, and it’s equally far from what bootcamps, an increasingly important source of candidates from underrepresented backgrounds, provide their students.

Why are bootcamps, universities, and employers all falling short and exacerbating the practice gap? With employers and bootcamps, the teams responsible for facilitating interview info sessions or mock interviews typically have non-technical backgrounds and lack a good grasp of the significant differences between preparing for behavioral interviews and technical ones.

Preparing for technical interviews (and other analytical, domain-specific interviews) is more similar to studying for a math test than rehearsing how to present yourself and tell your story. But until you’ve done it, you won’t really get it.

The worst effect of the practice gap occurs when companies simply walk away from their expanded recruiting efforts, with the mistaken belief that they should return to recruiting exclusively from elite institutions. Or worse, they sometimes have their unconscious biases about race and/or gender confirmed, when the actual problem isn’t the candidates but their lack of access to practice.

In fact, a few years ago, we ran a study where we looked at interview performance by school tier. For students who were actively and regularly practicing, [there was no difference between elite schools and non-elite schools](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)!

![Charts showing new grad interview performance by school tier](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FNew_Grad_Interview_Performance_by_School_Tier_b85571f044.png&w=1080&q=75 "New Grad Interview Performance by School Tier")

## How to actually increase representation

So far I’ve outlined my three main problems with diversity hiring initiatives—but what’s the solution? A few things:

### Stop over-indexing on resumes

Of course, this post lives on our blog, so I’ll take a moment to plug what we do. In a world where there’s a growing credentialing gap and where it’s really hard to figure out how to separate a mediocre non-traditional candidate from a stellar one, we can help. interviewing.io helps companies find and hire engineers based on ability, not pedigree. We give out free mock interviews to engineers, and we use the data from these interviews to identify top performers, independently of how they look on paper. Those top performers then get to interview anonymously with employers on our platform (we’ve hired for Lyft, Uber, Dropbox, Quora, and many other great, high-bar companies). And this system works. Not only are our candidates’ conversion rates 3X the industry standard (about 70% of our candidates ace their phone screens, as compared to 20-25% in a typical, high-performing funnel), about 40% of the hires made by top companies on our platform have come from non-traditional backgrounds. Because of our completely anonymous, skills-first approach, we’ve seen an interesting phenomenon happen time and time again: when an engineer unmasks at the end of a successful interview, the company in question realizes that the student who just aced their phone screen was one whose resume was sitting at the bottom of the pile all along (we recently had someone get hired after having been rejected by that same company 3 times based on his resume!).

Frankly, think of how much time and money you’re wasting competing for only a small pool of superficially qualified candidates when you could be hiring overlooked talent that’s actually qualified. Your CFO will be happier, and so will your engineers. Look, whether you use us or something else, there’s a slew of tech-enabled solutions that are redefining credentialing in engineering, from asynchronous coding assessments like CodeSignal or HackerRank to solutions that help you vet your inbound candidate pool, like Karat.

And using these new tools isn’t just paying lip service to a long-suffering D&I initiative. It gets you the candidates that everyone in the world isn’t chasing without compromising on quality, helps you make more hires faster, and just makes hiring fairer across the board. And, yes, it will also help you meet your diversity goals.

### Stop paying for unconscious bias training, and spend that budget on interview practice for your candidates

As you saw above, interview practice is absolutely key to success in technical interviews, but access to practice is not equitably distributed. I contend that tech giants, universities, and bootcamps could close the practice gap, level the playing field in software engineering, and hit our representation goals by simply providing candidates who need them with five professional mock interviews each. Hell, we would do it ourselves if we had the means. Yes, this will cost a few hundred dollars per candidate, but compared to the cost of sourcing and interviewing them, it’s a small change.

Moreover, if you’re spending a significant amount of money on initiatives like unconscious bias training, [which has been shown to be ineffective](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), you could make a much bigger impact by shifting that budget toward interview practice for your candidates.

If you’re an employer, [reach out to us](mailto:hello@interviewing.io) so we can help you make this a reality.

Whether you only do some of these things, and whether you do them with us or with other tools and products, let’s stop simply talking about how we need to fix representation in tech and instead do something that actually makes a difference. We can fix the diversity hiring initiatives that have failed us and ensure that the best people actually get hired.


# [What’s actually going on with Google and Facebook hiring freezes? We surveyed 1000 engineers to find out.](https://interviewing.io/blog/google-facebook-hiring-freeze)

By Aline Lerner | Published: July 31, 2022; Last updated: May 6, 2024

It looks like we’re entering a recession. One of the hardest things about it is the lack of reliable information about whether companies are still hiring and what hiring freezes even mean. Arguably the two most impactful eng hiring freezes were announced by [Facebook (May 4, 2022)](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5) and then [Google (July 20, 2022)](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs). Facebook’s freeze is allegedly partial, targeting roles below L7 and excluding machine learning engineers. Google’s freeze is allegedly all-encompassing but may only last 2 weeks. But what’s actually going on? To make some sense of a bunch of contradictory information about Google’s and Facebook’s hiring freezes in the press and on Blind, we decided to ask the people who, outside of Google leadership, will probably know best what’s going on – engineers who are *interviewing at these companies right now.* In this post, we’ll share what we learned from surveying 1003 of our users.

Why do we care about these freezes at interviewing.io? First off, they affect our users – a good chunk of our users are practicing for upcoming [Google](https://interviewing.io/guides/hiring-process/google) and [Facebook](https://interviewing.io/guides/hiring-process/meta-facebook) interviews. Moreover, these freezes materially affect our business. On our platform, you can practice with engineers from a specific company, and many of our users opt to practice with Google engineers before their Google interview and Facebook engineers before they begin the [Facebook interview process](https://interviewing.io/blog/google-facebook-hiring-freeze). To wit, here is a graph of mock interviews on our platform with Facebook and Google engineers, by week. As you can see, we’re definitely incentivized to care about what’s actually going on with these freezes.

![Chart showing Google and Facebook mock interviews on interviewing.io by week](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffacebook_google_mock_interviews_6f9e02e555.png&w=2048&q=75 "Chart showing Google and Facebook mock interviews")

Note that though our user base isn’t a perfectly representative sample of engineers interviewing at Google and Facebook, we have reason to believe that it’s reasonably representative. We’ve been around for about 7 years, have hosted over 100K technical interviews on our platform (split between mock interviews and real ones), and thousands of engineers sign up to use interviewing.io every month. Our average user has 7 years of experience, and about 40% of our users either currently work or have worked at a FAANG company.

## Our survey

Our survey went out to users on July 23rd and got 1003 responses (mostly between July 23rd and July 25th). There were a bunch of questions, but here are the 3 most relevant ones:

![Survey question: Are you currently actively interviewing with any of the following companies?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_one_1024x789_c325e2022a.png&w=1080&q=75 "Question #1")

![Survey question: For each of the companies you mentioned in the previous question, which level are you applying for (e.g. L5, E6, etc?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_two_1024x317_9faf480a7a.png&w=1080&q=75 "Question #2")

![Survey question: For the comapnies you mentioned in the previous question, are you appying for SWE/SDE roles or something more niche?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsurvey_question_three_1024x515_979811f9f2.png&w=1080&q=75 "Question #3")

As mentioned above, we had 1003 people fill out the survey. Here’s what we learned.

- Respondents were primarily based in the US with an average of 6 years of experience (median of 5).
- 48% were actively interviewing (way more than we expected, given all the recession talk)
- These are the companies where they were actively interviewing (percents below won’t add up to a 100 because respondents could select as many options as they liked):

![Screenshot of survey results](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fhiring_freeze_059e455d0f.png&w=1080&q=75 "Hiring Freeze")

I wasn’t surprised to see Amazon at the top of the list, given that they haven’t announced freezes and are [continuing to aggressively hire engineers](https://www.retaildive.com/news/amazon-to-hire-more-tech-corporate-workers-in-california-hubs/623167/).[1](#user-content-fn-1)

I *was* surprised that, despite their 2-week freeze announcement, Google was right at the top as well. Given that Facebook had supposedly been frozen for a while, I was surprised to see them in 3rd position.

So, we dug into the data for Google and Facebook specifically.

## What’s going on with Google and Facebook?

### Facebook

Facebook first [announced their hiring freeze on May 4th](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5). Though the announcement made it clear that the engineering org would be affected, it didn’t go into detail. Of course, around the time that the freeze was announced, commentary about the freeze also started popping up on Blind ([like this one](https://www.teamblind.com/browse/Facebook-Hiring-396)). Consistently from thread to thread, it looked like Facebook froze hiring for engineers below E7, with the exception of Machine Learning Engineers, whom Facebook continued to hire, at all levels.

Based on what we learned from the 76 survey respondents who were actively in process with Facebook, it does appear that SWE below E7 is indeed frozen, but there are some eng functions that are still hiring, in addition to Machine Learning.

| Facebook roles frozen | Facebook roles NOT frozen |
| --- | --- |
| SWE below E7 | E7+ SWEs |
|  | Machine Learning Engineers |
|  | Production Engineers (devops and/or SRE) |
|  | Enterprise Engineers (work on Facebook’s internal tools & systems; have  their own hiring process distinct from SWE) |
|  | Managerial tracks, including M1 (some sources had said that hiring was  frozen at M1 but active for M2 and up) |

We did one other test as a sanity check. What if Facebook was using the hiring freeze as a smoke screen and only continuing with the absolute strongest candidates while keeping the rest on the bench?

Our best insight into candidate ability is how they perform in interviews on our platform (we do both mock interviews and real ones and have hosted over 100K interviews to date). On interviewing.io, every user who’s done at least 2 mock interviews has a score, and this score is a weighted average of coding ability, problem solving ability, and communication on a scale from 0 to 1. We use it internally to figure out who the strongest performers are so we can best match them with companies.

First, we grabbed the overall mean score on interviewing.io, across all users. Then we compared that mean to the mean scores of people who were still interviewing at Facebook. There wasn’t a statistically significant difference. In other words, from what we know, Facebook probably isn’t using their hiring freeze as a smoke screen.

First off, Google is supposedly going to announce whether the two-week freeze is going to continue, and if so, on what terms, on August 3rd. So there’s a good chance this post will age poorly!

That said, I’m a firm believer in marching on with the information we have at the time, so here goes…

Analyzing Facebook was fairly straightforward because with the exception of a few new pieces of information (e.g. Production Engineering is still hiring), what was in the press and on Blind was consistent with reality. Figuring out what was going on at Google, on the other hand, was a harder task.

Google first [announced their hiring freeze on July 20th](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs), following an announcement that hiring would slow for the rest of 2022 but that headcount would remain open in critical roles, including engineering. A Google spokesperson then clarified that over these 2 weeks, Google would be pausing “most offers”.

In our survey, 210 people were actively in process with Google. That seemed high, so I reached out to all of them to get their take on what was going on and to confirm that they were indeed actively in process and not in nebulously waiting for some next step that may or may not come.

The results were fascinating. First, here’s what appears to be true across the board:

- Google is indeed not extending offers throughout the duration of the freeze. In other words, there are people in various stages of interviews, but even if they get past the hiring committee, they wouldn’t get an official offer until the freeze is lifted.
- L3 or “early career” hiring is indeed frozen until next year. Almost everyone who responded to us and had been interviewing for L3 was paused. That said, it looks like Google is willing to continue L3 interviews in some cases with the understanding that those results will be valid for a year, so that when headcount for L3 opens up again, candidates will be able to skip the onsite process.

The rest is… murky. From what I can tell, different recruiters are telling people different things. Here’s a sample.

> *I asked my Google recruiter the situation with Google hiring, and they confirmed that for at least L3 Early Career SWE, there [is no headcount] left for 2022…. They also said that other positions (L4+) are still actively hiring.*

> *I just received a phone call from the Google recruiter yesterday. She said that they are slowing down hiring for “early career” developers, which are L3 and L4, for the rest of the year, but higher levels are still in full swing.*

> *I just had a conversation with the recruiter and they HAVE a hiring freeze until Aug 3rd, but they didn’t stop the interviewing and team matching process.*

> *I am currently in a freeze mode with them. I was in the team matching process and after several hiring manager’s interviews, they came to me with the news to wait until they reprioritize stuff.*

> *Google is hiring in full swing… Their engineering teams are going to move people around in two weeks to fill some of the open roles but that’ll not affect much in hiring.*

> So, according to their response to me, I “may experience some delays in our hiring process” [but] I was specifically told that the hiring process is not halted with me.

> *I’m in process still, but they said that even if I do well on the interview I [will] not receive an offer until the hiring slowdown ends.*

As several of these statements blatantly contradict one another, I went back to the data and ran the same sanity check that I did with Facebook above. In other words, what if Google is using the hiring freeze as a smoke screen and only continuing with the strongest candidates, while keeping the rest on the bench?

As above, our best insight into candidate ability is how they perform in interviews on our platform (see the Facebook section above for an explanation of our scoring system and our approach).

Just like in the Facebook analysis, we grabbed the overall mean score on interviewing.io, across all users. Then, we compared that mean to the mean scores of Google interviewees, this time *at various levels* (L4, L5, and L6). Unlike the Facebook analysis, we did see some statistical significance at L4. Candidates who were still actively interviewing at L4 had significantly higher interviewing.io scores than the average interviewing.io candidate (p < 0.03 with a “medium” effect size of 0.45). We did NOT see any significance for candidates at L5 and above.

Note that we don’t have a nice, clean table in this section where we show what’s frozen and not, like we did for Facebook, precisely because the results were murky. It feels like Facebook made a clear set of rules and then stuck to them and communicated them effectively to candidates. With Google, it feels like decisions are being made on a case-by-case basis. That said, from this running the numbers and interacting with our users, here’s what we *think* is going on.

- L4 is partially frozen, but allowances/exceptions are possibly being made for interview performance – if someone interviews well, then they’re being moved forward, to onsite and to hiring committee. Others are being paused. It’s about half and half for L4 (see the graph below). Note that this is quite presumptive, and I don’t have a lot of conviction around it, but given the data we have, it’s a reasonable conclusion.

![Chart showing that Google appears to be selectively applying their hiring freeze, depending on level](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FGoogle_appears_to_be_selectively_applying_their_hiring_freeze_depending_on_level_811c48f4c3.png&w=1920&q=75 "Google continue to interview candidates")

- Unlike L4, L5 and up are actively hiring almost across the board, regardless of performance.
- Machine Learning, Technical Program Manager, and Engineering Manager roles are actively hiring across the board.

## Limitations and conclusion

After running the numbers and talking to our users, it appears that Facebook eng hiring is largely frozen, with some key exceptions. Google, on the other hand, has frozen extending offers for 2 weeks but is still actively interviewing candidates, except for junior ones, sometimes making decisions about whom to interview and whom to pause on a case-by-case basis. We’ll know more about the Google situation in a few days, once the 2 weeks period runs out.

Given that both companies are clearly prioritizing senior eng roles, we shipped something we hope will help. On interviewing.io, you can now choose to practice specifically with a Staff-level (or above) engineer from Google or Facebook. If you’re still in process, and especially if you’re interviewing for L6/E6 or above, this is a direct line to the best-calibrated, most experienced interviewer you can get.

Whether you use us to practice or not, these are uncertain times, and there’s a lot of bad information out there. With this post, we tried to offset it by reporting on what we’ve learned from primary sources, our users who are/were actively interviewing at both Google and Facebook. While I can count on one hand (maybe both?) the limitations to our approach (e.g. our scoring system being inaccurate, sampling bias, people not being honest in their survey responses, not enough data, us failing to analyze certain attributes of candidates, and more), when things are uncertain and you have limited information, you have to do the best you can with what you’ve got.

This analysis started as an internal exercise to help us get some predictability around our usage and our revenue. Then as I started working on it, I realized that people outside of interviewing.io might want to see it too. So I hope you, dear reader, will forgive us our data trespasses and know that we did this work with the best intention in mind – combating misinformation and getting our best attempt at the truth out there.

*If you have reliable information that contradicts what you’ve read in this post, please get in touch with me ([aline@interviewing.io](mailto:aline@interviewing.io)). I commit to making edits as I learn new things. Finally, a huge thank you to everyone who filled out our survey and answered my annoying followup questions.*

1. If you’re interviewing at Amazon, you should read our brand-new [Software engineer’s guide to the Amazon behavioral interview](https://interviewing.io/guides/amazon-leadership-principles). 20% or more of people who pass Amazon’s technical bar end up not getting an offer because they do poorly in the behavioral portion. [↩](#user-content-fnref-1)


# [The other half, or why interviewers aren't always great and how to make them better](https://interviewing.io/blog/the-other-half-or-why-interviewers-aren-t-always-great-and-how-to-make-them-better)

By Jos Visser | Published: June 10, 2024

*Hey, Aline (founder of interviewing.io) here. Time for another awesome guest post.*

*We’ve written before [about what the data says makes a good interviewer](https://interviewing.io/blog/best-technical-interviews-common) and about how [companies do a poor job of incentivizing engineers to be good interviewers](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it). But we’ve never really described what it feels like to interact with a great interviewer and how being one matters now, more than ever, given how well-prepared candidates have become and how much harder it is to get signal from them as a result.

Which is why I’m so excited about this guest post from Jos Visser. Enjoy!*

*Btw, if you have strong opinions about interviewing or hiring that you’ve been itching to write about, we’d love to hear from you. Please email me at [aline@interviewing.io](mailto:aline@interviewing.io).*

![Author avatar](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fjosv_mug_f6353fb1fd.jpg&w=384&q=75 "Jos Visser")

Jos Visser

Jos Visser is a consummate IT professional with over four decades of experience interviewing and being interviewed. He has over 500 interviews under his belt at companies like Google, Facebook, Amazon, and OpenAI. On top of that he was a member of hiring committees and created and delivered many different training programs aimed at making people better interviewers. Jos can be reached at [josvisser66@gmail.com](mailto:josvisser66@gmail.com) or through [LinkedIn](https://www.linkedin.com/in/josvisser1/).

It is high time we start talking about interviewing. I know it seems like we are talking about interviewing all the time, but we are usually talking about only one half of the equation: How to be a good candidate. What about [the other half](https://marillionofficial.bandcamp.com/track/the-other-half)? What about the interviewer?

In the last decade, there has been an explosion of attention for candidates and how to improve their interview performance. There are blogs, [sites](https://interviewing.io/), [companies](https://interviewing.io/), and [books](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850) that help you improve your interviewing skills. You can sharpen your coding skills, learn how to approach a [system design interview](https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF/ref=sr_1_1?dib=eyJ2IjoiMSJ9.CZwZ7txhICEtME2JuLCqj9fgDsTqCNi6KP7xke4eHp__iSdKSNwJz56sicfx_gWfRM7WlOrimmrvhV3DbzxY-0yUKmZGzld1ApW5-nESf1MKb1R_X9QteAFNuLJICcdWXjZKH8r4AH_0iEBs23dbBT4rpp3Nq87gQuIohh2HraAtqEvgw34HTXdR1ZnKnRR4-d9ZIA3m3wxWE2K8j4ZgubgB5dXZ3_0KjYWyti-kwcw.ysz-FWrN1DJdhG6ecHx7ipt7R1Cl3dOEnxwKurNtL18&dib_tag=se&keywords=alex+yu+system+design&qid=1717331123&sr=8-1), read entire volumes of [standard answers to standard questions](https://www.themuse.com/advice/interview-questions-and-answers), do [mock interviews](https://interviewing.io/), and learn how to use the [STAR method](https://www.themuse.com/advice/star-interview-method) to answer behavioral questions. These days, candidates are often very well prepared for a standard interview and therefore, bar a brain fart, do well at that.

This stands in stark contrast to the preparation of the *interviewer*. If you are lucky, your interviewer might have gotten a two hour class on how to ask only bona fide work related questions and has sat through two shadow interviews. Maybe they have even done a few interviews! Consequently, there is a lot of bad interviewing being done. That needs to change.

Let’s start by acknowledging that interviewing is very hard. The average interview is a 45 minute video conference in which you need to get the audio and video hardware working, engage in introductory chit chat, give the candidate some time to ask you any questions , and ascertain whether they got what it takes to be successful in the role. Given all the overhead involved, you *might* only get 35 minutes for the meat of the interview. This is not a lot of time, especially if the candidate is, as I indicated above, often very well prepared.

Don’t get me wrong, it is *good* that candidates are prepared. It shows an interest in doing well and displays the kind of methodical approach to reach a goal that I *want* to see in the candidate. But it *does* make interviewing a lot harder.

In the olden days, you could surprise a candidate by asking how to [remove a file called -f](https://www.linkedin.com/posts/josvisser1_a-standard-linux-interview-question-is-how-activity-7199771506543841281-1Rrj?utm_source=share&utm_medium=member_desktop) or [how to sort one million eight-digit numbers with only 1MB of RAM](https://stackoverflow.com/questions/12748246/sorting-1-million-8-decimal-digit-numbers-with-1-mb-of-ram/13004000#13004000). It would unfailingly be the first time the candidate ever saw that question and so you were immediately seeing how the candidate dealt with solving a problem that they had never solved before and probably never even thought about. This often prompted interesting discussions and you could see the candidate’s brain at work in solving a novel problem.

We live in different times now. A lot of candidates have seen all the common questions and are exquisitely prepared for them. As a result, whenever they give the right answer, you have learned nothing about the candidate, other than that they were probably well prepared for this particular question. Take the [min-stack question](https://leetcode.com/problems/min-stack/description/). I used to like this question because it is not entirely trivial and has some half-way interesting trade offs between time and space complexity. Then some genius came up with a non-trivial solution that [solves](https://www.geeksforgeeks.org/design-a-stack-that-supports-getmin-in-o1-time-and-o1-extra-space/#) the question in O(1) time and O(1) space. It is a super cool trick and I would not expect *anyone* to come up with it from scratch in 35 minutes. If you ask the question today and people immediately knock out the optimal solution, you *know* that they memorized the answer and you have learnt nothing. Same for how to design a [URL shortener](https://systemdesignschool.io/problems/url-shortener/solution) or a [key/value store](https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers/system-design-the-key-value-store).

This experience might give rise to the idea that all the current questions are known and that we just need a collection of new questions. That idea is wrong; it is not about the questions. Exaggerating a bit, I am of the opinion that all interview questions that can be asked within the scope of a 45-minute interview are known already. Coming up with a “new” question typically just means replacing the gift wrapping around the core algorithm of an old question with new gift wrapping, thereby making it look new. Even if, through a stroke of genius, you manage to come up with a brand new unique never-before-seen question, there is no way to keep it secret. As soon as you give it to a candidate, the question is out and the entire candidate prepping industry will cover it on their sites and blogs, and in their mock interviews and books. It’s like keeping the new Taylor Swift album secret, [it ain’t gonna happen](https://www.washingtonpost.com/entertainment/2024/04/19/taylor-swift-tortured-poets-department-album-leak/).

So interviewing is hard to begin with and even harder with well-prepared candidates. How to deal with that?

The non-answer to this question in my opinion is to replace (some) interviews with take-home assignments. [I wrote about this before](https://josvisser.substack.com/p/interviewing-has-become-a-mess) and by and large I consider it a mistake. Not only does it put the burden of getting a better signal solely on the candidates, it also is a method to ensure that great candidates, with lots of places to go, do not bother to enter your interviewing process. It’s just another example of penny wise and pound foolish. I am not saying it cannot be done right, but what I have seen so far does not inspire me with confidence.

Instead, my answer is to match like with like: If the candidates are becoming better at being candidates, the interviewers need to become better at being interviewers.

One of the ways in which a lot of interviewers need to become better is through getting rid of the idea that good interviewing starts and ends with good questions. Nothing could be further from the truth. Surely, a good question is always better than a bad question, but a *good* interviewer can still get lots of signal from handling a bad question well, whereas a *bad* interviewer cannot get any signal from a great question. It really is not about the question, and that is one of the things I think good interviewer training needs to cover.

But if good interviewing is not about the question, then what is it about? The answer is: **Digging**.

Let’s take the world famous “how to remove a file called -f” interview question. There is a canonical answer, which is to use rm -- -f. If the candidate knows this, that’s okay, but not more than that, because you haven’t learned anything about the candidate’s knowledge and thinking yet (other than they know this “trick”). When I get this answer, I would be wondering whether the candidate *understands* why this works. Do they understand if rm -f? Or rm '-f'? Or rm ./-f? And what about X=-f rm $X? And if so, or if not, why, or why not? And why exactly does a plain “rm -f” not work? Would rm -\* work? And does it matter what Unix system you are on? Or which shell you are using? And does python -c "import os; os.unlink('-f')" work? And if so why? And if rm -- -f works, does that automatically mean that touch -- -f works to create that file?

You can use this very simple question as a starting point for an extensive discussion about shell argument expansion, command line parsing by Unix binaries, the getopt(3) library function, the unlink(2) system call, and the many interesting ways in which the GNU, BSD, and System V versions of common utilities differ in this respect. While you are at it, you might as well throw in follow up questions about $IFS, spaces in file names, hidden files, inode numbers, device files, hard links and symbolic links If -f is a symbolic link, does rm -- -f remove the link or the file linked to? And what if -f was a hard link?

Interviewing is about *digging*, about never taking the candidate’s answer at face value, and about taking the answer and using it as a jumping board to ask follow-up questions. I really don’t care if the candidate knows the answer to my questions up-front. What I care about is whether they can reason towards the answer using the knowledge they have and the additional knowledge I might give them in the course of the interview. If the candidate comes in knowing next to nothing about the Unix shell, but they can deduce what is going on from the examples I give them and propose experiments to test the hypotheses they come up with, I am as [happy as a clam at high tide](https://idioms.thefreedictionary.com/happy+as+a+clam+at+high+tide).

Interviewers can learn how to dig. They can learn how to write better feedback. They can learn how to evaluate a candidate correctly against the hiring bar. It might not be easy, but it is not not rocket science either. That said, learning how to interview well is real work; real work that you can *only* expect from people who are *interested* in being great interviewers and who are duly recognized.

It is probably a dirty secret that in many companies, a lot of engineers are seeing interviewing as a chore to be dispensed with quickly and with as little effort as possible: They often get little to no training, there is no quality control, no feedback on feedback, and there are no incentives to be great or to get better. No wonder that a lot of interviews are suboptimal.

Fortunately, in most companies, you don’t need all engineers to be interviewers, so you can restrict yourself to the ones who *want* to interview. You can then create a culture of interviewing; a culture where great interviewing is the norm, and where that great interviewing is rewarded.

Rewards are essential here. To quote [Charlie Munger](https://fs.blog/great-talks/psychology-human-misjudgment/): “Show me the incentives, and I’ll show you the outcome.” It doesn’t have to be money. Or at least, not a lot of money, as rewards come in many forms: Public recognition, swag, a “Great Interviewer” mug, a badge on the internal phone book pages, or a shout out at the all-hands. All of these conspire to give people the idea that they are doing important and meaningful work. Plus it gives the managers of the interviewers the idea that they are enabling an essential company function. And of course: Great interviewing needs to help you get promoted. If it doesn’t, then you might as well go write another design document rather than spending your time on attending your local interviewing club.

There is unfortunately no cheap and easy way for your interviewers to become better. It is not hard to improve interviewing quality and effectiveness, but it does require time, money, and effort. Plus it helps if you know what you are doing (as a company hiring engineers).


# [Why resume writing is snake oil](https://interviewing.io/blog/why-resume-writing-is-snake-oil)

By Aline Lerner | Published: April 1, 2025

I just asked ChatGPT to size the global resume writing industry. Here’s what it had to say:

*The global resume writing industry was valued at approximately $1.37 billion in 2024 and is expected to grow steadily, reaching around $1.44 billion by 2025, and about $1.59 billion by 2033... The growth is driven by increased demand for professional resume services due to heightened job market competition, coupled with advancements in technology and personalization through AI-driven resume writing tools.*

I don’t know if these numbers are exactly true, and I don't know what portion of that is resume writing for engineers specifically, but it doesn’t really matter. I am certain that they are directionally correct. As the market has gotten worse, I’ve heard more and more job seekers ask for resume reviews and rewrites, and I’ve seen many companies in the interview prep space start offering resume reviews.

They’re all selling snake oil, and no one should spend a dime on it. I’ll explain why in a little bit, but first let’s talk about something else I found on the internet.

A few days ago, I saw [this post on Reddit](https://www.reddit.com/r/codingbootcamp/comments/1jhitoc/recruiter_accidently_emailed_me_her_secret/?share_id=WXxsyzj2kiIrpK7t6Xj66). It was a leaked internal set of hiring requirements (sometimes called a “hiring spec”) that looked like this:

![Image 1 from reddit post showing leaked internal set of hiring requirements](https://strapi-iio.s3.us-west-2.amazonaws.com/recruiter_accidently_emailed_me_her_secret_internal_v0_bbcbihi64bqe1_b9ed19715a.png)
  
![Image 2 from reddit post showing leaked internal set of hiring requirements](https://strapi-iio.s3.us-west-2.amazonaws.com/recruiter_accidently_emailed_me_her_secret_internal_v0_j2stzky64bqe1_89a24b8843.png)

Of course, there was the usual Reddit shock and awe and pearl clutching about whether this hiring spec could be real.

Yes, it’s real. As someone who’s been in hiring for over a decade, I’m certain of it. And not only is it real, but it’s routine. It’s business as usual.

I’ve been a head of talent at top startups, and I used to run my own recruiting agency where I hired for a bunch of companies who have since become household names. When I worked as an agency recruiter 10 years ago, companies regularly shared documents like this one with me. The only difference between then and now is the idea of a “diversity bonus.” Everything else hasn’t changed in a decade.

Documents like this are why I quit recruiting to start interviewing.io.

And documents like this are the reason that the entirety of the resume writing profession is a snake oil pit.

There is one notable exception to this rule, which I’ll talk about, but most people should not spend a dime on resume writers. Here’s why.

## Recruiters aren’t reading your resume. They’re skimming it for very specific things.

In 2024, we ran a [study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes) where we asked 76 recruiters to look at resumes and indicate which candidates they’d want to interview. Recruiters are most likely to contact you if:

- You look good on paper (i.e., you have top-tier companies and/or schools on your resume… just like in the “hiring spec” above)
- You belong to a group that’s been traditionally underrepresented in tech (i.e., you’re a woman or a person of color… just like in the “hiring spec” above)
- To some extent, if you have niche skills (e.g., ML engineering)

What's missing? Things like, for example, having a quantifiable impact or demonstrating teamwork. Essentially, everything recruiters look for is stuff that you either have or you don't.

In this same study, we also learned that when recruiters do look at resumes, they spend an average of 30 seconds reviewing them. That's not enough time to read every bullet. Instead, they are mainly skimming for recognizable companies and schools.

Here is an excellent example, [also from Reddit](https://www.reddit.com/r/recruitinghell/comments/qhg5jo/this_resume_got_me_an_interview/), that makes this difference very clear.

![this resume got me an interview 1.png](https://strapi-iio.s3.us-west-2.amazonaws.com/this_resume_got_me_an_interview_1_4beabb1543.png)

This resume certainly passes the skim-test: good companies, appropriate roles, and a good university too. It's only when you actually spend more than 30 seconds reading the resume that you learn that not only is this resume obviously fake, but it also celebrates accomplishments like "Spread Herpes STD to 60% of intern team.” And yet, it got a 90% callback rate. Recruiters just aren't reading the details.

**In other words, either you already have what recruiters are looking for (which often may be different than what’s explicitly listed in a job description… because they certainly aren’t sharing the real “hiring spec”) or you don’t. If you have it, then you don’t need a resume writer — though it’s always smart to make it easier for recruiters to find the things they’re looking for. If you don’t have what they’re looking for, no amount of agonizing over how you present yourself is going to move the needle.**

So, if recruiters aren’t reading and are just skimming for brands, why do people agonize over their resumes and give money to resume writers?

## Why the resume myth persists

In interviewing.io’s Discord server, I regularly see requests for resume reviews. I also see other interview prep companies charging money for resume reviews. Presumably they charge because the demand is there. But why are people willing to pay for something that is completely useless?

I think it’s a mix of misinformation and the desire for control.

Recruiters rarely admit that they’re skimming primarily for brands. If you read recruiters’ advice for job seekers, it almost always includes advice about quantifying your impact, including your side projects[1](#user-content-fn-1), and so on. These bits of advice are well-intentioned, I’m sure, but they perpetuate a harmful myth and an exploitative resume writing cottage industry.

The other reason is control. Job searches are intimidating, and putting yourself out there is hard. It’s much easier to retreat to the comfort of polishing up your bullet points because it’s something you can control. You get into a routine, rewrite your bullets, and upload your resume to a bunch of places. Then when you don’t hear back, you retreat to familiar ground, grind on your bullets some more, and rinse and repeat. Because it’s easier to believe that if you can just get your bullets right, you’ll finally hear back. That narrative sure beats out the idea that no one is reading your resume no matter how much you fine-tune it.

## The notable exception: If you already look good on paper, polishing your resume CAN be useful.

If you’re fortunate enough to have top brands on your resume, cleaning it up can be a good use of your time. I still wouldn’t hire a resume writer because the details don’t matter very much. Just make sure that recruiters can easily spot the brands.

Here’s an example. Take a look at the before and after screenshots of the resume below.

**Before**

This resume belongs to one of our users who was kind enough to let us share it. He actually has two of the three things that recruiters look for: FAANG experience and a niche title (ML engineer). But both are buried! And the section that gets the most attention is wasted on undergraduate awards.

After

Note that we didn’t really rewrite anything. We just moved stuff around. You can do this yourself without needing a professional writer.

## For everyone else, stop working on your resume, and start doing outreach.

If you’re like most people, you don’t have top brands on their resume, so no amount of rewriting is going to move the needle much. Instead of agonizing over it, stop applying and start doing outreach to hiring managers. It’s your best shot to get noticed and to get someone to look at you as a human being, instead of a collection of brands. [Here’s how to do it](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2). For a deeper dive into both resume writing and how to get in the door, you can read [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) (both [chapters are also available for free](https://bctci.co/free-chapters)).

1. Will side projects help you get a job? Good question and one that should be teased apart a bit. Getting a job has two components: getting in the door and doing well in interviews. In general, side projects are useless for getting in the door. Yes, every once in a while, a side project goes viral. Or if you build something really cool with your target company’s API, it can get some attention. But that’s pretty rare. Most side projects that adorn resumes go completely unnoticed. When it comes to performing well in interviews, it depends. If the companies you’re interviewing at test you on practical skills, then they can be a great use of time. They can also be a great use of time to help you understand how APIs work, how the internet works, how clients and servers talk to each other, and so on. But if the companies you’re targeting primarily ask algorithmic questions, then side projects probably aren’t the best use of time. Finally, will side projects make you a better engineer? Absolutely. And that’s the best reason to do them. But that’s not quite the same as getting a job, is it? Once you're actively looking for a job, your time is better spent on interview prep and outreach. [↩](#user-content-fnref-1)


# [We built voice modulation to mask gender in technical interviews. Here’s what happened.](https://interviewing.io/blog/voice-modulation-gender-technical-interviews)

By Aline Lerner | Published: June 28, 2016; Last updated: May 1, 2024

[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously and, in the process, find jobs based on their interview performance rather than their resumes. Since we started, we’ve amassed data from thousands of technical interviews, and in this blog, we routinely share some of the surprising stuff we’ve learned. In this post, I’ll talk about what happened when we built real-time voice masking to investigate the magnitude of bias against women in technical interviews. **In short, we made men sound like women and women sound like men and looked at how that affected their interview performance. We also looked at what happened when women did poorly in interviews, how drastically that differed from men’s behavior, and why that difference matters for the thorny issue of the gender gap in tech.**

When an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, Twitch, and Yelp, as well as engineering-focused startups like Asana, Mattermark, and others. For more context, some examples of interviews done on the platform can be found on our [mock interview](https://interviewing.io/mocks) page.

After every interview, interviewers rate interviewees on a few different dimensions.

![Feedback form for interviewers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_survey_86e6399c78.png&w=1200&q=75 "Interviewer feedback form")

As you can see, we ask the interviewer if they would advance their interviewee to the next round. We also ask about a few different aspects of interview performance using a 1-4 scale. On our platform, a score of 3 or above is generally considered good.

## Women historically haven’t performed as well as men…

One of the big motivators to think about voice masking was the increasingly uncomfortable disparity in interview performance on the platform between men and women.[1](#user-content-fn-1) At that time, we had amassed over a thousand interviews with enough data to do some comparisons and were surprised to discover that women really were doing worse. Specifically, **men were getting advanced to the next round 1.4 times more often than women. Interviewee technical score wasn’t faring that well either — men on the platform had an average technical score of 3 out of 4, as compared to a 2.5 out of 4 for women**.

Despite these numbers, it was really difficult for me to believe that women were just somehow worse at computers, so when some of our customers asked us to build voice masking to see if that would make a difference in the conversion rates of female candidates, we didn’t need much convincing.

## ...so we built voice masking

Since we started working on interviewing.io, in order to achieve true interviewee anonymity, we knew that hiding gender would be something we’d have to deal with eventually but put it off for a while because it wasn’t technically trivial to build a real-time voice modulator. Some early ideas included sending female users a Bane mask.

![Cartoon of the character Bane pretending to be Batman](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F1186d_bane_72ee94351f.webp&w=640&q=75 "Early voice masking prototype")

When the Bane mask thing didn’t work out, we decided we ought to build something within the app, and if you play the videos below, you can get an idea of what voice masking on interviewing.io sounds like. In the first one, I’m talking in my normal voice.

And in the second one, I’m modulated to sound like a man.[2](#user-content-fn-2)

Armed with the ability to hide gender during technical interviews, we were eager to see what the hell was going on and get some insight into why women were consistently underperforming.

The setup for our experiment was simple. Every Tuesday evening at 7 PM Pacific, interviewing.io hosts what we call practice rounds. In these practice rounds, anyone with an account can show up, get matched with an interviewer, and go to town. And during a few of these rounds, **we decided to see what would happen to interviewees’ performance when we started messing with their perceived genders**.

In the spirit of not giving away what we were doing and potentially compromising the experiment, we told both interviewees and interviewers that we were slowly rolling out our new voice masking feature and that they could opt in or out of helping us test it out. Most people opted in, and we informed interviewees that their voice might be masked during a given round and asked them to refrain from sharing their gender with their interviewers. For interviewers, we simply told them that interviewee voices might sound a bit processed.

We ended up with 234 total interviews (roughly 2/3 male and 1/3 female interviewees), which fell into one of three categories:

- Completely unmodulated (useful as a baseline)
- Modulated without pitch change
- Modulated with pitch change

You might ask why we included the second condition, i.e. modulated interviews that didn’t change the interviewee’s pitch. As you probably noticed, if you played the videos above, the modulated one sounds fairly processed. The last thing we wanted was for interviewers to assume that any processed-sounding interviewee must summarily have been the opposite gender of what they sounded like. So we threw that condition in as a further control.

After running the experiment, we ended up with some rather surprising results. **Contrary to what we expected** (and probably contrary to what you expected as well!), **masking gender had no effect on interview performance** with respect to any of the scoring criteria (would advance to next round, technical ability, problem solving ability). If anything, we started to notice some trends in the opposite direction of what we expected: for technical ability, it appeared that men who were modulated to sound like women did a bit better than unmodulated men and that women who were modulated to sound like men did a bit worse than unmodulated women. Though these trends weren’t statistically significant, I am mentioning them because they were unexpected and definitely something to watch for as we collect more data.

On the subject of sample size, we have no delusions that this is the be-all and end-all of pronouncements on the subject of gender and interview performance. We’ll continue to monitor the data as we collect more of it, and it’s very possible that as we do, everything we’ve found will be overturned. I will say, though, that had there been any staggering gender bias on the platform, with a few hundred data points, we would have gotten some kind of result. So that, at least, was encouraging.

## So if there’s no systemic bias, why are women performing worse?

After the experiment was over, I was left scratching my head. If the issue wasn’t interviewer bias, what could it be? I went back and looked at the seniority levels of men vs. women on the platform as well as the kind of work they were doing in their current jobs, and neither of those factors seemed to differ significantly between groups. But there was one nagging thing in the back of my mind. I spend a lot of my time poring over interview data, and I had noticed something peculiar when observing the behavior of female interviewees. Anecdotally, it seemed like women were leaving the platform a lot more often than men. So I ran the numbers.

What I learned was pretty shocking. **As it happens, women leave interviewing.io roughly 7 times as often as men after they do badly in an interview.** And the numbers for two bad interviews aren’t much better. You can see the breakdown of attrition by gender below (the differences between men and women are indeed statistically significant with P < 0.00001).

Also note that as much as possible, I corrected for people leaving the platform because they found a job (practicing interviewing isn’t that fun after all, so you’re probably only going to do it if you’re still looking), were just trying out the platform out of curiosity, or they didn’t like something else about their interviewing.io experience.

## A totally speculative thought experiment

So, if these are the kinds of behaviors that happen in the interviewing.io microcosm, how much is applicable to the broader world of software engineering? Please bear with me as I wax hypothetical and try to extrapolate what we’ve seen here to our industry at large. And also, please know that what follows is very speculative, based on not that much data, and could be totally wrong… but you gotta start somewhere.

If you consider the attrition data points above, you might want to do what any reasonable person would do in the face of an existential or moral quandary, i.e. fit the data to a curve. An exponential decay curve seemed reasonable for attrition behavior, and you can see what I came up with below. The x-axis is the number of what I like to call “attrition events”, namely things that might happen to you over the course of your computer science studies and subsequent career that might make you want to quit. The y-axis is what portion of people are left after each attrition event. The red curve denotes women, and the blue curve denotes men.

![Chart showing the gender gap](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fe92a1_gender_gap_v2_d243a94167.webp%3Fupdated_at%3D2022-12-08T14%3A31%3A14.044Z&w=1920&q=75 "Thar be gender gap")

See interactive graph with Demos: <https://www.desmos.com/calculator/tugmyjkaj6>

Now, as I said, this is pretty speculative, but it really got me thinking about what these curves might mean in the broader context of women in computer science. How many “attrition events” does one encounter between primary and secondary education and entering a collegiate program in CS and then starting to embark on a career? So, I don’t know, let’s say there are 8 of these events between getting into programming and looking around for a job. If that’s true, then we need 3 times as many women studying computer science than men to get to the same number in our pipelines. Note that that’s 3 times more than men, not 3 times more than there are now. If we think about how many there are now, which, depending on your source, is between 1/3 and a 1/4 of the number of men, **to get to pipeline parity, we actually have to increase the number of women studying computer science by an entire order of magnitude**.

## Prior art, or why maybe this isn’t so nuts after all

Since gathering these findings and starting to talk about them a bit in the community, I began to realize that there was some supremely interesting academic work being done on gender differences around self-perception, confidence, and performance. Some of the work below found slightly different trends than we did, but it’s clear that anyone attempting to answer the question of the gender gap in tech would be remiss in not considering the effects of confidence and self-perception in addition to the more salient matter of bias.

In a [study investigating the effects of perceived performance to likelihood of subsequent engagement](https://s3.wp.wsu.edu/uploads/sites/252/2014/10/EhrlingerDunning2003.pdf), Dunning (of Dunning-Kruger fame) and Ehrlinger administered a scientific reasoning test to male and female undergrads and then asked them how they did. Not surprisingly, though there was no difference in performance between genders, women underrated their own performance more often than men. Afterwards, participants were asked whether they’d like to enter a Science Jeopardy contest on campus in which they could win cash prizes. Again, women were significantly less likely to participate, with participation likelihood being directly correlated with self-perception rather than actual performance.[3](#user-content-fn-3)

In a different study, [sociologists followed a number of male and female STEM students over the course of their college careers](https://www.researchgate.net/publication/232564809_The_Validity_and_Utility_of_Selection_Methods_in_Personnel_Psychology) via diary entries authored by the students. One prevailing trend that emerged immediately was the difference between how men and women handled the “discovery of their [place in the] pecking order of talent, an initiation that is typical of socialization across the professions.” For women, realizing that they may no longer be at the top of the class and that there were others who were performing better, “the experience [triggered] a more fundamental doubt about their abilities to master the technical constructs of engineering expertise [than men].”

And of course, what survey of gender difference research would be complete without an allusion to the wretched annals of dating? When I told the interviewing.io team about the disparity in attrition between genders, the resounding response was along the lines of, “Well, yeah. Just think about dating from a man’s perspective.” Indeed, [a study published in the *Archives of Sexual Behavior*](https://link.springer.com/article/10.1023/B:ASEB.0000028892.63150.be) confirms that men treat rejection in dating very differently than women, even going so far as to say that men “reported they would experience a more positive than negative affective response after… being sexually rejected.”

Maybe tying coding to sex is a bit tenuous, but, as they say, programming is like sex — one mistake and you have to support it for the rest of your life.

## Why I’m not depressed by our results and why you shouldn’t be either

Prior art aside, I would like to leave off on a high note. I mentioned earlier that men are doing a lot better on the platform than women, but here’s the startling thing. **Once you factor out interview data from both men and women who quit after one or two bad interviews, the disparity goes away entirely.** So while the attrition numbers aren’t great, I’m massively encouraged by the fact that at least in these findings, it’s not about systemic bias against women or women being bad at computers or whatever. Rather, it’s about women being bad at dusting themselves off after failing, which, despite everything, is probably a lot easier to fix.

1. Roughly 15% of our users are female. We want way more, but it’s a start. [↩](#user-content-fnref-1)
2. If you want to hear more examples of voice modulation or are just generously down to indulge me in some shameless bragging, we got to demo it on [NPR](https://www.npr.org/2016/04/12/473912220/blind-hiring-while-well-meaning-may-create-unintended-consequences) and in [Fast Company](https://www.fastcompany.com/3059522/this-interviewing-platform-changes-your-voice-to-eliminate-unconscious-bias). [↩](#user-content-fnref-2)
3. In addition to asking interviewers how interviewees did, [we also ask interviewees to rate themselves](https://strapi-iio.s3.us-west-2.amazonaws.com/8ddad_interviewee_feedback_be549dadfb.png). After reading the Dunning and Ehrlinger study, we went back and checked to see what role self-perception played in attrition. In our case, the answer is, I’m afraid, TBD, as we’re going to need more self-ratings to say anything conclusive. [↩](#user-content-fnref-3)


# [Can fake names create bias? An exploration into interviewing.io’s pseudonym generator](https://interviewing.io/blog/interview-bias-pseudonyms)

By Atomic Artichoke | Published: March 6, 2019; Last updated: May 1, 2023

Hello everyone, my name is Atomic Artichoke, and I’m the newest employee of the interviewing.io team, having joined a couple months ago as a Data Scientist.

Atomic Artichoke isn’t my real name, of course. That’s the pseudonym the interviewing.io platform gave me, right before I took my final interview with the company. If you’ve never used interviewing.io before (and hey, if you haven’t already, [why not sign up now?](https://interviewing.io/signup)), it’s a platform where you can practice technical interviewing anonymously with experienced engineers (and do real job interviews anonymously too).

![Screenshot showing how an Interviewing.io user can pick their anonymous pseudonym](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_handle_3d625b5bb7.png&w=1920&q=75 "Choosing a pseudonym")

When it’s time to interview, you and your partner meet in a collaborative coding environment with voice, text chat, and a whiteboard (check out [recordings of real interviews](https://interviewing.io/mocks) to see this process in action). During interviews, instead of your name, your partner will see your pseudonym, like so:

![Screenshot showing how a user's anonymous pseudonym appears during a practice interview](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpseudonym_code_2ee6deedd1.png&w=1080&q=75 "Pseudonyms are always used unless an interviewer and interviewee have chosen to de-mask and reveal their names to each other")

In my opinion, “Atomic Artichoke” is a pretty cool name. It sounds like a Teenage Mutant Ninja Turtles villain, and alliterative phrases are always cool. However, I had some reservations about that handle, because I feel like the pseudonym represented me in ways with which I didn’t identify. I don’t know how to eat or cook an artichoke, I never really understood atoms much, and I possess no mutant superpowers.

But I wondered, how did the interviewer perceive me? Did this person think “Atomic Artichoke” was a cool name? If so, did that name influence his or her perception of me in any way? More importantly, **did my pseudonym have any influence in me getting hired?** If I had a different, less cool name, would I have gotten this job?

I know, it’s a silly question. I’d like to think I was hired because of my skills, but who really knows? I was curious, so I ~~wasted~~ invested a few days to investigate.

## What we already know about names in the hiring process

You might be asking, “Why does interviewing.io have pseudonyms, anyway?” Anonymity. We want candidates to be assessed on their actual skills, not on proxies of skill like the colleges they’ve attended, the notoriety of their social circles, or prior companies they’ve worked at. If a hiring manager knows a person’s name and knows how to use the Internet, it’s easy to find this information.

I’m not the first to wonder about names and hiring. Plenty of academic literature exists exploring the impact of name choice on various life outcomes. I’ll briefly touch on a handful of those perspectives.

- A 1948 paper concluded people with unique names tended to have [lower academic performance than those with more common names](https://www.tandfonline.com/doi/abs/10.1080/00224545.1948.9918930?journalCode=vsoc20).
- A 2003 study observing the relationship between “black” names and life outcomes concluded that after for controlling for other factors, [name choice did not affect life outcomes](https://www2.nber.org/papers/w9938.pdf?new_window=1).
- Finally, a 2004 paper specifically focused on the jobs market suggested that resumes containing [“black” names received fewer callbacks than those with “white” names](https://www.aeaweb.org/articles?id=10.1257/0002828042002561), even after controlling for resume quality.

As you can see, academic opinions differ. However, in the case that name-based bias actually exists, maybe we can implement a cheap-enough solution to eliminate the bias completely. Randomly-generated pseudonyms fits that bill nicely.

But as I wondered before, maybe the pseudonym name generator creates a different kind of bias, leaving us in a similarly biased place that using real people’s names leaves us. I first needed to understand how pseudonyms get generated, so I dug into some code.

## Exploring code

After dusting off what little Javascript knowledge I acquired 6 years ago, I found the 13 lines of code that generates pseudonyms. Mechanics-wise it’s simple: there are two lists, one containing adjectives and one containing nouns. The pseudonym generator randomly pulls one adjective and one noun from each list, and mashes them together in that order, with a space in between. The generator outputs some sweet sounding pseudonyms like:

- Serpentine Gyroscope
- Moldy Parallelogram
- Frumious Slide Rule
- Supersonic Llama

But they can also come up with less memorable, more commonplace, and more boring phrases like:

- Ice Snow
- Warm Wind
- Red Egg[1](#user-content-fn-1)
- Infinite Avalanche

After running through a few example pseudonyms, anecdotally I felt the first list was more attractive to me than the second. It sparked more joy in me, one could say. I just couldn’t articulate why.

That’s when I noticed that certain themes kept recurring. For example, there were multiple Alice in Wonderland references, a bunch of animals, and many types of foods listed. At first glance the chosen words seemed odd. But after getting to know my co-workers better, the list of words began to make a lot more sense.

The co-worker sitting across from me is a huge Alice in Wonderland fan. Our founders seem to love animals, since they bring their dogs to work most days. Finally, food and restaurant discussions fuel most lunchtime arguments. Just in my first month, I had heard more discussion about chicken mole and Olive Garden than I ever had in my life.

While it’s true the pseudonym generator chooses words randomly, the choice of which words get onto the list isn’t necessarily random. If anything, the choice of words reflects the interests of the people who built the application. Might it be possible that the first list appealed to me because they reference math concepts, and I happen to like math-y things?

This insight helped me craft my hypothesis more concretely: all else equal, do some candidates receive better ratings on interviews, **because interviewers happen to associate positively with users whose pseudonyms reference the interviewers’ personal interests?**

This hypothesis rests upon the assumption that people are drawn to stuff that’s similar to themselves. This seems intuitive: when individuals share common interests or backgrounds with others, chances are they’ll like each other. Therefore, is it possible that interviewers like certain candidates more because they find commonality with them, even though we manufactured that commonality? And did that likability translate to better interview ratings?

To test this, I categorized users into one of the following 6 categories based on the noun part of their pseudonym, which will be called Noun Category going forward.

- Animal
- Fantasy
- Food
- History
- Object
- Science

These broad categories aimed to differentiate among interest areas that might appeal differently to different interviewers. Among these 6 groups, I wanted to observe differences in interview performance. And knowing the pseudonym generator assigns names randomly, we would not expect to find a difference.

To proxy for interview performance, I used the “Would You Hire” response from the interviewer on the interviewee, which is the first item on the interviewer’s post-interview questionnaire.

![Screenshot of the interview feedback form for an anonymous company round](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffake_names_bias_f8cd1ac082.png&w=1080&q=75 "The interview feedback form for anonymous company rounds asks if the user wants to share their info")

These two pieces of data led to a clear, testable null hypothesis: **there should exist no relationship between Noun Category and the Would You Hire response.** If we reject this null hypothesis, we would have evidence suggesting our pseudonyms can impact hiring decisions.

## Data analysis and interpretation

I pulled data on a sample of a few thousand interviewing.io candidates’ first interview on our platform, and performed a Chi-Squared test against the observed frequencies of the 6 “Noun Categories” and 2 “Would You Hire” interviewer responses. Each cell of the 6 x 2 matrix contained at least 40 observations.

Below are the mean percentage of candidates who received a Yes from their interviewer, broken out by Noun Category. While most of the categories seemed to clump around a similar pass rate, the History group seemed to under-perform while the Fantasy group over-performed.

![Chart showing Would You Hire Pass % vs Noun Category](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fd1730_would_you_hire_pass_vs_noun_category3_c020791d7b.webp&w=1200&q=75 "Would You Hire Pass % vs Noun Category")

The Chi-Square test rejected the null hypothesis at a 5% significance level.

These results suggest **a relationship might exists between Noun Category and an interviewer’s Would You Hire response**. Which again, should *not* occur because a candidate’s Noun Category was randomly assigned![2](#user-content-fn-2)

## What next?

While this analysis doesn’t predict outcomes for specific individuals, the result suggests it isn’t totally crazy to believe I may gotten lucky on my interview. Maybe I don’t suffer from imposter syndrome, maybe I am an imposter. How depressing.

So what now? Fortunately (or unfortunately) for my new company, if we want to eliminate this bias, I can suggest potential next steps.

One solution might be to pander to an interviewer’s interests. We could randomly generate a new pseudonym for candidates every time they meet a different interviewer, ensuring that pseudonym creates positive associations with the interviewer. Similarly, we could generate more pseudonyms referencing Lord of the Rings and Warcraft, if we know our interviewer pool tends to be fantasy-inclined.

An alternative solution might be to give candidates pseudonyms with no meaning at all. For example, we could generate random strings, similar to what password managers generate for you. This would eliminate any real world associations, but we’d lose some whimsy and human readability that the current pseudonyms provide.

Yet another alternative solution could be to do more analysis before acting. The analysis didn’t quantify the magnitude of the bias, so we could construct a new sample to test a more specific hypothesis about bias size. It’s possible the practical impact of the bias isn’t huge, and we should focus our energy elsewhere.

## Zooming out

On the face of it, this pseudonym bias seems trivial, and in the universe of all biases that could exist, that’s probably true. However, it makes me wonder how many other hidden biases might exist elsewhere in life.

I think that’s why I was hired. I’m obsessed with bias. Though I’ll be doing normal business-y Data Scientist stuff, my more interesting responsibilities will be poking at all aspects of the hiring market and examining the myriad of factors, mechanisms, and individuals that make the hiring market function, and perhaps not function effectively for some people.

Going a step further than identifying hiring biases, I’d like to shift discussions toward action. It’s great that the tech industry talks about diversity more, but I think we can facilitate more discussions around which concrete actions are being taken, and whether those actions actually achieve our goals, whatever those goals may be.

I think it all starts with being introspective about ourselves, and investigating whether something as innocuous as a randomly generated phrase could ever matter.

Atomic Artichoke
(Ken Pascual)

![Atomic Artichoke's personalized Interviewing.io sticker](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F28d03_image_from_ios_3_e1235e7732.webp%3Fupdated_at%3D2022-12-08T13%3A55%3A14.749Z&w=640&q=75 "Atomic Aritichoke")

1. This is the shortest pseudonym possible on interviewing.io [↩](#user-content-fnref-1)
2. This is not entirely true. Users can re-generate a random pseudonym as often as they want, meaning a user can choose their name if they re-generate a lot. However, there's no evidence this happens often, because we found no significant difference in the observed and theoretical randomized distribution of Noun Categories. [↩](#user-content-fnref-2)


# [Technical interview performance is kind of arbitrary. Here’s the data.](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data)

By Aline Lerner | Published: February 16, 2016; Last updated: March 28, 2024

*Note: Though I wrote most of the words in this post, there are a few people outside of interviewing.io whose work made it possible. **[Ian Johnson](https://twitter.com/enjalot)**, creator of [d3 Building Blocks](http://blockbuilder.org/), created the graph entitled Standard Dev vs. Mean of Interviewee Performance (the one with the icons) as well as all the interactive visualizations that go with it. **[Dave Holtz](https://twitter.com/daveholtz)** did all the stats work for computing the probability of people failing individual interviews. You can see more about his work on [his blog](https://daveholtz.net/).*

[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously and, in the process, find jobs. In the past few months, we’ve amassed data from hundreds of interviews, and **when we looked at how the same people performed from interview to interview, we were really surprised to find quite a bit of volatility, which, in turn, made us question the reliability of single interview outcomes.**

When an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice[1](#user-content-fn-1), text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role, and interviewers typically come from a mix of large companies like Google, Facebook, and Yelp, as well as engineering-focused startups like Asana, Mattermark, KeepSafe, and more. If you’d like to see an interview an action, head over to our [public recordings](https://interviewing.io/mocks) page for a few examples.

After every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “meh” and 4 is “amazing!” ([you can see the feedback form here](https://strapi-iio.s3.us-west-2.amazonaws.com/49613_interviewer_feedback_64893c98f9.png)). On our platform, a score of 3 or above has generally meant that the person was good enough to move forward.

## Performance from interview to interview is pretty volatile

Let’s start with some visuals. In the graph below, every tiny human icon represents the mean technical score for an individual interviewee who has done 2 or more interviews on the platform.[2](#user-content-fn-2) The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes. If you hover over each , you can drill down and see how that person did in each of their interviews. Anytime you see bolded text with a dotted underline, you can hover over it to see relevant data viz. Try it now to expand everyone’s performance. You can also hover over the labels along the x-axis to drill into the performance of people whose means fall into those buckets.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_22_at_11_45_36_2x_9d9ce0d21d.png)

As you can see, roughly 25% of interviewees are consistent in their performance, and the rest are all over the place.[3](#user-content-fn-3) If you look at the graph above, despite the noise, you can probably make some guesses about which people you’d want to interview. **However, keep in mind that each represents a *mean*. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey.** For instance:

- Many people who scored at least one 4 also scored at least one 2.
- If we look at high performers (mean of 3.3 or higher), we still see a fair amount of variation.
- Things get really murky when we consider “average” performers (mean between 2.6 and 3.3).

**To me, looking at this data and then pretending that I had to make a hiring decision based on one interview outcome felt a lot like peering into some beautiful, lavishly appointed parlor through a keyhole.** Sometimes you see a piece of art on the wall, sometimes you see the liquor selection, and sometimes you just see the back of the couch.

At this point you might say that it’s erroneous and naive to compare raw technical scores to one another for any number of reasons, not the least of which is that one interviewer’s 4 is another interviewer’s 2. We definitely share this concern and address it in the appendix of this post. It does bear mentioning, though, that most of our interviewers are coming from companies with strong engineering brands and that correcting for brand strength didn’t change interviewee performance volatility, nor did correcting for interviewer rating.

So, in a real life situation, when you’re trying to decide whether to advance someone to onsite, you’re probably trying to avoid two things — false positives (bringing in people below your bar by mistake) and false negatives (rejecting people who should have made it in). Most top companies’ interviewing paradigm is that false negatives are less bad than false positives. This makes sense right? With a big enough pipeline and enough resources, even with a high false negative rate, you’ll still get the people you want. With a high false positive rate, you might get cheaper hiring, but you do potentially irreversible damage to your product, culture, and future hiring standards in the process. And of course, the companies setting the hiring standards and practices for an entire industry ARE the ones with the big pipelines and seemingly inexhaustible resources.

The dark side of optimizing for high false negative rates, though, rears its head in the form of our current engineering hiring crisis. Do single interview instances, in their current incarnation, give enough signal? Or amidst so much demand for talent, are we turning away qualified people because we’re all looking at a large, volatile graph through a tiny keyhole?

So, hyperbolic moralizing aside, **given how volatile interview performance is, what are the odds that a good candidate will fail an individual phone screen?**

## Odds of failing a single interview based on past performance

Below, you can see the distribution of mean performance throughout our population of interviewees.

In order to figure out the probability that a candidate with a given mean score would fail an interview, we had to do some stats work. First, we broke interviewees up into cohorts based on their mean scores (rounded to the nearest 0.25). Then, for each cohort, we calculated the probability of failing, i.e. of getting a score of 2 or less. Finally, to work around our starting data set not being huge, we [resampled](https://www.burns-stat.com/documents/tutorials/the-statistical-bootstrap-and-other-resampling-methods-2/) our data. In our resampling procedure, we treated an interview outcome as a multinomial distribution, or in other words, pretended that each interview was a roll of a weighted, 4-sided die corresponding to that candidate’s cohort. We then re-rolled the dice a bunch of times to create a new, “simulated” dataset for each cohort and calculated new probabilities of failure for each cohort using these data sets. Below, you can see the results of repeating this process 10,000 times.

As you can see, a lot of the distributions above overlap with one another. This is important because these overlaps tell us that there may not be statistically significant differences between those groups (e.g. between 2.75 and 3). Certainly, with the advent of LOT more data, the delineations between cohorts may become clearer. On the other hand, if we do need a huge amount of data to detect differences in failure rate, it might suggest that people *are* intrinsically highly variable in their performance. **At the end of the day, while we can confidently say that there is a significant difference between the bottom end of the spectrum (2.25) versus the top end (3.75), for people in the middle, things are murky.**

Nevertheless, using these distributions, we did attempt to compute the probability that a candidate with a certain mean score would fail a single interview (see below — the shaded areas encapsulate a 95% confidence interval). The fact that people who are overall pretty strong (e.g. mean ~= 3) can mess up technical interviews as much as 22% of the time shows that there’s definitely room for improvement in the process, and this is further exacerbated by the general murkiness in the middle of the spectrum.

Generally, when we think of interviewing, we think of something that ought to have repeatable results and carry a strong signal. However, the data we’ve collected, meager though it might be, tells a different story. And it resonates with both my anecdotal experience as a recruiter and with the sentiments we’ve seen echoed in the community. Zach Holman’s [Startup Interviewing is Fucked](https://zachholman.com/posts/startup-interviewing-is-fucked/) hits on the disconnect between interview process and the job it’s meant to fill, the fine gentlemen of TripleByte [reached similar conclusions](https://triplebyte.com/blog/who-y-combinator-companies-want) by looking at their own data, and one of the more poignant expressions of inconsistent interviewing results recently came from [rejected.us](https://rejected.us/).

You can bet that many people who are rejected after a phone screen by Company A but do better during a different phone screen and ultimately end up somewhere traditionally reputable are getting hit up by Company A’s recruiters 6 months later. And despite everyone’s best efforts, the murky, volatile, and ultimately stochastic circle jerk of a recruitment process marches on.

So yes, it’s certainly one possible conclusion is that technical interviewing itself is indeed fucked and doesn’t provide a reliable, deterministic signal for one interview instance. Algorithmic interviews are a hotly debated topic and one we’re deeply interested in teasing apart. One thing in particular we’re very excited about is tracking interview performance as a function of interview type, as we get more and more different interviewing types/approaches happening on the platform. Indeed, one of our long-term goals is to really dig into our data, look at the landscape of different interview styles, and make some serious data-driven statements about what types of technical interviews lead to the highest signal.

**In the meantime, however, I am leaning toward the idea that drawing on aggregate performance is much more meaningful than a making such an important decision based on one single, arbitrary interview.** Not only can aggregative performance help correct for an uncharacteristically poor performance, but it can also weed out people who eventually do well in an interview by chance or those who, over time, submit to the beast and memorize *Cracking the Coding Interview*. I know it’s not always practical or possible to gather aggregate performance data in the wild, but at the very least, in cases where a candidate’s performance is borderline or where their performance differs wildly from what you’d expect, it might make sense to interview them one more time, perhaps focusing on slightly different material, before making the final decision.

## Appendix: The part where we tentatively justify using raw scores for comparative performance analysis

For the skeptical, inquiring minds among you who realize that using raw coding scores to evaluate an interviewee has some pretty obvious problems, we’ve included this section. The issue is that even though our interviewers tend to come from companies with high engineering bars, raw scores are still comprised of just one piece of feedback, they don’t adjust for interviewer strictness (e.g. one interviewer’s 4 could be another interviewer’s 2), and they don’t adjust well to changes in skill over time. Internally, we actually use a more complex and comprehensive rating system when determining skill, and if we can show that raw scores align with the ratings we calculate, then we don’t feel so bad about using raw scores comparatively.

Our rating system works something like this:

1. We create a single score for each interview based on a weighted average of each feedback item.
2. For each interviewer, we pit all the interviewees they’ve interviewed against one another using this score.
3. We use a Bayesian ranking system (a modified version of [Glicko-2](https://www.npmjs.com/package/glicko2)) to generate a rating for each interviewee based on the outcome of these competitions.

As a result, each person is only rated based on their score as it compares to other people who were interviewed by the same interviewer. That means one interviewer’s score is never directly compared to another’s, and so we can correct for the hairy issue of inconsistent interviewer strictness.

So, why am I bringing this up at all? You’re all smart people, and you can tell when someone is waving their hands around and pretending to do math. Before we did all this analysis, we wanted to make sure that we believed our own data. We’ve done a lot of work to build a ratings system we believe in, so we correlated that with raw coding scores to see how strong they are at determining actual skill.

These results are pretty strong. Not strong enough for us to rely on raw scores exclusively but strong enough to believe that raw scores are useful for determining approximate candidate strength.

*Thanks to Andrew Marsh for co-authoring the appendix, to [Plotly](https://plotly.com/) for making a terrific graphing product, and to everyone who read drafts of this behemoth.*

1. While listening to interviews day in and day out, I came up with a drinking game. Every time someone thinks the answer is hash table, take a drink. And every time the answer actually is hash table, take two drinks.[4](#user-content-fn-4) [↩](#user-content-fnref-1)
2. This is data as of January 2016, and there are only 299 interviews because not all interviews have enough feedback data and because we threw out everyone with less than 2 interviews. Moreover, one thing we don’t show in this graph is the passage of time, so you can [see people’s performance over time](https://chart-studio.plotly.com/~aline_interviewingio/858/interviewee-performance-over-time-299-interviews-w-67-interviewees/?share_key=JFfMkYzESYN6gMH2E7vlJC) — it’s kind of a hot mess. [↩](#user-content-fnref-2)
3. We were curious to see if volatility varied at all with people’s mean scores. In other words, were weaker players more volatile than strong ones? The answer is no — when we ran a regression on standard deviation vs. mean, we couldn’t come up with any meaningful relationship (R-squared ~= 0.03), which means that people are all over the place regardless of how strong they are on average. [↩](#user-content-fnref-3)
4. I almost died. [↩](#user-content-fnref-4)

### [Odd Even Linked List](/questions/odd-even-linked-list)

[Given the head of a singly linked list, group all the nodes with odd indices together followed by the nodes with even indices, and return the reordered list.](/questions/odd-even-linked-list)


# [We have the best technical interviewers on the market. Here's how we do it.](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it)

By Aline Lerner | Published: February 27, 2023; Last updated: August 26, 2023

We make money in two ways: engineers pay us for mock interviews, and employers pay us for access to the best performers. This means that we live and die by the quality of our interviewers in a way that no single employer does, no matter how much they say they care about people analytics or interviewer metrics or training. If we don’t have really well-calibrated interviewers, who also create great candidate experience, we don’t get paid.

In a recent post, we shared how, over time, we came up with two metrics that, together, tell a complete and compelling story about interviewer quality[: the candidate experience metric and the calibration metric](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too). In this post, we’ll talk about how to apply our learnings about interviewer quality to your own process. We’ve made a bunch of mistakes so you don’t have to!

But first, a pessimistic word. The more time I spend in this industry, the more I’m convinced that many top companies end up hiring great engineers not because of their process but despite it. The reality is that top companies are generally not incentivized to care about candidate experience or metrics. As long as you have a revolving door of candidates, both can be pretty meh, and if you’re just a little bit better on calibration than a coin flip and not bad enough to outright scare people away, you’ll be fine.

In the rare instances where I’ve seen companies really care about interviewer quality, it’s because an eng leader there has taken it upon themselves, as a labor of love. Otherwise, if we’re pragmatic about it, hiring is a cost center, and it’s never going to get the attention that a profit center will. At the same time, some of you reading this post will care enough about interviewer quality and candidate experience to make some changes within your organization.

On the back of that fervent hope, below is a punch list of things you can do to move the needle:

1. Choose the people who are passionate about interviewing to conduct your interviews, and don’t force the others
2. Track metrics
3. Make being a good interviewer part of your culture, and reward it explicitly (i.e., create the right incentive structure)
4. Make a point of delivering constructive feedback after each interview

## Choose the people who are passionate about interviewing to conduct your interviews, and don’t force the others

Look, the reality is that conducting interviews is a polarizing task — people either are *passionate* (so passionate that I put it in italics) about it, or they despise it. Speculating on what perfect storm of past experiences, heredity, and political leanings will make an interviewer fall into one camp or the other is outside the scope of this post. However, after personally interviewing hundreds of interviewers, I am 100% convinced that conducting interviews is polarizing.

What does this mean for you? Choose the people who are *passionate* about interviewing to conduct your interviews. It’s easy to figure out who they are. Just ask them. Over the years, I’ve listened to a lot of interview replays. You can immediately identify when an interviewer is checked out. It’s painful. You’ll hear them typing. You’ll hear them go silent for a while. You certainly won’t hear them collaborating with their candidate or gently guiding them away from a perilous rabbit hole. Compare that to a good experience [like this one](https://www.youtube.com/watch?v=EhzF81xV1so) [1](#user-content-fn-1). We’ve all been on the receiving end of an interviewer’s callous indifference, but it’s absolutely preventable. *If someone hates conducting interviews, don’t make them.*

In our experience, a terrible question in the hands of a skilled, engaged interviewer can become great & carry lots of signal. A great question asked by an unskilled, disconnected interviewer will always be bad… like so:
![](https://strapi-iio.s3.us-west-2.amazonaws.com/resize_b152080cc8.png)

If you want to be deliberate about choosing your best interviewers, follow the [best practices we've found after sifting through thousands of interview recordings](https://interviewing.io/blog/best-technical-interviews-common). The best interviewers see every interview as a collaborative exercise with the goal of seeing if they can “be smart together". The data shows that engagement really does matter.

## Track metrics

First, please take a look at our [post about the 2 metrics we track](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too) – the candidate experience score and the calibration score – if you haven’t already. Why track metrics in the first place? There’s an old adage that says you can’t fix what you can’t measure. I think this expression is overused — sometimes intuition is good enough.

That doesn’t work for interviews though — most of the time, an interview is a private interaction between two people, so an independent observer can’t acquire the anecdotal intuition to form opinions. Things are regularly happening, but unless you measure them, they’re likely happening in a vacuum. Yes, you might do some reverse shadows when training a new interviewer, but once they’re on their own, things tend to go sideways.

The second reason intuition isn’t good enough here is that you’re often not privy to interview outcomes, either because there’s a latency to them or because the candidate isn’t interviewing for your team… or because if you reject someone at the phone screen stage, you don’t get to find out if they were a false negative.

Unfortunately, as I mentioned earlier, no single company can reproduce the kind of data we have, either about candidate experience or about outcomes. For candidate experience, candidates don’t usually give post-interview feedback in the wild, and when they do, because the interviews aren’t anonymous and the candidate wants to work there, they’re not going to say bad things. And for outcomes, though you’ll know if someone passed an onsite, it’s just one data point for that person (e.g., you don’t know how they did in their interviews at other companies, and you typically won’t know what ended up happening to people you rejected).

Here’s what you can do despite these limitations:

1. Send candidates (truly) anonymous surveys AFTER the interview process is over. Ask about question quality, interviewer engagement, whether they’d want to be on a team with their interviewer, and so on.
2. Make a point of checking the LinkedIn of candidates you reject to see where they ended up. This will give you a decent proxy for what portion of your candidates you wrongfully rejected.
3. Determine how well calibrated your interviewers are by identifying [superforecasters](https://interviewing.io/blog/technical-phone-screen-superforecasters), i.e., track the onsite pass rate for all the candidates interviewed by a given interviewer.

Once you do these three things (or even if you do the first one and just one of the other two), you’ll be able to create a “candidate experience score” and an “accuracy score” for each interviewer. Track these scores over time, and use them to inform interview scheduling frequency and assignment.

## Create the right incentive structure

We just talked about metrics. The right incentive structure gives your metrics teeth.

Every month, we run an onboarding session for all of our new interviewers where we talk about our standards, our business model, the metrics we track, and why they matter.

During these onboarding sessions, we hear one thing over and over from new interviewers — how much better the experience of being an interviewer is when they’re paid for their time. If you read that sentence again, you might find it somewhat ironic. After all, these are people who conduct interviews day in and day out at their jobs, where they are literally being paid for their time.

Unfortunately, even though that’s technically true, engineers don’t really feel like they’re being paid to do interviews because it’s so perfunctory. There’s no reward for doing a good job and, usually, no punishment for doing a bad job — it’s just something you have to do. Worse, at most companies, conducting interviews is an unwelcome distraction from doing real work (i.e., shipping product), and the interviewers who are passionate about doing a good job do it despite their incentive structure and not because of it.

This is simple to fix. Outside of using the two aforementioned metrics to inform who gets booked and how often, you can take it a step further and actually reward being a good interviewer.

If you use OKRs (Objectives and Key Results), make staying above a certain candidate experience score one of those OKRs and achieving a certain accuracy score another one.

Alternatively (or additionally), institute a cash bonus for conducting interviews while maintaining good ratings.

When establishing criteria for promotions, ensure that no one can become a people manager without being a stellar interviewer — I don’t think I have to convince you that those skill sets overlap.

## Make a point of delivering constructive feedback after each interview

I know this is a big ask, but trust me, it’s the right thing to do.

Your legal department will probably tell you not to do it, but we did the research, and literally [zero companies (at least in the US) have ever been sued by an engineer](https://interviewing.io/blog/no-engineer-has-ever-sued-a-company-because-of-constructive-post-interview-feedback-so-why-dont-employers-do-it) who received post-interview feedback.

In cases where the candidate performed well, our data shows that [delivering feedback will increase their odds of ultimately accepting your offer](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire). The hard part, of course, is delivering feedback when the interview goes poorly. No one wants to deal with an angry, defensive candidate, even if they’re not worried about the candidate’s litigiousness.

While post-interview feedback is baked into interviewing.io, it’s not something that really happens in the wild, so we had to invent our own best practices for how to do it. Below is a slide taken verbatim from our monthly interviewer onboarding sessions. I hope that sharing it encourages a few of you to try this out. The most important takeaway from this slide is to not focus on the outcome but rather to get specific right away — this will keep your candidate from getting defensive and will set them up to actually hear and internalize your feedback.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/delivering_honest_feedback_synchronously_6d5c927a30.png)

If you do decide to give post-interview feedback, you can also check out our [detailed playbook for exactly how to do it](https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire) (it’s a much more detailed version of the slide pictured above).

We have no delusions that you’ll do most of these things. But even if you do a few, you will stand out, your candidates will remember you, and you will feel great about having helped someone. Years ago, back when I was head of recruiting at a startup, because I was a rare breed of recruiter who had also been an engineer, I also conducted technical interviews. In some ways, this setup gave me more freedom because I was the one that had to deal with candidates, end to end – when I took risks, I was only creating problems for myself.

At some point, I started giving candidates verbal feedback after their interviews, especially in cases where it was close and where the candidate would benefit from a nudge in the right direction (“Hey, make sure you get comfortable with manipulating hash tables.”) Then I took it a step further and started sending them a book.

Years later, many of those users who had failed their technical interview with me became interviewing.io’s first customers.

So, yeah, if you do any of these things, you generate a lot of good will, and accrued good will over time can make all the difference between a stellar employer brand and a mediocre one. It can also help you edge out companies who pay more or have flashier brands but don’t care about making interviewing better.

1. We also have replays of shitty interviews. That’s what made us come up with our metric, but I won’t share them to protect the guilty. [↩](#user-content-fnref-1)


# [You can’t fix diversity in tech without fixing the technical interview.](https://interviewing.io/blog/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview)

By Aline Lerner | Published: November 21, 2016; Last updated: July 14, 2023

In the last few months, several large players, including [Google](https://blog.google/outreach-initiatives/diversity/focusing-on-diversity30/) and [Facebook](https://newsroom.fb.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/), have released their latest and ultimately disappointing diversity numbers. Even with increased effort and resources poured into diversity hiring programs, Facebook’s headcount for women and people of color hasn’t really increased in the past 3 years. Google’s numbers have looked remarkably similar, and both players have yet to make significant impact in the space, despite a number of initiatives spanning everything from a [points system rewarding recruiters for bringing in candidates](https://www.wsj.com/articles/facebooks-point-system-fails-to-close-diversity-gap-1471387288) from diverse backgrounds, to increased funding for tech education, to efforts to hire more candidates from diverse backgrounds in key leadership positions.

Why have gains in diversity hiring been so lackluster across the board?

Facebook justifies these disappointing numbers by citing the ubiquitous [pipeline problem](https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring), namely that not enough people from underrepresented groups have access to the education and resources they need to be set up for success. And Google’s take appears to be similar, judging from what portion of their diversity-themed, forward-looking investments are [focused on education](https://time.com/3849218/google-diversity-investment/).

In addition to blaming the pipeline, since Facebook’s and Google’s announcements, a growing flurry of conversations have loudly waxed causal about the *real* reason diversity hiring efforts haven’t worked. These have included everything from how diversity training isn’t sticky enough, to how work environments remain exclusionary and thereby unappealing to diverse candidates, to improper calibration of performance reviews to not accounting for how marginalized groups actually respond to diversity-themed messaging.

While we are excited that more resources are being allocated to education and inclusive workplaces, at [interviewing.io](https://interviewing.io/), we posit another reason for why [diversity hiring initiatives aren’t working](https://interviewing.io/blog/diversity-hiring-initiatives-wrong). **After drawing on data from thousands of technical interviews, it’s become clear to us that technical interviewing is a process whose results are nondeterministic and often arbitrary. We believe that technical interviewing is a broken process for everyone but that the flaws within the system hit underrepresented groups the hardest… because they haven’t had the chance to internalize just how much of technical interviewing is a numbers game.** Getting a few interview invites here and there through increased diversity initiatives isn’t enough. It’s a beginning, but it’s not enough. It takes a lot of interviews to get used to the process and the format and to understand that the stuff you do in technical interviews isn’t actually the stuff you do at work every day. And it takes people in your social circle all going through the same experience, screwing up interviews here and there, and getting back on the horse to realize that poor performance in one interview isn’t predictive of whether you’ll be a good engineer.

## A brief history of technical interviewing

A definitive work on the history of technical interviewing was surprisingly hard to find, but I was able to piece together a narrative by scouring books like [How Would You Move Mount Fuji](https://www.amazon.com/How-Would-Move-Mount-Fuji/dp/0316778494), [Programming Interviews Exposed](https://www.amazon.com/Programming-Interviews-Exposed-Secrets-Landing/dp/1118261364/), and the bounty of the internets. The story goes something like this.

Technical interviewing has its roots as far back as 1950s Palo Alto, at Shockley Semiconductor Laboratories. Shockley’s interviewing methodology came out of a need to separate the innovative, rapidly moving, Cold War-fueled tech space from hiring approaches taken in more traditionally established, skills-based assembly-line based industry. And so, he relied on questions that could gauge analytical ability, intellect, and potential quickly. One canonical question in this category has to do with coins. You have 8 identical-looking coins, except one is lighter than the rest. Figure out which one it is with just two weighings on a pan balance.

The techniques that Shockley developed were adapted by Microsoft during the 90s, as the first dot-com boom spurred an explosion in tech hiring. As with the constraints imposed by both the volume and the high analytical/adaptability bar imposed by Shockley, Microsoft, too, needed to vet people quickly for potential — as software engineering became increasingly complex over the course of the dot-com boom, it was no longer possible to have a few centralized “master programmers” manage the design and then delegate away the minutiae. Even rank and file developers needed to be able to produce under a variety of rapidly evolving conditions, where just mastery of specific skills wasn’t enough.

The puzzle format, in particular, was easy to standardize because individual hiring managers didn’t have to come up with their own interview questions, and a company could quickly build up its own interchangeable question repository.

This mentality also applied to the interview process itself — rather than having individual teams run their own processes and pipelines, it made much more sense to standardize things. This way, in addition to questions, you could effectively plug and play the interviewers themselves — any interviewer within your org could be quickly trained up and assigned to speak with any candidate, independent of prospective team.

Puzzle questions were a good solution for this era for a different reason. Collaborative editing of documents didn’t become a thing until Google Docs’ launch in 2007. Without that capability, writing code in a phone interview was untenable — if you’ve ever tried to talk someone through how to code something up without at least a shared piece of paper in front of you, you know how painful it can be. In the absence of being able to write code in front of someone, the puzzle question was a decent proxy. Technology marched on, however, and its evolution made it possible to move from the proxy of puzzles to more concrete, coding-based interview questions. Around the same time, [Google itself publicly overturned the efficacy of puzzle questions](https://www.nytimes.com/2013/06/20/business/in-head-hunting-big-data-may-not-be-such-a-big-deal.html).

So where does this leave us? **Technical interviews are moving in the direction of more concreteness, but they are still very much a proxy for the day-to-day work that a software engineer actually does. The hope was that the proxy would be decent enough, but it was always understood that that’s what they were and that the cost-benefit of relying on a proxy worked out in cases where problem solving trumped specific skills and where the need for scale trumped everything else.**

As it happens, elevating problem-solving ability and the need for a scalable process are both eminently reasonable motivations. But here’s the unfortunate part: the second reason, namely the need for scalability, doesn’t apply in most cases. Very few companies are large enough to need plug and play interviewers. But coming up with interview questions and processes is really hard, so despite their differing needs, smaller companies often take their cues from the larger players, not realizing that companies like Google are successful at hiring because the work they do attracts an assembly line of smart, capable people… and that their success at hiring is often despite their hiring process and not because of it. **So you end up with a de facto interviewing cargo cult, where smaller players blindly mimic the actions of their large counterparts and blindly hope for the same results.**

**The worst part is that these results may not even be repeatable…** for anyone. To show you what I mean, I’ll talk a bit about some data we collected at interviewing.io.

## Technical interviewing is broken for everybody

### Interview outcomes are kind of arbitrary

interviewing.io is a platform where people can practice technical interviewing anonymously and, in the process, find jobs. Interviewers and interviewees meet in a collaborative coding environment and jump right into a technical interview question. After each interview, both sides rate one another, and interviewers rate interviewees on their technical ability. And the same interviewee can do multiple interviews, each of which is with a different interviewer and/or different company, and this opens the door for some interesting and somewhat controlled comparative analysis.

We were curious to see how consistent the same interviewee’s performance was from interview to interview, so we dug into our data. After looking at thousands of interviews on the platform, we’ve discovered something alarming: **[interviewee performance from interview to interview varied quite a bit, even for people with a high average performance](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary)**. In the graph below, every represents the mean technical score for an individual interviewee who has done 2 or more interviews on interviewing.io. The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes.

![Screen-Shot-2021-01-19-at-7.04.20-PM.png](https://strapi-iio.s3.us-west-2.amazonaws.com/Screen_Shot_2021_01_19_at_7_04_20_PM_f6010739c4.png)

As you can see, roughly 25% of interviewees are consistent in their performance, but the rest are all over the place. And over a third of people with a high mean (>=3) technical performance bombed at least one interview.

Despite the noise, from the graph above, you can make some guesses about which people you’d want to interview. However, keep in mind that each person above represents a mean. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey. **Looking at this data, it’s not hard to see why technical interviewing is often perceived as a game.** And, unfortunately, it’s a game where people often can’t tell how they’re doing.

### No one can tell how they’re doing

I mentioned above that on interviewing.io, we collect post-interview feedback. In addition to asking interviewers how their candidates did, we also ask interviewees how they think they did. Comparing those numbers for each interview showed us something really surprising: people are terrible at gauging their own interview performance, and impostor syndrome is particularly prevalent. In fact, **[people underestimate their performance over twice as often as they overestimate it](https://interviewing.io/blog/own-interview-performance)**. Take a look at the graph below to see what I mean:

**Note that, in our data, impostor syndrome knows no gender or pedigree — it hits engineers on our platform across the board, regardless of who they are or where they come from.**

Now here’s the messed up part. During the feedback step that happens after each interview, we ask interviewees if they’d want to work with their interviewer. As it turns out, there’s a very strong relationship between whether people think they did well and whether they would indeed want to work with the interviewer — **when people think they did poorly, even if they actually didn’t, they may be a lot less likely to want to work with you**. And, by extension, it means that in every interview cycle, some portion of interviewees are losing interest in joining your company just because they didn’t think they did well, despite the fact that they actually did.

**As a result, companies are losing candidates from all walks of life because of a fundamental flaw in the process.**

### Poor performances hit marginalized groups the hardest

Though impostor syndrome appears to hit engineers from all walks of life, we’ve found that women get hit the hardest in the face of an actually poor performance. As we learned above, **poor performances in technical interviewing happen to most people, even people who are generally very strong. However, when we looked at our data, we discovered that after a poor performance, [women are 7 times more likely to stop practicing than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews)**:

A [bevy of research appears to support confidence-based attrition as a very real cause for women departing from STEM fields](https://www.theatlantic.com/magazine/archive/2014/05/the-confidence-gap/359815/), but I would expect that the implications of the attrition we witnessed extend beyond women to underrepresented groups, across the board.

## What the real problem is

**At the end of the day, because technical interviewing is indeed a game, like all games, it takes practice to improve. However, unless you’ve been socialized to expect and be prepared for the game-like aspect of the experience, it’s not something that you can necessarily intuit.** And if you go into your interviews expecting them to be indicative of your aptitude at the job, which is, at the outset, not an unreasonable assumption, you will be crushed the first time you crash and burn. But the process isn’t a great or predictable indicator of your aptitude. And on top of that, you likely can’t tell how you’re doing even when you do well.

**These are issues that everyone who’s gone through the technical interviewing gauntlet has grappled with. But not everyone has the wherewithal or social support to realize that the process is imperfect and to stick with it. And the less people like you are involved, whether it’s because they’re not the same color as you or the same gender or because not a lot of people at your school study computer science or because you’re a dropout or for any number of other reasons, the less support or insider knowledge or 10,000 foot view of the situation you’ll have.** Full stop.

## Inclusion and education isn’t enough

To help remedy the lack of diversity in its headcount, [Facebook has committed to three actionable steps](https://newsroom.fb.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/) on varying time frames. The first step revolves around creating a more inclusive interview/work environment for existing candidates. The other two are focused on addressing the perceived pipeline problem in tech:

- Short Term: Building a Diverse Slate of Candidates and an Inclusive Working Environment
- Medium Term: Supporting Students with an Interest in Tech
- Long Term: Creating Opportunity and Access

Indeed, efforts to promote inclusiveness and increased funding for education are extremely noble, especially in the face of potentially not being able to see results for years in the case of the latter. However, both take a narrow view of the problem and both continue to funnel candidates into a broken system.

Erica Baker really cuts to the heart of it in [her blog post about Twitter hiring a head of D&I](https://medium.com/this-is-hard/wrong-acacd043229b#.cf3iajdvn):

> *“What irks me the most about this is that no company, Twitter or otherwise, should have a VP of Diversity and Inclusion. When the VP of Engineering… is thinking about hiring goals for the year, they are not going to concern themselves with the goals of the VP of Diversity and Inclusion. They are going to say ‘hiring more engineers is my job, worrying about the diversity of who I hire is the job of the VP of Diversity and Inclusion.’ When the VP of Diversity and Inclusion says ‘your org is looking a little homogenous, do something about it,’ the VP of Engineering won’t prioritize that because the VP of Engineering doesn’t report to the VP of Diversity and Inclusion, so knows there usually isn’t shit the VP of Diversity and Inclusion can do if the Eng org doesn’t see some improvement in diversity.”*

Indeed, this is sad, but true. When faced with a high-visibility conundrum like diversity hiring, a pragmatic and even reasonable reaction on any company’s part is to make a few high-profile hires and throw money at the problem. Then, it looks like you’re doing something, and spinning up a task force or a department or new set of titles is a lot easier than attempting to uproot the entire status quo.

**As such, we end up with a newly minted, well-funded department pumping a ton of resources into feeding people who’ve not yet learned about the interviewing being a game into a broken, nondeterministic machine of a process made further worse by the fact that said process favors confidence and persistence over bona fide ability… and where the link between success in navigating said process and subsequent on-the-job performance is tenuous at best.**

## How to fix things

In the evolution of the technical interview, we saw a gradual reduction in the need for proxies as companies as the technology to write code together remotely emerged; with its advent, abstract, largely arbitrary puzzle questions could start to be phased out.

What’s the next step? **Technology has the power to free us from relying on proxies**, so that we can look at each individual as an indicative, unique bundle of performance-based data points. At interviewing.io, we make it possible to move away from proxies by looking at each interviewee as a collection of data points that tell a story, rather than one arbitrary glimpse of something they did once.

But that’s not enough either. Interviews themselves need to continue to evolve. The process itself needs to be repeatable, predictive of aptitude at the actual job, and not a system to be gamed, where a huge benefit is incurred by knowing the rules. And the larger organizations whose processes act as a template for everyone else need to lead the charge. Only then can we really be welcoming to a truly diverse group of candidates.


# [Announcing our Pay Later Program: Don’t pay for mock interviews until you get a job](https://interviewing.io/blog/announcing-our-pay-later-program-dont-pay-for-mock-interviews-until-you-get-a-job)

By Aline Lerner | Published: February 8, 2022; Last updated: June 20, 2023

I started interviewing.io 6 years ago with the mission of making eng hiring efficient and fair. In my mind, as long as employers were obsessed with where people had gone to school and worked previously, instead of focusing on what people could actually do, hiring would stay broken.

So, we set out to build a better system. On our platform, you get mock interviews with senior engineers who’ve been involved in hiring decisions at top companies. If you do well in practice, you get to bypass recruiter calls and resume screens and instantly book real technical interviews at top companies, as early as tomorrow, regardless of how you look on paper. *Real interviews are also* completely anonymous, and because we use interview data, not resumes, our candidates end up getting hired consistently by companies like Facebook, Uber, Amazon, Lyft, Dropbox, and many others, and 40% of the hires we’ve made to date have been candidates from non-traditional backgrounds (many were literally rejected from the same company when they went through the front door and someone saw their resume… only when they got to interview anonymously did they get the chance to show what they could do).

Here’s a new video we made explaining what we do. People don’t usually believe us when we explain it in words (“*Wait, what, you can book a real interview at Amazon tomorrow? What’s the catch?”*), so here are some nice animations. It’s real, we promise, and it works.

For most of our lifetime, practicing on our platform was completely free for engineers — you could get paired up with a professional interviewer (a senior engineer from a FAANG or FAANG-adjacent company) and do 3 mock interviews completely on us. This model worked pretty well because we monetized employers who wanted to hire our top-performing users, and the proceeds went to paying our awesome professional interviewers.

But it wasn’t perfect. To keep our costs manageable, we had to cap people at 3 mock interviews, and we had to waitlist a lot of people because either they weren’t experienced enough (despite our best efforts, we couldn’t get companies to pay us for access to junior engineers) or because they weren’t looking for jobs in our target locations.

Then COVID-19 happened and with it, a deluge of layoffs, hiring slowdowns, and freezes. We found ourselves down from 7-figure revenue to nothing. Companies didn’t really want or need to pay for hiring anymore.

If we wanted to keep offering practice in this climate, while still being able to pay our interviewers, we needed to find another revenue stream. After the quarantine began, we made the very hard call to start charging people for practice. But, charging felt anathema to our mission, so we also made a [public promise back in June of 2020](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all) to launch a program in the future where engineers could defer paying for practice til they found a job.

Today, we’re making good on that promise by launching our Pay Later Program.

## What is the Pay Later Program?

- **You get $512 or $1024 to spend on mock interviews.** Other than being cute powers of 2, we chose these numbers because our data shows that [after 5 mock interviews, your chances of passing real interviews will double](https://interviewing.io/blog/how-know-ready-interview-faang), so we wanted to get people enough practice to make a difference in their outcomes.
- You put down a credit card and promise to pay us when you find a new job. There’s no interest and no gotchas, and enrolling is really as simple as putting down a card and promising not to screw us over.
- We check in with you in 4 months (most of our users find jobs within 4 months of starting to practice). **If you’ve found a new job, great, we charge you, and any unused credits are yours to keep. If you have not, no worries, we grant you an extension** (and keep granting you extensions til you’ve found a job).
- Here’s the best part. **If you find a job through interviewing.io with one of our employer partners, then you don’t have to pay us anything!** (If you’re not part of the Pay Later program and you find a job through us, we *still* refund all the $$ you spent on practice.)

## Who’s it good for?

The Pay Later Program is available to all of our users who are authorized to work in the US, Canada, the UK, and Australia. We’ll be rolling out in other countries as we go.

Beyond that one criterion, this program is for any engineer or aspiring engineer who will be interviewing in the near to medium-term future. We know how much practicing for technical interviews sucks. If you’re a junior engineer, it’s scary because you haven’t done it before. If you haven’t come from a traditional computer science background, and especially if you’re coming out of a bootcamp, it’s scary because, chances are, you haven’t been taught this material properly (bootcamps aren’t good at this, sorry).

And if you’re an experienced engineer, interviews are scary because they’re not the work that you do every day, and ironically, the more experienced you are, the worse you do in these interviews, even as the bar keeps getting higher and higher.

Below you can see the seniority of users currently enrolled in our program:

![Chart showing seniority of users enrolled in interviewing.io's Pay Later Program](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FSeniority_of_users_enrolled_in_interviewing_ios_Pay_Later_Program_ee3a6ea4ab.png&w=750&q=75 "Pay Later Program participant seniority")

If you need interview practice, whether you’re junior or experienced, whether you look good on paper or not, you should sign up, and you’ll get value out of it. Whether you don’t have the $$ now or whether you’re just not sure if the investment is worth it, this program is for you.

## Does it really work?

Yes! The majority (over 80% of our users find jobs in 4 months or less), and by the 6 month mark, 94% of our users are employed. Here are some testimonials from our users. It was hard to pick which ones to share. We have a lot.

> *I love your platform, and you can confirm I use it all the time! I push using your platform as the #1 advice when anyone I mentor asks me for ‘prep advice’. I always tell them: “When you have that job at Google and all that stock, won’t you think the paid prep was worth it?”*

> *Your program has definitely changed my life. I was able to use some of my signing bonus to buy a home in the DC area, so this work is really starting to pay off.*

> *I can’t speak highly enough of the [Pay Later] program. As an engineer looking to make a jump, the mock interviews are an invaluable practice resource. Being able to utilize the mock interviews even before getting that really well-paying job, is life-changing. It gave me access to a resource which literally catapulted my career in ways I could only vaguely imagine a year ago. I really hope they keep the program around to help others who aren’t yet flush with cash.*

> *My experience with interviewing.io was awesome. I used the Pay Later program to schedule six practice interviews, which led to offers from my top targets: Google, Pinterest, Airtable and Stripe. It is hard to simulate an actual interview while using Leetcode, and I definitely feel the practice interviews were a key factor in the success I had during technical interviews throughout my job search.*

> *I signed up for the [Pay Later] program as I worked on transitioning from teaching high school to software engineering. The program seemed like a no-brainer. I could get all of the excellent practice opportunities I needed and I would only have to pay for the service after I landed a new job. A job which would nearly double my previous salary.*

> *interviewing.io’s [Pay Later] program was a godsend for me. I was not in the best place financially (as a job seeker does) and I needed the practice to improve my interviewing skills. Of all the platforms that I have tried, the most engaging and the most helpful is interviewing.io. They are so patient with the deferral process and I even received my current job through the platform!*

> *My mock interviews on interviewing.io were instrumental in my preparation for my latest job search. I also really appreciated the flexible payment options they were offering during the pandemic. Every mock left with me specific, actionable feedback that I could apply to my next phone screen, and it didn’t take long to see serious improvements in my performance, and the number of onsites I was booking. Eventually, I accepted an offer from Facebook, even with no degree and limited field experience. Sincerely, I can’t thank your team enough for helping make this dream of mine come true!*

> *Prior to using interviewing.io, I had been very discouraged and frustrated when I was constantly rejected after on-site interviews and not knowing what I did wrong or where to improve. To give it a perspective, I found myself clearing 90% of the phone screens but receiving a rejection after the onsite interviews. Even though there are lots of interview preparation resources out there such as Leetcode, AlgoExpert, Growking the Coding Interview, none of them truly bridges the gap between my technical knowledge and the interview expectation. The feedback provided by the professional interviewers during the mock interviews were extremely helpful as they provided points of improvement on areas I have never thought of. On top of all, the deferral program really provided me a risk free and worry free opportunity to try out the platform.*

> *I just cleared Amazon and Google interviews. I have a learning disability and social phobia. Prior to using interviewing.io I had never gotten past the technical phone screens. Getting in the habit of doing back to back interviews every weekend helped with both anxiety and getting better at timing myself to solve all problems within 45 min. $600 owed in a couple of months… will have been worth it 100x over 😊*

> *I think the practice at interviewing.io really helped me land a job at Google. The interviewers were professional and really improved my communication skills during the interview. Since each mock interview is structured just like a real interview, I got more familiarized with the environment and the process. The feedback helped me strengthen my foundation and make up for parts that I missed. Especially with the [Pay Later] program, I could afford the charges as a poor student… it felt like interviewing.io had trust in me and this really boosted my confidence.*

> *After I got laid off from my job, I greatly appreciated the opportunity to be a part of the [Pay Later] program. (I found something within the 4 month window!) The interview practice prepared me for my actual technical interviews and helped me approach my technical interviews with confidence. I got a lot of good feedback about how I could improve my presentation during a living coding session. Being able to listen to live recordings of my coding sessions was painful, but enlightening. My interviewers were helpful and asked tailored questions and gave useful feedback. I’ve recommended this to all of my friends who are looking for software engineering jobs.*

> *I absolutely loved the program. It allowed me to focus my energy on preparing myself for my technical interviews, instead of worrying about the cost of my practice. The mock interviews allowed me to get comfortable in a real interview environment, which helped me pass the interviews at Facebook, Google, Amazon, and more!*

> *Ended up getting a few offers but went with Google! I’m from a non-CS/non-SWE background and without a doubt, your platform helped tremendously with landing these offers. Additionally, the deferral program is a fantastic idea… I really appreciate what you are trying to do with interviewing.io and also with shaping the broader interview process. Keep up the great work!!*

> *My experience with the [Pay Later] program was great. I felt a bit anxious spending money initially on the site without the guarantee of receiving a job offer, [but the] flexibility the program offered to me both made me feel comfortable opting into the program as well as gave me the chance to get quality interviews that helped me receive a job. [The program offered] both offered a vital resource to me while also giving me comfort knowing that I wouldn’t need to worry about the cost until I achieved the goal of the program. If I were ever in the need of more mock interviews, interviewing.io would… be my first choice of sites to visit.*

> *I am happy to share with you that I was offered the software engineering position at Facebook that I had been preparing for. Your assistance through the [Pay Later] payment program and the mock interviews I utilized (both general and company-specific) were an integral part of my outcome as I received feedback on my interview performance that I had only ever guessed at. I am so grateful for the opportunity to have been able to do the deferred payment arrangement with Interviewing.io since I would’ve foregone paid mock interviews otherwise!*

Mathematics

### [Reverse Integer](/questions/reverse-integer)

[Given a 32-bit signed integer, reverse digits of the integer.](/questions/reverse-integer)

### [Confusing Number](/questions/confusing-number)

[Write a function that, given a room with 800 BIDDERS, identifies all the confusable numbers.](/questions/confusing-number)


# [How well do LeetCode ratings predict interview performance? Here's the data.](https://interviewing.io/blog/how-well-do-leetcode-ratings-predict-interview-performance)

By Mike Mroczka | Published: September 11, 2024; Last updated: September 12, 2024

Have you ever wondered if you should spend more time on LeetCode, participate in those contests, or focus on solving harder problems? A [popular Reddit post](https://www.reddit.com/r/leetcode/comments/1eg8060/you_probably_need_700_questions_to_actually_crack/) suggests you need 700+ questions and a LeetCode rating between 1800-2000 to pass FAANG interviews. Is this really what the data supports? To answer these questions and more, we looked at our users' LeetCode ranks and ratings and tied them back to interview performance on our platform and whether those users worked at FAANG.

In this post, we’ll share what we’ve learned.

interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. Hundreds of thousands of engineers have used our platform to prepare for interviews, and we have performance data for over 100k technical interviews (split between real interviews and mocks).

We surveyed almost 700 of our users and asked them to share their LeetCode and LinkedIn profiles. From those profiles, we pulled our users’ employment history, as well as their LeetCode data: number of problems worked, ratings, and, if they had it, contest performance. Finally, we cross-referenced all this data with their performance in mock and real interviews on interviewing.io.

This data set allowed us to start asking interesting questions. How much LeetCoding is useful? What kinds of problems are most useful to practice? How does LeetCode performance relate to performance in interviews with real people? Do LeetCode contest scores predict interview performance, and is a competitive coder likelier to work at a big tech company? Are hard questions worth solving or should we just stick with mediums? Let's find out!

The matrix below summarizes our findings. In addition to all LeetCode fields listed below, we also looked into global ratings and contest ratings. It was harder to find a large enough sample size to see any effect of those who did contests. Since the results were not large enough to be informative we have excluded them from the rest of this piece. All findings listed below are statistically significant, with lighter squares indicating stronger correlations.

For each profile, we looked at the following attributes:

- **Total questions**: the total number of questions solved on LeetCode
- **Hard questions**: the number of "hard" questions solved on LeetCode
- **Medium questions**: the number of "medium" questions solved on LeetCode
- **Easy questions**: the number of "easy" questions solved on LeetCode
- **Worked at FAANG**: whether or not the user has ever worked at a FAANG company
- **interviewing.io percentile**: how an interviewing.io user stacks up against other users of the platform, after having completed at least one interview

![How much Leetcode activity correlates with interview performance and FAANG employment](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fleetcode_activity_relationship_to_interview_performance_and_faang_empoyment_b84cc797d6.png&w=1200&q=75)

How LeetCode attributes relate to having worked at FAANG and performance in mock and real interviews on interviewing.io. Correlations range from 1 (directly positively correlated) to -1 (directly negatively correlated). 0 means there’s no relationship.

This matrix summarizes the correlations between LeetCode attributes (y axis) with 1) whether people worked at a FAANG and 2) how well they performed in interviews on interviewing.io (x axis). The higher the number (and the darker the color), the stronger the relationship.

For instance, the number of questions a user solved correlates with working at a FAANG company and a user’s percentile ranking on interviewing.io. We see that it is a stronger predictor of interview performance than it is for working at a FAANG company (0.27 and 0.17, respectively).

Let's dig into the most exciting findings!

### The total number of questions you have completed matters!

**Unsurprisingly, the total number of questions a user has completed correlates with having FAANG on their resume and doing well in technical interviews.** Those who work at FAANG companies appear to have completed more questions than those who do not.

![FAANG engineers question volume compared to nonfaang engineers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffaang_engineers_question_volume_compared_to_nonfaang_engineers_301c4b7405.png&w=1200&q=75)

Don't fret, though, as this doesn't mean you're doomed to be behind people who have started sooner and have amassed thousands of questions. The vast majority of our top users stop LeetCoding after they've hit about 500 questions. Only a fraction of top users have done more than ~500 questions, and only about 10 did more than a thousand questions. **As you might expect, there are seriously diminishing returns associated with doing more than 500 questions. Those who continued past the 500 questions performed only marginally better than those who had stopped near 500.**

*In the zero-sum interviewing game, those who complete the most LeetCode questions generally get the highest interview scores and work at the most desirable tech companies, but completing ~500 questions will put you among the top talent in the current market.*

### The difficulty level you attempt matters, too!

It is common wisdom on the LeetCode discussion board and subreddits to "do medium questions, not hard questions," with the argument that companies don't ask LeetCode hards. Does this advice stand up to scrutiny? **Mostly no.**

The data shows a clear bias towards those who solved more challenging problems, the exact point at which this matters is up for debate. At interviewing.io we find that before tackling difficult questions, you just need to first start doing problems. **Getting started matters more than attempting to complete a particular question difficulty, so don't focus arbitrarily on medium/hard questions, just focus on practicing**!

**Tip 1: Start with anything. Getting started matters more than picking the perfect set of questions.** For those just starting, you might think, "I'm a straight-A student and a fast learner, so I should be able to manage hard questions." Don't make this mistake, or you'll burn out quickly. Instead, just focus on doing questions and getting into the habit of translating your thoughts into code efficiently.

![probability of faang question difficulty](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fprobability_of_faang_question_difficulty_a08c9d52a1.png&w=1200&q=75)

Those who solved more challenging questions needed to do far fewer questions to have the same chance of getting into a FAANG company

![interviewing io percentile vs total questions by difficulty](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Finterviewing_io_percentile_ee5c50d4f0.png&w=1200&q=75)

Those who solved more challenging questions needed to do far fewer questions to have a high interviewing.io percentile

Before tackling mediums and hards, we need to first get good at easy questions. **Once we can tackle mediums, for each additional 50 questions you complete, you increase your score of passing your interviews by three percentage points.**

**On the other hand, a LeetCode hard carries over *twice* the benefits of a medium question. Completing 50 hard questions increases your interview score by seven percentage points!** Our data shows that you'd need to complete close to 233 medium questions to get the same benefits that 100 hard questions would give. Consider these two people (fictitious but inspired by real examples in our data):

| Question Difficulty | Alex | Kara |
| --- | --- | --- |
| Easy | 50 | 25 |
| Medium | 630 | 50 |
| Hard | 10 | 135 |
| Total | 690 | 210 |

These two people are statistically likely to have similar interview scores, yet Kara completed half the number of questions Alex did. While it goes against standard advice, harder questions having a higher benefit make sense. A LeetCode medium tends to require you to do one thing (perform a DFS, scan an array, etc.) to get the correct answer. In contrast, hard questions require you to do multiple things (memoize results while performing a DFS, tally prefix sums while scanning an array, etc.). **The compounding nature of multiple tasks in harder questions necessitates deeper learning in fewer questions.**

Admittedly, there is a chicken-and-egg scenario going on here. Naively, one might think everyone should just do hard questions, but that doesn't work in practice. Jumping straight to hard questions is an infeasible leap for most people, and the reason many people can complete hard questions is precisely *because* they have finished many medium questions first. Our advice is not to jump to hard questions immediately. Instead, you should be mindful and not stay at the medium difficulty level longer than necessary. It isn't that the advice to "do medium questions" is wrong—it is just overused. It is natural to want to be able to complete a question regularly in a short period of time. Still, if you find yourself completing LeetCode mediums quickly, it indicates that you have stagnated and would be better off moving on to hards. If you have solved 100 or more medium questions, you'll likely get more bang for your buck by transitioning to hard.

**Tip 2: Don't ignore hard questions.** Balance your medium questions with the hard ones. The compounding nature of multiple tasks in harder questions necessitates deeper learning in fewer questions.

Finally, we must also acknowledge that not all hard questions are created equal. This tip is my gut feeling, and we don't have data to back it up, but intuitively, we've all done hard questions that seem *impossible*, hard questions that seemed like they should have been rated *medium*, and even medium questions that seem like they should have been *hard*. Question difficulty is somewhat subjective, so if you focus on hard questions, choose questions with a high acceptance rating and/or ones that are "frequently asked." These questions tend to be *achievable* (they don't have a single impossible trick you need to just have known), *realistic* (they don't test nonsense algorithms no one knows like Manacher’s algorithm), and *solution-flexible* (there is often more than one valid approach).

**Tip 3: When choosing hard questions, pick popular questions.** They are more likely to be *achievable*, *realistic*, and *solution-flexible*, which will likely result in more learning per question.

### Contests and rankings don't matter

Surprisingly, we found no correlation between LeetCode ratings and their interviewing percentile. Those who were great at contests also didn't appear more likely to have FAANG on their resume. Two confounding variables that might have caused these unexpected results are:

1. **Small contest selection size:** Despite our best efforts, we received a relatively small group of candidates that attended any number of contests, let alone several. The data set may be too small to find a correlation.
2. **Biased candidate selection:** We incentivized candidates with free interviews, so those who regularly complete contests (and therefore have a lot of practice and likely feel confident in their skills) might not be interested in the rewards and weren't studied in this experiment.

**Tip 4: Don't worry about contests unless you enjoy them.** They provide a way to practice managing your time and keep yourself honest with how fast you are moving, but they don't contribute much to your success apart from containing more questions for you to attempt.

In the end, LeetCode questions are a reasonable proxy for predictors of interview performance. While ratings and contest scores don't seem to matter,[1](#user-content-fn-1) questions —and question difficulty—do. Focus on tackling increasingly difficult questions until you've hit that point of diminishing returns near ~500. Don't stagnate with medium questions; pick popular and frequently asked hard questions, too. And don't stress if your friend has a higher LeetCode score than you do; turns out it doesn't mean much!

1. A caveat here is that, strictly speaking, we're reporting correlations—not necessarily causal relationships. This leaves open the possibility that what we were measuring was not exactly a return to LeetCode but rather the effect of some third variable that increases both LeetCode activity and job prospects. For example, say MIT has a course that emphasizes coding puzzles, and MIT grads also get better jobs. In that case, what might look like a LeetCode benefit is actually an MIT benefit, and controlling for that would make the apparent benefit disappear. At the very least, though, these patterns are informative about the typical profile of successful engineers, which is likely a useful thing to emulate in general. Also, remember that our data still shows solving a healthy number of questions regardless of their difficulty can boost your chances of getting through difficult interviews. By definition, many people can pass interviews with far fewer problems completed, and many will pass after doing more problems. [↩](#user-content-fnref-1)


# [We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)

By Samantha Jordan | Published: February 12, 2018; Last updated: September 30, 2023

interviewing.io is a platform where engineers practice technical interviewing anonymously. If things go well, they can unlock the ability to participate in real, still anonymous, interviews with top companies like Twitch, Lyft and more. Earlier this year, we launched an offering specifically for university students, with the intent of helping level the playing field right at the start of people’s careers. The sad truth is that with the state of college recruiting today, if you don’t attend one of very few top schools, your chances of interacting with companies on campus are slim. It’s not fair, and it sucks, but university recruiting is still dominated by career fairs. Companies pragmatically choose to visit the same few schools every year, and despite the career fair being one of the most antiquated, biased forms of recruiting that there is, the format persists, likely due to the fact that there doesn’t seem to be a better way to quickly connect with students at scale. **So, despite the increasingly loud conversation about diversity, campus recruiting marches on, and companies keep doing the same thing expecting different results.**

In a previous blog post, we explained [why companies should stop courting students from the same five schools](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools). Regardless of your opinion on how important that idea is (for altruistic reasons, perhaps), you may have been left skeptical about the value and practicality of broadening the college recruiting effort, and you probably concede that it’s rational to visit top schools, given limited resources — while society is often willing to agree that there are perfectly qualified students coming out of non-top colleges, they maintain that they’re relatively rare. We’re here to show you, with some nifty data from our university platform, that this not true.

To be fair, this isn’t the first time we’ve looked at whether where you went to school matters. In a previous post, we found that [taking Udacity and Coursera programming classes mattered way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews). And way back when, one of our founders figured out that where you went to school didn’t matter at all but that [the number of typos and grammatical errors on your resume did](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/). So, what’s different this time? The big, exciting thing is that these prior analyses were focused mostly on engineers who had been working for at least a few years already, making it possible to argue that a few years of work experience smoothes out any performance disparity that comes from having attended (or not attended a top school). In fact, the good people at Google found that while GPA didn’t really matter after a few years of work, [it did matter for college students](https://www.businessinsider.com/how-google-hires-people-2013-6). So, we wanted to face this question head-on and look specifically at college juniors and seniors while they’re still in school. **Even more pragmatically, we wanted to see if companies limiting their hiring efforts to just top schools means they’re going to get a higher caliber of candidate.**

Before delving into the numbers, here’s a quick rundown of how our university platform works and the data we collect.

For students who want to practice on interviewing.io, the first step is a brief (~15-minute) coding assessment on [Qualified](https://www.qualified.io/) to test basic programming competency. Students who pass this assessment, i.e. those who are ready to code while another human being breathes down their neck, get to start booking practice interviews.

![Screenshot of the Interviewing.io interview feedback form highlighting the question: How were their technical skills?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F9fdaa_new_interviewer_feedback_circled_3de7112d80.webp%3Fupdated_at%3D2022-12-08T01%3A21%3A24.672Z&w=1200&q=75 "How were their technical skills?")

**On our platform, we’re fortunate to have thousands of students from all over the U.S., spanning over 200 universities. We thought this presented a unique opportunity to look at the relationship between school tier and interview performance for both juniors (interns) and seniors (new grads).** To study this relationship, we first split schools into the following four tiers, based on rankings from U.S. News & World Report:

- **“Elite”** schools (e.g. MIT, Stanford, Carnegie Mellon, UC-Berkeley)
- **Top 15 schools** (not including top tier, e.g. University of Wisconsin, Cornell, Columbia)
- **Top 50 schools** (not including top 15, e.g. Ohio State University, NYU, Arizona State University)
- **The rest** (e.g. Michigan State, Vanderbilt University, Northeastern University, UC-Santa Barbara)

Then, we ran some statistical significance testing on interview scores vs. school tier to see if school tier mattered, for both interns (college juniors) and new grads (college seniors), comprising a set of roughly 1000 students.

## Does school have anything to do with interview performance?

In the graphs below, you can see technical score distributions for interviews with students in each of the four school tiers (see legend). As you recall from above, each interview is scored on a scale of 1 to 4, where 1 is the worst and 4 is the best.

First, the college juniors…

And then, the seniors…

**What’s pretty startling is that the shape of these distributions, for both juniors and seniors, is remarkably similar. Indeed, statistical significance testing revealed no difference between students of any tier when it came to interview performance.[1](#user-content-fn-1) What this means is that top-tier students are achieving the same results as those in no-name schools.** So the question becomes: if the students are comparable in skill, why are companies spending egregious amounts of money attracting only a subset of them?

## Okay, so what are companies missing?

Besides missing out on great, cheaper-to-acquire future employees, companies are missing out on an opportunity to save time and money. Right now a ridiculous amount of money is being spent on university recruiting. We’ve previously cited the [$18k price tag just for entry to the MIT career fair](https://interviewing.io/blog/if-you-care-about-diversity-you-should-stop-hiring-from-the-same-five-schools). In a [study done by Lauren Rivera through the Harvard Business Review](https://hbr.org/2015/10/firms-are-wasting-millions-recruiting-on-only-a-few-campuses), she reveals that one firm budgeted nearly $1m just for social recruiting events on a single campus.

The higher price tag of these events also means it makes even less sense for smaller companies or startups to try and compete with high-profile, high-profit tech giants. Most of the top schools that are being heavily pursued already have enough recruiters vying for their students. Unwittingly, this pursuit seems to run contrary to most companies desires for high diversity and long-term sustainable growth.

Even when companies do believe talent is evenly distributed across school tiers, there are still reasons for why companies might recruit at top schools. There are other factors that help elevate certain schools in a recruiter’s mind. There are long-standing company-school relationships (for example, the number of alumni who work at the company currently). There are signaling effects too — companies get Silicon Valley bonus points by saying their eng team is comprised of a bunch of ex-Stanford, ex-MIT, ex- etc. etc. students.

## A quick word about selection bias

Since this post appeared on Hacker News, there’s been some loud, legitimate discussion about how the pool of students on interviewing.io may not be representative of the population at large because we have a self-selected pool of students who decided to practice interviewing. Certainly, all the blog posts we publish are subject to this (very valid) line of criticism, and for this post in particular. As such, selection bias in our user pool might mean that 1) we’re getting only the worst students from top schools (because, presumably, the best ones don’t need the practice), or 2) we’re getting only the best/most motivated students for non-top schools, or both. Any subset of these is entirely possible, but we have a few reasons why we believe what we’ve published here might hold true regardless.

First off, in our experience, regardless of their background or pedigree, everyone is scared of technical interviewing. Case in point… before we started working on interviewing.io, we didn’t really have a product yet. So before investing a lot of time and heartache into this questionable undertaking, we wanted to test the waters to see if interview practice was something engineers really wanted, and more so, who these engineers that wanted practice were. So, we put up a pretty mediocre landing page on Hacker News… and got something like 7,000 signups the first day. Of these 7,000 signups, roughly 25% were senior (4+ years of experience) engineers from companies like Google and Facebook (this isn’t to say that they’re necessarily the best engineers out there… but just that the engineers the market seems to value the most still need our service).

Another data point comes from one of our founders. Every year, Aline does a guest lecture on job search preparedness for a technical communication course at MIT. This course is one way to fulfill the computer science major communication requirement, so enrollment tends to span the gamut of computer science students. Before every lecture, she sends out a survey asking students what their biggest pain points are in preparing for their job search. Every year, trepidation about technical interviewing is either at the top of the list of 2nd from the top.

And though this doesn’t directly address the issue of whether we’re only getting the best of the worst or the worst of the best (I hope the above has convinced you there’s more to it than that), here’s the distribution of school tiers among our users, which I expect mirrors the kinds of distributions companies see in their student applicant pool:

## So what can companies do?

As such, companies may never stop recruiting at top-tier schools entirely, but they ought to at least include schools outside of that very small circle in the search for future employees. The end result of the data is the same: for good engineers, school means a lot less than we think. The time and money that companies put in to compete for candidates within the same select few schools would be better spent creating opportunities that include everyone, as well as developing tools to vet students more fairly and efficiently.

As you saw above, we used a 15-minute coding assessment to cull our inbound student flow, and just a short challenge leveled the playing field between students from all walks of life. At the very least, we’d recommend employers do the same thing in their process. But, of course, we’d be remiss if we didn’t suggest one other thing.

1. Of course, this hinges on everyone completing a quick 15-minute coding challenge first, to ensure they’re ready for synchronous technical interviews. We’re excited about this because companies can replicate this step in their process as well! [↩](#user-content-fnref-1)


# [When is hiring coming back? Our predictions for 2024.](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024)

By Aline Lerner | Published: December 10, 2023; Last updated: December 7, 2024

**EDIT**: *If you don't like reading, here's me presenting the contents of this blog post in a video. Pick your poison.*

Predictions are hard, and, inevitably, most of them turn out wrong. But we’d like to brave the scathing mockery of the internets and try anyway! Our courage is bolstered by some useful data we have (both proprietary and gathered from the internet), which we’ll use to guess what will happen in 2024 and to answer the question foremost in many of our minds: *“When is hiring coming back?*”

interviewing.io is an anonymous mock interview platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. In our lifetime, we’ve hosted over 100k technical interviews, split between mocks and real ones. **Data from these interviews helps us get an insider’s perspective on what’s actually going on in the engineering market. For instance, we know how many people are practicing for interviews at Google vs. Meta vs. other FAANGs. Because we offer salary negotiation, we know how often people are negotiating, how successful they are, and what their compensation looks like. We also know what the “bar” looks like in different kinds of technical interviews over time — how well you have to do to pass algorithmic interviews, system design interviews, and so on.**

And, finally, we ran several surveys with our users recently (thank you, kind and patient users), to learn about their recent job searches and outcomes. We also asked our users to share some advice based on their experience with the current job market. We’ll include bits of advice as quotes throughout this piece.

Putting all of this data together gives us the kind of clarity that wouldn’t be possible from looking at any one piece individually. For instance, once you realize that most FAANGs are only hiring for backfills, while Meta is on a hiring spree, you start to understand why they’ve been treating candidates poorly and why the market has allowed them to do it. We’ll get to all of that in a bit. First, here’s Nostradamus Corgi with our predictions for 2024.

## Predictions for 2024

![Nostradamus Corgi](https://strapi-iio.s3.us-west-2.amazonaws.com/Nostradamus_Corgi_c5e6d122c7.png)

Those who've owned a touch bar MacBook with butterfly keyboard will appreciate that a hologram-snow-globe provides a vastly superior input device.

**Prediction #1: 2024 is the year when hiring comes back.**  
Whether we like it or not, the FAANGs are driving overall eng hiring volume. As you’ll soon learn, Meta and Netflix are hiring aggressively while the other FAANGs are not, but I’m inclined to think that come next year, most of the FAANGs will follow Meta and Netflix’s lead and start actively hiring again.

**Prediction #2: Mid-level & senior eng hiring will pick up significantly, come Jan 2024.**  
We’re less confident about this part, but we expect that hiring will be back to (or quite close to) H1 2022 levels in the spring of 2024.

**Prediction #3: For at least the next 6 months, compensation at a given level will stay flat.**

**Prediction #4: For at least the next 6 months, down-leveling will continue because of inertia.**

**Prediction #5: For at least the next 6 months, recruiters are going to be increasingly stretched thin, which means that applying online is going to be an even less effective way to get into companies.**

**Prediction #6: The hiring bar will NOT return to where it was for a long time.**

Of course, anybody on the internet can make predictions about hiring, but ours are supported by actual data (below), which we want you to dive into to see if you agree with us. And at the end of this piece, we’ll include a final bonus prediction and actionable advice and resources you can use to capitalize on these predictions.

First, let’s look at some benchmarks from the broader internet about the state of hiring and layoffs.

## Eng jobs are recovering, and layoffs are way down

Below is a graph of open tech jobs over time, from TrueUp. TrueUp indexes open jobs at “tech and tech-ish companies” that have one or both of the following traits: “product + services would not exist without the internet” and/or “raised money from a VC”.

![Number of open tech jobs at tech startups, tech unicorns, and public tech companies ](https://strapi-iio.s3.us-west-2.amazonaws.com/Number_of_open_tech_jobs_be3f1062fe.png)

Source: <https://www.trueup.io/job-trend>

As you can see, tech jobs started to drop in May 2022 (around the time the FAANGs froze hiring) and began to recover slowly in March 2023.

At the same time, layoffs are way down after peaking in Q1 2023. The number of people laid off is down 88%, and companies doing layoffs are down 68%. **Most importantly, layoffs have solidly returned to pre-downturn levels.**[1](#user-content-fn-1)

![Number of tech employees laid off each quarter ](https://strapi-iio.s3.us-west-2.amazonaws.com/Tech_employees_laid_off_f5bea7bcdf.png)

Source: layoffs.fyi; Q4 projections are ours

**So, tech jobs are recovering and layoffs are way down. Woohoo!**

Back to open tech jobs for a moment. When we saw fewer people booking mock interviews in 2022, we started to use the TrueUp graph as a sanity check to learn if it was just us. Turns out… it’s not just us. Here is the same graph from above, overlaid with mock interview purchases on interviewing.io. We’ve hidden the y-axis values because we’re not comfortable sharing our exact sales numbers with the world, but the shape of the graph is what matters most here.

![interviewing.io purchases and number of open tech jobs are related](https://strapi-iio.s3.us-west-2.amazonaws.com/iio_purchases_and_number_of_open_tech_jobs_related_1cc780f368.png)

Source: <https://www.trueup.io/job-trend> and proprietary interviewing.io data

Along with feeling some relief that we weren’t entirely to blame for fucking up our business and could, in good conscience, cast some blame on macroeconomic conditions, we also noticed another interesting thing — **purchases on our platform mirror what’s going on in the broader market and effectively give us insight into hiring trends.**

If you believe, like we do, that purchase activity is a good proxy, then we can conclude that **eng jobs appear to be recovering faster than tech jobs as a whole. In the graph above, hiring started turning around in January 2023 and has grown 58% since then, with most of the growth happening between Q3 and Q4 of this year.**

We’ll be using interviewing.io purchase activity as a proxy for open eng jobs for the rest of the post.

## What’s going on at specific FAANGs?

As you’ll see in the next few sections, our mock interview purchase data is tightly coupled to events in the outside world. In addition to learning what’s going on with hiring as a whole, our data shows us what’s going on at specific companies, letting us see which ones are actively hiring and how much. We can clearly see when various companies freeze, go on hiring sprees, or just start hiring a little bit. Since many of our users are preparing specifically for FAANGs, we’ll start there.

We’ve done our best to corroborate the trends we’re seeing in our data with insiders at these companies, but that doesn’t mean we haven’t missed something or made mistakes. If something looks incorrect, we want to hear from you, and we’ll fix it. Just email [hello@interviewing.io](mailto:hello@interviewing.io).

Below is a graph of Google-specific purchasing activity on interviewing.io over time. (As before, we’ve hidden y-axis values, but what matters most is the shape of the graph.) As you can see, it tracks with what was happening in the world — Google slowed their hiring in May 2022, along with other companies like Uber, and [officially froze hiring in July](https://www.theverge.com/2022/7/20/23271634/google-hiring-pause-two-weeks-review-headcount-needs).

![Mock interview purchases over time for Google](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Google_e1498fc2d0.png)

**Since then, Google’s hiring continued to slow, falling 90% from peak and 50% since January of this year, until it finally leveled out around July. The latest news we’ve heard is that while Google is currently hiring, it’s primarily backfills for attrition rather than new headcount.**

Want to know if you’re ready to interview at Google? Do anonymous mock interviews with real Google interviewers, and see exactly where you stack up.

### Meta

![Mock interview purchases over time for Google and Meta](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_overtime_for_Google_and_Meta_b5d62232e5.png)

Like Google, Meta froze hiring in 2022 (they actually did it before Google – it was officially announced in [May](https://www.businessinsider.com/facebook-is-freezing-hiring-heres-why-and-who-it-impacts-2022-5)). **However, unlike Google, they’re currently hiring aggressively, and if you believe that mock interview purchases are a good proxy for hiring volume, they’re up about 800% since January of this year, and are now back to pre-freeze levels (though not yet back to peak).**

Some anecdotal data we have backs this up. At interviewing.io, we offer salary negotiation help to our users, which means that we talk to a lot of people, in depth, about where they’re interviewing and how it’s going. Almost every negotiation candidate we’ve had recently has been interviewing at Meta. What’s even more interesting is the volume of possible teams that candidates talk to during team matching. Earlier this year, Meta’s story seemed to be:

> *We’re primarily hiring for backfills. These are the 1-2 teams that might have open headcount. You gotta decide fast, or those slots will be filled.*

Now, our users are regularly presented with more than 10 teams to choose from.

Meta’s hiring boom, relative to most other FAANGs’ conservative hiring, has one notable dark side. Meta’s hiring volume is clearly outpacing all the other FAANGs (Netflix is also up, but Meta’s eng team is more than 10X the size of Netflix’s, so in the absolute, Netflix’s hiring volume isn’t enough to balance Meta out) — for all intents and purposes they’re the only FAANG that’s really hiring at scale — so they’re currently getting away with treating candidates really poorly.

We’ll talk more about this in the down-leveling section, but Meta has capitalized on the market imbalance and has been rampantly down-leveling candidates, presumably because they can — in the pre-downturn market, competition would have made that difficult, but now, given that they’re responsible for much of the FAANG hiring volume, they can rely on their candidates not having many comparable counteroffers and can present candidates with low-ball offers.

Outside of down-leveling, which, in fairness, doesn’t happen to every candidate, Meta also has changed their stance on negotiation.

Before the downturn, it was basically enough to say that you were interviewing at Google to get a signing bonus (or a higher signing bonus). Now, Meta will not lead with signing bonuses unless you’ve been down-leveled, in which case they’ll use them as a consolation prize of sorts (still cheaper for them because it’s a conditional one-time payment). Moreover, Meta will not negotiate unless you can show higher counteroffers (they don’t actually make you show the paperwork, but they ask for details). Recruiters have been trained to say, “I’ll go to bat for you and take this to the compensation committee, but only if you can show me a compelling reason, i.e., other offers.” If you can’t show other offers, they will not budge a dime.

Finally, Meta has gotten much more aggressive with offer deadlines. Recruiters will push you to sign within a day or two of completing team matching and getting your offer numbers.

These kinds of high-pressure, candidate-unfriendly hiring practices would have been unthinkable in a more competitive hiring market, but right now Meta can get away with it. I would argue that their strategy is shortsighted, however, because this kind of market instability can’t last long, and when the other FAANGs are back, Meta will start to see serious attrition from the engineers they’ve down-leveled and low-balled. I’m sure Meta talent execs have made this decision with their eyes open and are prepared to take the hit, but shame on them nevertheless, and I hope that the talent drain they experience next year is swift and brutal. Sorry, reader, I don’t usually insert my opinions in a primarily data-driven piece, but the kind of unnecessary stress and sleepless nights their shortsighted policies have created for our users, who are highly competent, thoughtful, and kind senior engineers, makes my blood boil.

**TL;DR: [Meta is on a hiring spree](https://interviewing.io/guides/hiring-process/meta-facebook#meta-facebook), and it’s created an unstable equilibrium, which means that likely the other FAANGs will start hiring again soon. More on that later.**

### Amazon

![Mock interview purchases over time for Google, Meta and Amazon](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_overtime_for_Google_Meta_and_Amazon_739953c328.png)

Amazon is an interesting beast. After both Google and Meta froze hiring in summer of 2022, Amazon went on an opportunistic hiring spree, and again, you can see in the graph above that mock interview purchase patterns do indeed reflect what’s going on in the outside world.

**After a few months of basically a monopoly on eng hiring, Amazon pulled back and froze in [November 2022](https://techcrunch.com/2022/11/03/amazon-exec-confirms-corporate-hiring-freeze-through-end-of-year/). They’ve been flat since January of 2023 and, from what we’ve heard, are only hiring for backfills and very senior roles.**

It is rare to see one FAANG deviate drastically from the others when it comes to hiring, whether aggressively hiring when everyone else has paused or pausing when everyone else is hiring. It creates an unstable equilibrium, one that can’t last very long. We’ll talk a bit more about this later.

Want to know if you’re ready to interview at Amazon? Do anonymous mock interviews with real Amazon interviewers, and see exactly where you stack up.

### Apple and Microsoft

Microsoft officially froze hiring [sometime in Q4 of 2022](https://www.ciodive.com/news/aws-hiring-freeze-microsoft-google-cloud/635865/) (though [unofficially it seems to have been much earlier, around July](https://www.glassdoor.com/Community/jobs-in-tech/has-anyone-heard-if-theres-a-hiring-freeze-at-microsoft-right-now)). **Since then, [Microsoft hiring](https://interviewing.io/guides/hiring-process/microsoft#microsoft) has been flat and, from what we know, limited to backfills.**

Apple, on the other hand, [did not freeze hiring in 2022](https://9to5mac.com/2022/11/02/apple-hiring-freeze-budgets-report/) and continued to hire on a “deliberate basis” (headlines about it have been consistently misleading), and they’re the only FAANG that hasn’t done a large-scale layoff. **[Apple’s hiring](https://interviewing.io/guides/hiring-process/apple#apple) has been slow for the past year and a half but has fallen about 70% since January of 2023.**

![Mock interview purchases over time for Apple and Microsoft](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Apple_and_Microsoft_66656291ce.png)

### Netflix

We couldn’t find an official hiring freeze announced by Netflix, but according to Blind and anecdotal reports, they slowed down hiring on a department-by-department basis.

We have a gap in our Netflix purchase data because we stopped offering Netflix-themed practice for some time, and this gap would make a multi-year graph misleading. That said, we have been offering **Netflix practice since the beginning of 2023, and since then, demand** (and [Netflix hiring](https://interviewing.io/guides/hiring-process/netflix#netflix)) **appears to have grown, up almost 300% since January.**

![Mock interview purchases over time for Netflix](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_over_time_for_Netflix_1cb05d59f9.png)

Want to know if you’re ready to interview at Netflix? Do anonymous mock interviews with real Netflix interviewers, and see exactly where you stack up.

## Who’s actually hiring right now?

Here’s a [list of all the (larger) companies](https://docs.google.com/spreadsheets/d/1icCMfo3NQk-UqL6GJrQxYkebz0UUIayiGfGL3YZGpck/edit#gid=0) where our users are interviewing right now (which means they’re actively hiring!), based on our onboarding flow, salary negotiation data, and a recent survey. In cases where we’ve written guides for these companies’ interview processes, I’ve linked to them in the doc as well.

[![List of companies hiring in Q4 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/List_of_companies_hiring_in_q4_2023_defad29b21.png)](https://docs.google.com/spreadsheets/d/1icCMfo3NQk-UqL6GJrQxYkebz0UUIayiGfGL3YZGpck/edit#gid=0)
  
It appears that despite the slowdowns and primarily just hiring for backfills, FAANG is still dominating — over half of our users are interviewing at FAANG companies, with startups comprising only 17% of interview volume.
![Where interviewing.io users are interviewing](https://strapi-iio.s3.us-west-2.amazonaws.com/Where_our_users_are_interviewing_d34e61755b.png)

## Salary negotiation: an important trailing indicator

Before we analyze all these disparate FAANG data points and connect them to our 2024 predictions, we have a few more important pieces of data to share with you. One of them is around salary negotiation, a useful and important trailing indicator.
At interviewing.io, in addition to providing mock interviews, we provide help with salary negotiation. I do a lot of these sessions myself (it’s one of the most rewarding parts of my job), and leading these sessions gives me a direct line to our users and ultimately an invaluable ear to the ground. Users share with us where they’re interviewing, how it’s going, what their offers look like with respect to both compensation and level, and more.

I’ll talk more about what we’ve learned in these conversations (in aggregate, of course) later in the post, but for now, I’d like to share how our salary negotiation business has changed over time and what insights these changes give us into the broader market.

Below is a graph of salary negotiation requests over time on interviewing.io (a request is someone booking salary negotiation help — they put down a credit card but don’t have to pay anything unless the negotiation is successful, so we don’t call them purchases like we did with mock interviews above).

![Salary negotiation requests over time](https://strapi-iio.s3.us-west-2.amazonaws.com/Salary_Negotiation_Requests_Over_Time_18316fcf2f.png)

There are a few important things that immediately jump out in this graph. First, you can see that salary negotiation requests were at a relative high in June 2022 (the data we have from before isn’t shown because we were logging activity differently). Then they took a nose-dive in November 2022 (dropping by 4X). Compare that to mock interview purchases over time above. You can see that salary negotiation activity has roughly a 3-month delay on mock interview activity, which makes sense because people tend to start thinking about negotiation around the time it looks like they’re going to get an offer, whereas they start practicing much earlier.

Though negotiation requests are still not up to peak levels, they’ve been growing steadily and have 2X’ed since the start of 2023.

One other bit of anecdotal negotiation data confirms that hiring is on the rise — we have seen our candidates interviewing with roughly 2X more companies in Q3 and Q4 than in Q1.

Because of the delay between practice (and ultimately real interviews) and negotiation, salary negotiation requests are a trailing indicator. In other words, these changes in salary negotiations are only observable after hiring volume has already shifted. **Therefore, they don’t predict changes in hiring volume but rather confirm them after they have already occurred and ultimately help demonstrate that the growth we’ve seen in hiring volume is not a fluke.**

## What about compensation? And are people negotiating their salaries in this climate?

We talked a bit already about the kinds of insights we get from our salary negotiation program, both qualitatively and quantitatively. One of the quantitative pieces is offer sizes, broken down by cash, equity, and bonuses.

Below is average total compensation (initial offers, before negotiating, just for public companies, as we don’t presume to accurately value private company equity) among our users over time. As you can see, it has largely stayed flat, despite the downturn.

![Average initial offers over time](https://strapi-iio.s3.us-west-2.amazonaws.com/Average_initial_offers_over_time_e4f1c58592.png)

We were also fortunate to get a data set from the folks at [comprehensive.io](https://comprehensive.io), a source of comp data from the maker of [layoffs.fyi](https://layoffs.fyi). What we really liked about this data set is that it had engineering salaries for the kinds of eng roles that our users were actually targeting, in the locations where they were likely to work (if you look at broader aggregators, you’ll see that the numbers are much lower than you’d expect because they factor in engineering-adjacent positions in non-tech hubs).

![Software engineer salaries stayed flat throughout 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/Eng_salaries_stayed_flat_throughout_2023_45916eef35.png)

Source: [comprehensive.io](http://comprehensive.io) (from the maker of layoffs.fyi)

As you can see, comprehensive.io’s data backs up what we’ve seen among our negotiation users — compensation has indeed stayed flat despite the downturn.

Here are a few tidbits not visible in the graphs above:

- **We have NOT seen a difference in the average increase from successful negotiations.**
- **That said, it’s now much harder to negotiate successfully without multiple offers than in H1 2022.** It used to be enough to say that you’re interviewing at a few high-profile companies and that if you get meaningfully more comp, you’ll stop (basically companies would pay you to stop interviewing). Nowadays, companies are less likely to pay to get you off the market and will insist on needing at least one other offer to justify a comp increase (as you saw above with Meta).
- **Companies are much less likely to lead with a signing bonus.** Instead, they’ll leave them in their back pocket and only pull them out if necessary. One notable exception is scenarios where the engineer got down-leveled (more on that later). In those cases, companies might soften the blow by offering a meaningful signing bonus.

> *“The market is really making things difficult. Expect things to be a bit harder and the offers to be lower than previous years. Negotiating is going to be absolutely crucial — now more than ever.”*

## The changing role of recruiters, and the very practical thing it means for you

Take a look at the graph below. The black curve is the same one we first shared above, when we graphed it against total open tech jobs — it’s total mock interview purchases on interviewing.io, which we have found to be a good proxy for the state of eng hiring (we’re using this graph and not the one from trueup.io because the latter doesn’t specifically show engineering jobs, and seeing engineering jobs specifically matters for the point we’re about to make). As before, we’ve hidden the y-axis values to protect our business.

Since the start of the year, mock interview purchases (and resulting eng jobs) have grown by 58%.

![Software engineer jobs are recovering, recruiter jobs are not](https://strapi-iio.s3.us-west-2.amazonaws.com/Eng_jobs_vs_recruiter_jobs_a1cdbe8e46.png)

Source: interviewing.io proprietary data and [lightcast.io](https://lightcast.io)

The dark blue line is new. It’s the number of open tech recruiter jobs over time. As you can see, engineering jobs are recovering, but recruiter jobs are not (and because this isn’t our proprietary data, we’ve included numbers on the right y-axis). In fact, since the start of the year, open recruiter jobs have shrunk by 52%.

We’ll talk about the implications of this disparity in a moment, but before we do that, let’s look at one more graph:

![Rise in inbound applications in business and technical roles](https://strapi-iio.s3.us-west-2.amazonaws.com/business_and_technical_roles_apps_ef78404990.png)

Source: [Ashby’s 2023 Trends Report | Applications Per Job](https://www.ashbyhq.com/talent-trends-report/reports/2023-trends-report-applications-per-job?utm_source=adwords&utm_medium=ppc&utm_campaign=Ashby+Keyword&utm_term=ashbyhq))

This graph comes from Ashby, a new applicant tracking system (ATS) that’s used by a growing number of startups and tech companies. Because they’re an ATS, they have a treasure trove of data about who applies to what jobs, when. As you can see, the number of applicants for technical roles held steady from 2021 to 2022, and then it began to creep up before it hit a serious growth inflection point at the end of 2022. Though Ashby’s graph only goes up to April 2023, since the start of 2023, inbound applications for technical positions have doubled.

**Plainly put, the number of eng positions recruiters are responsible for filling is increasing, and the volume of applicants is growing, while the number of recruiters among whom that work is distributed is shrinking.**

You might argue that recruiters might not end up having to do more work because this disparity will force them to rely even more on automated filtering of applications. This may very well be true. However, whether increasingly harried recruiters will take less time to review inbound applicants or whether this is an opportunity for a new wave of resume-filtering solutions, one thing is clear. While applying online has always been a pretty BAD IDEA — because it’s the moral equivalent of shouting into a black hole — it’s more true now than ever before.

In addition to the market data above, we have some anecdotal info and survey results. In our survey that preceded this post, 17% of senior engineers cited that getting in the door was the hardest part of their job search, even harder than technical interviews.

Finally, eng leaders we’ve spoken to have confirmed their recruiting teams (and sometimes their eng teams) are overwhelmed with inbound applications.

**So, do not apply online in this climate, full stop.** Instead, reach out to hiring managers — recruiters are not incentivized to break rules and will likely not get back to you, especially if you don’t look perfect on paper. Moreover, the numbers will be on your side, as relatively few candidates are targeting hiring managers directly. We plan to write a full blog post on how to do this kind of outreach well, but this CliffsNotes version will get you started:

- Get a LinkedIn Sales Navigator account
- Make a target list of hiring managers at the companies you’re interested in
- Figure out their emails (you can use a tool like RocketReach), and send them something personalized. Do not use LinkedIn. The same way that you don’t live in LinkedIn, eng managers don’t either.

Some advice from our users about getting in the door:

> *“Be prepared, and also be proactive. Don't just wait for the recruiters to contact you.”*

> *“Networking is key. If you know the name of the person who will be interviewing you, research them online to try to get a feel for who they are and what types of questions they might ask.”*

## Down-leveling

As I mentioned above, Meta has been brazen in their down-leveling of candidates. It usually manifests as a down-level (e.g., you interview for an E5 role and then end up at an E4). This kind of down-leveling has happened to almost every negotiation client we've had who has interviewed at Meta.

Making matters worse, we've found that it's virtually impossible to negotiate without another offer on the table. It doesn't necessarily have to be from another FAANG, but it needs to be from a notable FAANG-adjacent company.

Basically Meta's approach post-downturn has been as follows:

- Interview someone
- Down-level them, citing poor performance in system design interviews
- Make them a lowball offer even for that band but compensate for it with a decent signing bonus (~$50-70k) as a way to soften the blow and make it more likely candidates will accept quickly
- Refuse to negotiate unless candidates show them other offers: "I can't take this to the compensation committee without a compelling reason, i.e., another offer."

Though we’d argue Meta has drawn the hardest line with respect to negotiation, as discussed above, they’re not the only company that’s down-leveling candidates. **According to our survey, of the people who got offers in 2023, 33% were down-leveled.**

**We didn’t have enough data for Apple and Amazon, but using our survey results, we’ve computed the probability of getting down-leveled at the rest of the FAANGs, given that you receive an offer there.** At Netflix, it’s 50%. At Meta, it’s 55%. At Google, it’s 59%, and finally at Microsoft it’s a whopping 66%. Again, although Meta’s down-leveling percentage isn’t as high as Microsoft’s, they’re hiring far more engineers, so their actions have a much greater effect on the marketplace.

![Probability of being down-leveled by FAANG companies](https://strapi-iio.s3.us-west-2.amazonaws.com/Probability_of_being_down_leveled_by_FAANG_74be6f98df.png)

Source: interviewing.io survey data

**For all other companies (small startups, large startups, and FAANG-adjacent), the probability of getting down-leveled is about 37%.** Almost everyone we heard from was down-leveled by just one level, but we saw a handful of people drop by two levels.

As you’d expect with the rise down-leveling, the bar for interview performance has been going up as well, and this rising bar accounts for some of the down-leveling we’re seeing (though we expect that cost savings for the employers who are doing it accounts for much of the rest).

## The rising bar

The rising bar makes sense — after all, more engineers are competing for relatively fewer positions. Because we have a bunch of interview data, we are actually able to quantify the increase.

Specifically, after every interview on our platform, whether it’s a mock or a real one, the interviewer leaves feedback. The feedback form looks like this, and it includes a yes/no question for whether they’d move the candidate forward, as well as star ratings of their performance on a scale of 1 to 4, where 4 is best. These star ratings cover 3 areas: coding ability, problem-solving ability, and communication skills.

![interviewing.io feedback form](https://strapi-iio.s3.us-west-2.amazonaws.com/feedback_93fb5e447f.png)

To track how the bar has changed, we can look at the average coding and problem-solving ratings for successful interviews over time (we haven’t focused on communication ability because we previously discovered that it doesn’t significantly affect interview outcomes except for very senior roles).

![The bar for coding ability and problem solving continue to rise](https://strapi-iio.s3.us-west-2.amazonaws.com/Bar_for_coding_ability_and_problem_solving_ed03a59d2f.png)

Source: Proprietary interviewing.io data, based on tens of thousands of interviews

As you can see above, the coding and problem-solving scores for successful interviews have risen steadily since the start of 2022. To make these numbers concrete, you now need to perform 22% better in technical interviews than you did at the start of last year (with respect to both coding and problem solving). [The last time we wrote about the rising bar was in November 2022](https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews), and back then you had to do 15% better. The bar has only continued to go up.

We came up with these numbers after bucketing interview performance into percentiles (more realistic than just looking at average raw scores because relative differences in raw scores may amount to much larger or smaller differences when competing against other engineers). At the start of 2022, you had to be in the 65th percentile to pass interviews, namely outperform 65% of engineers. At the end of 2022, you had to be in the 78th percentile, and now you have to be in the 83rd.

This data is corroborated by our users’ experience with the job market. In our survey, we asked our users if they felt that the bar was higher and/or the interview process was more difficult than the last time they were interviewing, 77% said yes.

## Bonus prediction about AI

**Prediction #7: For a long time in the future, we won’t see major changes to interview styles, despite the advent of ChatGPT. We will, however, see changes to interview questions.** Specifically:

- Companies will have to move away from asking verbatim LeetCode questions because of how easy it is to cheat.
- Similarly, we expect that async coding tests will go away for the same reason.
- There will be a premium on good human interviewers.

This final prediction might feel a bit random, as we haven’t discussed AI at all till now, but we’ve been doing some experimentation on how tools like ChatGPT are going to change technical interviewing. We’ll publish details soon, and we decided to include the AI predictions in this post because the topic is too salient not to.

## Predictions recap

We’re currently in the eye of an unstable period. It’s simply not sustainable for most FAANGs to just be backfilling while Meta and Netflix plow ahead with 2022-level hiring volume. Though Amazon did something similar in 2022, unstable equilibria like this don’t last. Sure, you could make the argument that Meta and Netflix will just freeze again (like Amazon did, after their hiring spree), but between the recent rise in mock interview bookings overall, the increase in salary negotiation requests, and the huge drop in layoffs, it seems likelier that next year the other FAANGs will join Meta and Netflix in actively hiring again.

Here are some key data points that support our predictions:

- Meta and Netflix mock interview bookings, and subsequent hiring, are way up (+800% and +300% respectively). Again, having only 2 FAANGs hiring is untenable because this type of unstable equilibrium can’t last long.
- Open tech jobs are growing (+14% since Feb 2023).
- Tech layoffs are down, and specifically the number of people laid off is -93% since Jan 2023.
- Mock interview bookings are growing (+58% since Jan 2023).
- Salary negotiation is up 100% (trailing indicator).

Even though hiring is going to come back, it’s not going to feel quite like the heady days of 2021 and early 2022. The bar is going to stay high for the foreseeable future — as you saw, it’s up 22% over the start of 2022, and walking that kind of thing back takes a long time.

Similarly, until most of the FAANGs are back to their pre-downturn hiring velocity, which will probably take at least 6 months, we will continue to see down-leveling, not just because of the rising bar but, frankly, because companies can get away with it while candidates have less leverage. For the same reason, salaries will continue to stay flat — they’ve been flat for so long, despite market changes, that until we’re firing on all cylinders again, inertia will keep them there.

Finally, as hiring recovers, more recruiters will get hired back, but that, too, will take some time. For at least the next 6 months, recruiters will continue to be overworked, which means that, for the foreseeable future, applying online is going to be a fool’s errand.

## Actionable advice and resources

We have every reason to believe that by H2 of 2024, hiring will be back to normal, or close to it. In the meantime, though hiring is coming back, it’s not back to normal quite yet. During this liminal period, you can’t approach your job search the same way you did during the boom.

As such, even though this was a very long post, the main takeaways are surprisingly short: It’s more important than ever to have to get creative with how you get in the door (so you don’t get lost in the shuffle), to be even more prepared for interviews than before (especially system design), and to do your best to get multiple offers (if you don’t, it will be much harder to negotiate. Here are some specific resources and tips to help you do these things:

- If you’re targeting specific companies but don’t have connections there, don’t apply online and don’t wait. Instead, reach out to hiring managers directly. And don’t do it on LinkedIn; email is a much better channel.
- Use our [Company Intros](https://start.interviewing.io/company-introductions) feature if you have access — we get you in front of a decision-maker human immediately.
- Use our [company process guides](https://interviewing.io/topics#companies) to help you plan your job searches and to prepare for specific companies. And use our [Learning Center](https://interviewing.io/learn) too. It has technical topic explanations, a bunch of interview replays, and a whole lot more.
- Prepare for your interviews. You don’t have to use us. Just find a way to do some mock interviews before your real ones.
  - Our [AI Interviewer](https://start.interviewing.io/interview-ai) is a great, free resource that’s more effective than LeetCoding on your own — it lets you practice algorithmic questions with a human-like interviewer in CoderPad and gives you feedback at the end.
  - If you don’t have an interviewing.io account, you can also try out our [Technical Interviewer custom GPT](https://chat.openai.com/g/g-Fhk16eFON-technical-interviewer-by-interviewing-io) (it does system design interviews as well!).
- Study system design (we have the [best system design guide that’s out there](https://interviewing.io/guides/system-design-interview)). Lackluster performance in system design interviews is the main reason people are getting down-leveled.
- In this climate, negotiating is harder. Don’t set yourself up to fail. [Read this post immediately](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) to set yourself up for success and to prevent making mistakes at the beginning of the process.

We’ll close this post with some more independent advice from our users:

> *"Make sure to practice system design as much or more than ds/a.”*

> *“Learn what the process involves at each company you're applying to. Aggressively plan and prepare for each. Do a lot of mock interviews, and be ready for the unexpected.”*

> *“After waves of layoffs, there are so many candidates with great experiences and skills who are looking for jobs. Even if you ace the interview, they may fill the position with someone else who did better, so don't be too discouraged if that happens, and keep trying!”*

> *“Be honest with yourself about how much you actually need to prepare. And give yourself that time. Prepare prepare, prepare!”*

1. Though we didn’t have data on what portion of layoffs were engineers specifically, we did some unscientific spot checking, and generally engineers comprised 20-30% of layoffs at a given company. [↩](#user-content-fnref-1)


# [How to sabotage your salary negotiations efforts before you even start](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start)

By Aline Lerner | Published: August 22, 2023; Last updated: September 6, 2024

*Note: If you’d like a practical primer on negotiation, read my [previous post on negotiation first](https://interviewing.io/blog/negotiate-salary-recruiter) — it tells you exactly what to say in a bunch of situations. This post is longer and more academic, but of course I include some practical tips and teach you what to say in a few situations, as well.*

At interviewing.io, we’ve coached hundreds of people through salary negotiation. We’re good at it — our average user gets $50k more in cash, and we have a 94% success rate.

Having done this a lot, we’ve seen our users make the same two mistakes, over and over, BEFORE they start working with us. These mistakes are costly and make it harder for us to do our jobs. Our advice is applicable to everyone, but I wrote this post primarily to share with interviewing.io’s user base, so that future clients of our negotiation service don’t shoot themselves in the foot.

These are the two things you **must** avoid. Both involve how you talk to recruiters at the start of your job search, way before there’s an offer:

1. Revealing information too early in the game
2. Negotiating before you’re ready

In this post, I’ll explain why these two mistakes routinely sabotage salary negotiation efforts and what to say to recruiters instead. In a nutshell, if you can just be in “passive information gathering” mode (more on that later) for most of your recruiter interactions, you’ll be golden. It’s hard to not to share info about your job search with your recruiter, especially as you build more rapport with them, but we’ll tell you exactly what to say instead.

Before we get into all of that, I want to go over two foundational things about negotiation.

1. Recruiters are not your friend, and they don't work for you.
2. What negotiation is and what it’s not

## Recruiters are not your friend, and they don’t work for you

*“It is difficult to get a man to understand something when his salary depends on his not understanding it.*”   
-Upton Sinclair

I used to be a recruiter. I ran my own agency, and I also worked in-house before starting interviewing.io. That means that I’ve had to [struggle with the tangled incentive structure that comes with being a recruiter](https://blog.alinelerner.com/if-youre-an-engineer-who-wants-to-start-a-recruiting-business-read-this-first) (see the section called “You should write down your principles”). There’s always a tension — recruiters are, by and large, good human beings who genuinely want to help their candidates, but they also have an employer they’re beholden to, as well as a comp/bonus structure that rewards certain behaviors, some of which run counter to candidates’ best interests.

There’s some distinction between in-house recruiters and third-party recruiters (recruiters who work for an agency that does placement, rather than a specific company that’s hiring engineers).

### Third party recruiters

My general policy with third-party recruiters is to not tell them ANYTHING and to always deal directly with the companies they introduce you, once you establish a point of contact there. You should assume that anything you tell your recruiter is going to get back to every company you’re working with. Why? Because their primary objective is to place your butt in the seat of one of the companies they’re working with, and they will do whatever they need to do to make the deal happen. Often, those things will run counter to your interests.

A big misconception that many candidates labor under is the idea that because third-party recruiters get paid every time they make a placement, their interests are fundamentally aligned. At a high level, this is kind of true, but once you dig into the details you'll see a lot of nuance.

A recruiter, depending on market conditions, gets anywhere from 8%-25% of the candidate’s base salary when they make a placement. In the current climate, it’s around 10%. However, that cut is going to the recruiting agency as a whole rather than to the individual recruiter — you will almost always end up working with large agencies rather than a sole-proprietor shop where the owner gets to take all of it home.

Let's say that you get an offer with a base salary of $150,000. You talk to your third-party recruiter and tell them that you would like more money. The recruiter may go to the hiring manager and try to advocate for you, but they're not going to push very hard because the incremental difference in their cut is going to be pretty small and to them the thing that matters most is getting butts in seats. After all, they're evaluated on the number of hires they make, first and foremost, independent of comp. Understanding that, let's do the math anyway. Say that they’re able to risk closing the deal and get you $165k. Before, the agency would have gotten paid $15k. Now the agency gets paid $16.5k. That incremental $1.5k isn’t worth risking a deal over (even a few thousand dollars would not justify jeopardizing the deal). On top of that, the individual recruiter is only going to maybe get a few hundred dollars total from that increase. So for them the difference really isn’t worth it. **Third party recruiters are incentivized to get the deal done, not to risk the deal by negotiating hard for you.**

Moreover, because they’re incentivized to get the deal done, you should assume that your recruiter will share anything you share with them with the company or companies they’ve introduced you to. If you tell them that a company is your first choice and that you’re tempted to accept, they will likely share that with the company and may even recommend that they not raise your comp, since you’re already so enthusiastic. If you share that you’re not very interested in a company, and the recruiter has other candidates they’re presenting, they will prioritize those candidates’ experience over yours and will possibly tell the company not to invest in you as hard.

### In-house recruiters

What about in-house recruiters? In-house recruiters may or may not get a bonus for hires that happen on their watch; it depends on the company. But if they do, that bonus is generally NOT tied to your compensation, and in some cases, they may get a bigger bonus if they’re able to negotiate you down. At big companies, in particular, in-house recruiters follow a playbook. They’re trained to make offers within specific bands, and they’re trained to mobilize such that they don’t lose candidates to other big companies — if you wave a Facebook counteroffer in front of Google, they will act. If you tell them you’re interviewing at a startup, they will not, because they know that startups don’t pay as much. They’re actually evaluated on how well they follow the playbook. Because of that, there is no reason to assume that their incentives align with yours. They’re incentivized, first and foremost, to follow the rules their head of department sets for them. This is true for how they evaluate candidates, who they let through, and how they read resumes. And it’s definitely true for how they negotiate.

If you’re interested in peeking behind the curtain on how recruiters think, I interviewed three of the best ones in the industry recently. You can watch that below:

I’ll close this section the way I began it. Recruiters want to help, and many are rooting for their candidates. But they’re also operating inside a box, and that box isn’t set up to put your interests first.

## What negotiation is and what it’s not

Probably because of bad books and airplane magazine ads (for those of you old enough to remember those), people often think that negotiation is all about saying the right thing, or how firm your handshake is, or any other amount of silly nonsense. **The reality is that negotiation is all about preparation and leverage.**

Don't forget to prepare for your technical interviews too. Sign up today for anonymous mocks with senior engineers from top companies.

Preparation and leverage means doing the work to make sure that you have multiple offers, that all your offers come in at the same time, and that you don’t tip your hand too early. Laying this foundation is 80% of the work. You’ll need to slow some companies down, speed some companies up, and hold off questions from recruiters until you’re ready to negotiate, and not before. If you do this right, the actual negotiation part will be easy and almost a foregone conclusion.

Is it possible to negotiate when you don’t have multiple offers and when you haven’t done the foundational work? Sure, it is, and we’ve sometimes had success with our users doing that. But it’s much harder, and the ceiling on how much more money you can get is lower.

With all that out the way, let’s talk about how the two biggest mistakes people make and how to not make them!

## Mistake #1: Revealing information before you’re ready to negotiate

You’ve probably never been arrested, but if you’re like me, you’ve watched a lot of police procedurals on TV. You know the bit where they read the suspect their Miranda rights? They start like this:

*You have the right to remain silent. Anything you say can and will be used against you in a court of law…*

Talking to recruiters is exactly the same, and one of the biggest mistakes we see our users make is sharing information too early. This is generally the only mistake we can’t walk back — once you share information, you can’t undo it, and sharing information actually has no upside, only downside. When you’re ready to negotiate, you’re doing so deliberately because you already *know*
the state of the world, and you’re choosing to reveal the parts that set you up for success. Before that, you’re just revealing stuff that can be used against you.

**Specifically, do not share with recruiters anything about your salary history (though it’s illegal in many states to ask this directly, there are indirect ways of asking, and many still do), your salary expectations, where else you’re interviewing, and how far along in the process you are with other companies. In short, don’t share any information about money or other interviews.**

The main question recruiters ask up front about money is: “What are your compensation expectations?” They claim that it’s because they want to make sure that you’re not so far off in your expectations that interviewing with that company would be a waste of time. This is a nonsense reason — *very few* companies pay so much below market that it would be a nonstarter. Those companies know who they are, and they know to give you a heads up that they pay below market. Moreover, with the recent advent of companies sharing salary bands, you’ll have some idea if they're grossly below market before you interview. The real reason recruiters ask about compensation expectations is so that they can use it against you later in negotiations.

As such, if you answer this question with a number, you set an artificial ceiling on your offer. Do not even utter a single number to a recruiter until you’re ready to bargain. Do not go on levels.fyi and comment on the ranges listed for your level, even if you’re currently underpaid and an average offer from them would be life changing. **Do not say a number first — ever.**

You can see exactly what to say when you get asked about compensation expectations in the section called “How to handle recruiter calls” below.

The most obvious way to lose leverage is revealing information about money. The other way to lose leverage is by sharing information about where else you’re interviewing. If you share this information, you risk prematurely scaring off smaller companies because they don’t think they can win in a bidding war with FAANG. You also risk cornering yourself into a situation where the company knows your options are limited, and they might be inclined to lowball you as a result. Finally, you risk getting an exploding offer to try to force you to make a decision before you’re ready.

Below are some examples that I hope will drive these points home.

### Example #1: You’re interviewing at Google, Meta, and two startups

Let’s say that you’re currently interviewing at Google, Meta, and two startups (let’s call them A and B). You’re at the onsite stage with Google, you’re doing technical phone screens at both startups, and you’re just doing your first recruiter call with Meta. This is actually a very strong position to be in!

Of course, your Meta recruiter asks you about your comp expectations and where else you’re interviewing.

**If you reveal your comp expectations, it will be hard to walk them back:**

- Let’s say that you currently work at a startup and make $150k in cash with some amount of equity. You go to levels.fyi or a similar site and look up Facebook’s salary bands for the role you’re targeting. Let’s say those bands for total comp are $250k-$350k. Hell, that’s way more cash than you’re making now, so you decide to share that range, thinking that if those are their bands already, it does no harm. That’s reasonable, except that let’s say Google ends up making you an offer, and it’s $400k (we’ve seen this scenario happen to a bunch of our users). Now you have to walk back what you said, in which case your recruiter will invariably ask why. And now you have to reveal, before you’re ready, that you have a Google offer, which means you’ll probably end up revealing that it’s for $400k. Now you’ve set an artificial ceiling for your Facebook counteroffer to be $400k as well, when in reality that ceiling may have been closer to $450k or even $500k.

**If you reveal that you’re at the onsite stage with Google and talking to some startups, here's what will happen:**

- Your recruiter will do the math and start asking you in a few weeks if you got to team matching.
- Now Meta knows that Google is the only possible offer on the table that they should be worried about. They (and other big companies) don’t take startup offers nearly as seriously because equity is monopoly money 'til it’s not… and even if you don’t reveal the cash portion of your comp, they’ll assume it’s smaller than what they’re offering.
- If you didn’t pass the onsite, it’s going to be hard to not share that when you’re persistently asked about it, unless you lie (which I absolutely do not condone). Now you’ve lost leverage because Meta knows that you cannot possibly have any other big tech company offers.
- If you DO perform well in your Google onsite, that’s great, but team matching can take a while (yes, I know Google is changing their process such that you’re now interviewing for a specific team at the outset, but the broader point still stands). So now if Meta is about to make you an offer, they can set an artificially fast expiration date to run out the clock.

**Though you started in a strong position with multiple interviews, including at companies that are known to pay well, you’ve now weakened that position by sharing details.**

Here’s another thing that could happen in this scenario. Let’s say that it’s the same set of companies as above, but this time you’re talking to the recruiter from startup A. The recruiter asks you where else you’re interviewing.

**If you mention that you’re interviewing at both Google and Meta, they might get spooked.**

- I’ve seen this happen a bunch. Dropping FAANG names can be a good power move, or it can shoot you in the foot, and which it’ll be really depends on the situation. Many small startups view FAANG candidates as risky because they know they can’t compete on comp and are worried that you’re going to walk the moment that you get a FAANG offer. This may or may not be true (not everyone is motivated just by comp!), but it’s not to your advantage to reveal it. YOU should be in control of if and when we play the FAANG card.

### Example #2: You’re interviewing at one company and are also up for a promotion

Here’s a different example. Let’s say that you work at a startup, and you’re up for a promotion soon. You figured it’d make sense to see what’s out there as well, so you’ve started interviewing with another startup.

Your recruiter asks you in your first call about where else you’re interviewing and what your comp expectations are. You may be tempted to mention that you’re up for a promotion because that feels like it’ll give you leverage — if you get a promotion, the startup will have to work harder to entice you to leave, after all. Not so fast!

**If you mention that you’re up for a promotion:**

- Your recruiter will start checking in consistently on whether you got it. Promotions always take longer than you think, and the increase in your comp may not be what you expect. At some point, if you haven’t gotten it yet, the recruiter will assume it’s not coming, and then you actually lose leverage because they know that you’re going to be more likely to walk.

**If you mention that you’re not interviewing anywhere else, that’s just a giveaway that you have no leverage:**

- Promotion or not, many of our users assume that they have leverage because, “I don’t have to leave my job, so my current job is leverage.” That’s not true, though — even in this climate, and definitely in a hiring boom, an engineer having another job is table stakes. Almost every candidate you’re competing with will be currently employed. So even if having a job gives you a little bit of leverage, it gets canceled out when everyone has the same exact thing.

The details may differ in your case, but the fundamental mechanics are the same. When you reveal information before you know what hand you’ve been dealt, it can only hurt you. I’m struggling to think of a scenario where revealing something has been beneficial.

I suppose the one exception to revealing information is this: Sometimes it can be useful to give your recruiter a rough estimate for when you’ll be collecting offers, e.g., “I’ve just started interviewing. I expect to get through all my interviews and onsites in the next 6 weeks and start collecting offers 2 months from now. Does that timeline work for you?”

This technique can be helpful for aligning expectations up front and then keeping recruiters off your back, as they won’t need to chronically text you to make sure you haven’t taken another offer yet (we’ll talk more about texting with recruiters in the next section). But note that even in this example, we’re not actually revealing any information about where you’re interviewing, how long it’s taking, or compensation. You’re just setting a timeline based on *hypotheticals* without giving out any details that can be used against you later. When you share the actual timeline you’re working with, you no longer control the timing of your job search, and a huge part of negotiation is controlling timing so you can make all your offers come in at the same time.

## Mistake #2: Premature negotiation

*“Don't fire until you see the whites of their eyes!*”  
-Unknown officer at the Battle of Bunker Hill

Just like not revealing information too early, you also want to avoid negotiating too early. They’re two sides of the same coin.

Think of it like a hand of cards. At the beginning, you have no idea what you’re going to draw. The longer you wait to negotiate, assuming you’ve timed things correctly, the more information you have. Then, when you’re ready to negotiate, you can look at your hand and selectively share information that puts you in the strongest position. For instance, if you have a high base salary from one company, a great equity package from a public company, and a signing bonus from a third company, you can strategically share those portions of the offers without sharing the weaker parts. Each negotiation is different, and it’s hard to give catch-all advice, but that’s generally the situation you should set yourself up to be in.

With that in mind, I’m a firm believer in negotiating when you’re ready and not before. Until you know what else is on the table, it’s really hard to 1) have the bravado that comes with actually having multiple offers (this is possible to fake, but trust me, it’s hard) and 2) negotiate effectively — you will never know as well as your recruiter what salary bands are like, what market comp is, and so on. They do this all day. This may be your first or fifth time doing it, but there’s massive experience and information asymmetry. There are two ways to combat this power imbalance: have as many of your interactions be asynchronous as possible (we discussed that earlier) and do everything you can to negotiate when you’re at the point of maximum information, and not before. Daniel Biales, one of our former negotiation clients, [explained the latter really well](https://levelup.gitconnected.com/learn-how-to-negotiate-your-salary-with-expert-help-5afedd11a178).

*When I received a low offer, my first inclination was to start the negotiating process. Aline helped me to realize that this was not the best course of action. The problem with this approach is that I wanted to start negotiating before receiving my highest offer. If I negotiated an increase then, I would have had to renegotiate when I received the higher offer. This will cause negotiating fatigue for you and the company. They will be less likely to negotiate a second time because they don’t know how many times you will ask them for more. First, focus on strategies to draw out your decision. Then, when you have all your offers, start negotiating. There may be a couple of back and forth communications, but they will be over a short time span rather than drawn out.*

### Example #1 revisited: You're interviewing at Google, Meta, and two startups

Let’s review our first example again. Imagine that you’re interviewing at Google, Meta, and two startups, A and B, just like before. Startup A makes you an offer: $160k base, 0.1% of the company in options over four years, no signing bonus. You react to it and say that you were hoping for a signing bonus. The recruiter comes back with a $10k signing bonus quickly and pressures you to make a decision, saying that they have other candidates waiting.

By starting to negotiate, you accelerated their timeline, and this is going to make it hard to go back and ask for more signing bonus.

You try to stall, and then a few days later, Google makes you an offer that includes a $25k signing bonus. You’re still excited about the startup for reasons other than compensation, but now you have to go back to them and say that you actually got a $25k signing bonus at Google. They are unlikely to move again.

So, don’t negotiate until you’re ready. It’s hard to walk things back.

That doesn’t mean it’s not possible to negotiate in stages and gradually start bringing up all your offers. In my experience, however, this is a much more difficult maneuver, takes way more experience, is much more stressful for the candidate, and often ends up with the same results as laying a solid foundation and just negotiating once at the end.

So what do you say when you get asked pointed questions by your recruiter about your comp expectations or where you’re interviewing? And how do you delay negotiation until you’re ready?

## How to handle recruiter **calls:** “passive information gathering” mode

Until you’re ready to negotiate, your default mode should be “passive information gathering.” This means that you listen rather than talk. I coach all of our negotiation clients to be in this mode when they get on a call with a recruiter.

- **Be polite and gracious to a fault.** If it’s an offer call, thank them for the work they did to put your offer together and for advocating for you. If it’s earlier in the process, thank them for their help so far.
- **Express genuine enthusiasm.** If it’s an early call, express excitement about the company or the interview process if there’s something idiosyncratically cool about it. If it’s an offer call, express excitement for the team, the projects, the hiring manager, whatever it is. And be genuine. Every offer will have something exciting about it.
- If the recruiter is making an offer, do not react to what you’re being told beyond expressing enthusiasm. **Say that you need some time to process and/or talk to your {family, partner, spouse}.** Why should you not react? Because recruiters do this a lot, and you don’t. Negotiating on the phone on the fly is really hard. You’re going to need time to think, and you need a way to level the playing field. Email is the great leveler in these interactions because it’s asynchronous, and it gives you time to think and plan. No one is realistically expecting you to react to major life decisions on the fly!
- **Use email (not phone when possible and *DEFINITELY* not texts) to interact with recruiters. We strongly advise you to leave your phone number off your resume, and if you have to fill in any required phone number fields while applying, to put a Google voice number. Also, in your first conversation with your recruiter, let them know that you are very bad at using the phone and strongly prefer email. Finally, whenever they text or call, wait a few hours, and answer with email and remind them that email is the best way to reach you (you can always say that you don’t have notifications enabled during work hours).** Why does all of this matter? Phone calls are extremely disruptive, as are texts. The only way to level the playing field in negotiation is to have time to think and to possibly ask for advice. In particular, the casual nature of texting lulls you into a false sense of security. Moreover, the fact that texts or phone calls interrupt you from something else puts you at a disadvantage — when you get interrupted, your instinct is to quickly respond to make the interruption go away. But knee-jerk responses are rarely the right ones, and you’ll find yourself giving away information you shouldn’t have. You will have to get on the phone eventually, of course, but you never have to text.
- If a recruiter asks you pointed questions about comp expectations or where else you’re interviewing, refer to the section called “Exactly what to say” right below!

## Exactly what to say

In all the snippets below, you’ll notice that they end with the same sentence: *I promise not to accept other offers until I have a chance to discuss them with you.*

This is deliberate, and it’s there because it’s disarming. Fundamentally, recruiters ask you all of these questions because they don’t want to lose out on you and have you go to another company. If you can speak to that worry head-on, there’s not much they can say back.

**For questions about comp expectations at the beginning of the process:**   
*At this point, I don’t feel equipped to throw out a number because I’d like to find out more about the opportunity first – right now, I simply don’t have the data to be able to say something concrete. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*

**For questions about comp expectations at the end of the process:**   
*It sounds like there’s an offer coming, and I’m really excited about it. I’m not sure exactly what number I’m looking for, but if you’d be able to share what an offer package might look like, then I will gladly iterate on it with you if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*

**For questions about where else you’re interviewing at the beginning of the process:**  
*I’m currently speaking with a few other companies and am at various stages with them. I’ll let you know if I get to the point where I have an exploding offer, and I promise not to accept other offers until I have a chance to discuss them with you.*

**For questions about where else you’re interviewing at the end of the process:**  
*I’m wrapping things up with a few companies and in process with a few more. I promise to keep you in the loop, and I promise not to accept other offers until I have a chance to discuss them with you.*

**For when a recruiter provides you a salary range and asks you to comment on it, at the beginning of the process:**  
*Thank you for sharing that with me. Right now I don’t know enough about the opportunity to value it concretely, and I honestly haven't done my market research. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*

**For when a recruiter provides you a salary range and asks you to comment on it, at the end of the process:**  
*Thank you for sharing that with me. I haven't done my research, so I am unable to comment on that range. However, if you do make me an offer, I promise to iterate on it if needed and figure out something that works. I promise not to accept other offers until I have a chance to discuss them with you.*

I said it in the beginning, and I’ll say it again. Negotiation isn’t about saying the right thing. It’s about laying a foundation: not revealing anything until you’re ready to negotiate, not negotiating too early, and making sure that you’ve set yourself up to have multiple offers.

Then, once those offers come in, you swoop in with sharp precision, negotiate once (possibly with just your top choice company), and be done with it.

If you’ve set yourself up for success, done the foundational work, and haven’t made the mistakes in this post, the negotiation will feel like a foregone conclusion.

*If you need some hands-on help navigating salary negotiation, [sign up for our salary negotiation package](https://start.interviewing.io/salary-negotiation). You don't pay anything unless you get more, and we’ll be with you every step of the way, for every recruiter call, every email you need ghostwritten, and every strategy discussion. Unlimited sessions, unlimited help, whatever we need to do to get you results.*


# [Are recruiters better than a coin flip at judging resumes? Here's the data.](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes)

By Aline Lerner and Peter Bergman | Published: April 30, 2024; Last updated: May 19, 2024

*This post is a very exciting first for interviewing.io because it’s about a proper experiment run by a real, live academic research lab. If you’ve been reading my work for the past decade, you know that I’ve always been something of an armchair researcher. I ran some experiments before starting interviewing.io, and since then, my team and I have [kept it up](https://interviewing.io/blog/category/data-deep-dives).*

*One of the experiments I ran before I founded interviewing.io was an attempt to figure out how good recruiters were at judging candidate quality based on resumes. I ran it 10 years ago and discovered that not only was everyone bad at judging resumes (about as accurate as flipping a coin), [they all disagreed with each other about what a good candidate looked like](https://blog.alinelerner.com/resumes-suck-heres-the-data/).*

*Even though these results were shocking at the time, the study had some serious limitations. First, I had no objective measures for which candidates were actually good. I was working as a recruiter at the time, so I knew whom I had been able to place, but that’s obviously not the be-all and end-all of engineering ability. Second, I had a non-representative sample of software engineers. Due to my brand, I had managed to attract a lot of excellent, non-traditional candidates — engineers who were actually very good but didn’t look good on paper. These types of resumes are the hardest for recruiters to judge, and the data was full of them. Finally, my sample size wasn’t that big: I ended up with 716 data points in total, only about half of which came from recruiters (the rest came from engineers and hiring managers — my original hypothesis was that they might be better at the task, but I was wrong… everyone was bad at judging resumes).*

*So, now that I’m CEO of interviewing.io, with access to a lot more data, resources, and a team of excellent academics at [Learning Collider](https://www.learningcollider.org/), we decided to run this study again, but with a more rigorous treatment and better conditions, to see if we could replicate the results. This time, we focused just on recruiters, given that they’re most often the gatekeepers who decide which candidates get an interview.*

***Below are all the details, but here’s the TL;DR: we reproduced my results from 10 years ago! Our new study showed that recruiters were only a bit better than a coin flip at making value judgments, and they still all disagreed with each other about what a good candidate looks like.***

*In this piece, we also talk about:*

- *How far off recruiters were in their predictions and how much they disagreed with each other*
- *What recruiters say they look for vs. what the data shows they actually look for*
- *Why recruiters taking more time to parse resumes would lead to better outcomes (median parse time is just 31 seconds)*
- *Whether AI can do a better job at judging resumes (spoiler: yes, it can)*

*The rest of this piece is co-authored by Peter Bergman, Tushar Kundu, and Kadeem Noray of Learning Collider.*

In the real world, resumes (or LinkedIn profiles) are evaluated by recruiters in minutes — even seconds — and these evaluations are THE thing that determines who gets an interview.

But what do these word walls tell recruiters? How predictive are their evaluations of actual interview success? Ultimately, how good are recruiters at judging resumes?

To answer these questions, we designed a study approximating technical recruiters’ decisions in the real world. We asked[1](#user-content-fn-1) 76 technical recruiters (both agency and in-house) to review and make judgments about 30 engineers’ resumes each, just as they would in their current roles.

They answered two questions per resume:

- Would you interview this candidate?[2](#user-content-fn-2) (Yes or No)
- What is the likelihood this candidate will pass the technical interview (as a percentage)?

We ended up with nearly 2,200 evaluations of over 1,000 resumes.

The resumes in this study belonged to interviewing.io users (with their consent) — actual engineers currently on the job market.

Collaborating on this study with interviewing.io is an ideal scenario, precisely because outcome data were available for comparison purposes. Each engineer in this study has completed multiple mock interviews on the platform. Performance in these interviews is quite predictive of performance in real interviews: top performers (roughly the top 5% of users) on interviewing.io are 3X more likely to pass technical interviews at top-tier companies than candidates from other sources. Even passing a single interview on interviewing.io is a strong predictor of outcomes; it's associated with a 32% increase in the chance of working at a FAANG company post-interview.

Once we had recruiters’ evaluations of the resumes, we compared them to how those engineers actually performed on interviewing.io: skills scores, feedback from interviewers, and ultimately, whether they passed or failed their mock interviews.

## Recruiters’ resume judgments are just slightly better than a coin flip

### Question #1: Would you interview this candidate?

In aggregate, recruiters in the study recommended 62% of candidates for an interview. But how did recruiter evaluations stack up against candidates’ performance on the platform?

**We calculated recruiter accuracy by treating each candidate’s first interview (pass/fail) as the truth, and recruiters’ decision to interview as a prediction. It turns out that recruiters chose correctly 55% of the time, which is just slightly better than a coin flip.**

### Question #2: What is the likelihood this candidate will pass the technical interview?

Recruiters predicted the likelihood that each candidate would pass the technical interview. In most hiring processes, the technical interview follows the recruiter call and determines whether candidates proceed to the onsite. Being able to accurately predict which candidates will succeed at this stage is important and should inform the decision about whether to interview the candidate or not.

**What we found most surprising is how far their predictions were from the truth:**

**- When recruiters predicted the lowest probability of passing (0-5%), those candidates actually passed the technical interview with a 47% probability.**- **When recruiters predicted the highest probability of passing (95-100%), those candidates actually passed with a 64% probability.**

Below is a graph that shows recruiter predictions vs. actual performance. The x-axis is the bucketed recruiter rating. In other words, the first point is all the candidates that recruiters assigned a 0-5% likelihood of passing. The y-axis is the average interviewing.io pass rate for those candidates. The red dotted line represents 100% accuracy – in an ideal world, the higher a recruiter's ranking of a candidate, the higher their actual performance would be. The orange line represents reality – as you can see, there isn’t much correspondence between how recruiters predicted candidates would perform and their actual performance.

![recruiter predictions vs interviewing.io performance](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpredictions_0e401e8d92.png&w=1200&q=75)

Recruiters’ predictions below 40% underestimate these candidates by an average of 23 percentage points. Above 60%, they’re overestimating by an average of 20 percentage points. **If this was predicting student performance, recruiters would be off by two full letter grades.**

## Recruiters can’t agree on what a good candidate looks like

Clearly, there is lots of noise in resume evaluations. Were recruiters’ noisy judgments at least consistent when reviewing the same resumes?

Nearly 500 resumes were evaluated by more than one recruiter. Based on a random selection of two evaluations per resume, the overall likelihood of two recruiters agreeing to either interview or not interview a given candidate was 64%.

Since recruiters also guess the probability a candidate will pass the technical interview, we can compare how different these guesses are for a given candidate. **The average differential between two randomly selected recruiters’ evaluations of the same resume was 41 percentage points. So, let’s say one recruiter predicts a 30% probability the candidate would pass; another recruiter evaluating the same resume would predict, on average, a 71% probability of passing.**

To further understand just how prevalent the disagreement is, we looked at the standard deviations for across-candidate evaluations and same-candidate evaluations:

- 0.34 across different candidates
- 0.32 across the same candidates

**So, when two recruiters are asked to judge the same candidate, their level of disagreement is nearly the same as if they evaluated two completely different candidates.**

## The most sought-after resume attributes

**Despite the noise and variability in the study’s resume evaluations, there were some characteristics that recruiters consistently favored: experience at a top-tier tech[3](#user-content-fn-3) company (FAANG or FAANG-adjacent) and URM (underrepresented minority) status (in tech, this means being Black or Hispanic).**

Most predictive for Question #1 (whether a recruiter would want to interview that candidate) was experience at a top company — these candidates were 35% more likely to be picked. Black or Hispanic candidates are also associated with an increased likelihood a recruiter would interview a candidate — by 21%.[4](#user-content-fn-4)

With Question #2 (how likely the candidate was to pass a technical interview), having a top company on your resume is associated with a 21% increase in the likelihood that recruiters believe the candidate will pass the interview. Compared to the actual pass rates, recruiters’ predictions of FAANG candidates are generally accurate (average 4 percentage point overestimate).[5](#user-content-fn-5) Unlike the presence of a top company, URM status didn't appear to influence recruiter decisions here.

## How do recruiters’ stated reasons for rejecting candidates line up with actual rejection reasons?

So, we know what recruiters tend to favor, whether they’d admit to it or not: 1) FAANG/FAANG-adjacent experience and 2) URM status. But what’s even more interesting than why a recruiter would say yes is why they would say no.

When we asked recruiters to judge a resume, we also asked them WHY they made that decision.[6](#user-content-fn-6) Below are recruiters’ stated reasons for rejecting candidates. As you can see, “missing skill” is the main reason by far, with “no top firm” a distant third.

![Bar chart of recruiter's stated reasons for rejection](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frej_reasons_full_a70e7af161.png&w=1200&q=75)

So, then, we wondered… How do recruiters’ stated reasons for rejecting candidates line up with reality? To figure that out, we analyzed the resumes that ended up in the rejected pile and looked at common traits.

**Below is a graph of actual rejection reasons, based on our analysis. The main rejection reason isn’t “missing skill” — it’s “no top firm.” This is followed, somewhat surprisingly, but much less reliably (note the huge error bars), by having an MBA. “No top school” and having a Master’s degree come in at third and fourth. Note that these top four rejection reasons are all based on a candidate’s background, NOT their skill set.**

![Predictors of recruiter rejections](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frej_regression_8776500012.png&w=1200&q=75)

The y-axis is the coefficient from regressing rejection on that variable. So, a coefficient of Y for a given trait means that trait is associated with a Y\*100% percentage point increase in the likelihood of being rejected.

## Slowing down is associated with better decisions

Another key piece of this study is time. In hiring settings, recruiters make decisions quickly. Moving stacks of candidates through the funnel gives little room to second-guess or even wait before determining whether or not to give a candidate the opportunity to interview.

In our study, **the median time spent on resume evaluations was just 31 seconds**. Broken down further by Question #1 — whether or not the recruiter would interview them — the median time spent was:

- 25 seconds for those advanced to a technical interview
- 44 seconds for those placed in the reject pile

![Distribution of time taken to evaluate candidates](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ftimetaken_yesno_b30ffa76f7.png&w=1200&q=75)

Given the weight placed on single variables (e.g., experience at a top firm), how quickly recruiters make judgments isn’t surprising. But might they be more accurate if they slowed down? **It turns out that spending more time on resume evaluations, notably >45 seconds, is associated with more accurate predictions — just spending 15 more seconds appears to increase accuracy by 34%.[7](#user-content-fn-7) It could be that encouraging recruiters to slow down might result in more accurate resume screening.**

![Recruiter accuracy vs time taken](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frecruiter_accuracytime_f308373dfa.png&w=1200&q=75)

## Can AI do better?

As a gaggle of technologists and data geeks, we tested whether algorithms could quiet the noise and inconsistencies in recruiters’ predictions.

We trained two local, off-the-rack machine-learning models.[8](#user-content-fn-8)

Just like human recruiters, the models were trained to predict which candidates would pass technical interviews. The training dataset was drawn from interviewing.io and included anonymized resume data (years of experience, whether they had worked at a top firm, and whether they had attended a top 10 school for either grad or undergrad), candidates’ race and gender, and interview outcomes.[9](#user-content-fn-9)

**Despite the very limited types of data we input into both models, when presented with out-of-sample candidate profiles, both models made predictions more accurately than human recruiters.**

Random Forest was somewhat more accurate than recruiters when predicting lower performing candidates. XGBoost, however, was more accurate across the board than both the Random Forest model AND recruiters.

![predictions vs interviewing.io performance](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fprediction_comp_e3cf3757f5.png&w=1200&q=75)

## Where does this leave us?

In this section, when we say “we,” we are speaking as interviewing.io, not as the researchers involved in this study. Just FYI.

### Advice for candidates

At interviewing.io, we routinely get requests from our users to add resume review to our list of offerings. So far, we have declined to build it. Why? Because we suspected that recruiters, regardless of what they say publicly, primarily hunt for name brands on your resume. Therefore, highlighting your skills or acquiring new skills is unlikely to make a big difference in your outcomes.

We are sad to see the numbers back up our intuition that it mostly is about brands.[10](#user-content-fn-10) As such, here’s an actionable piece of advice: maintain a healthy skepticism when recruiters advise you to grow your skill set. Acquiring new skills will very likely make you a better engineer. But it will very likely NOT increase your marketability.

If enhancing your skill set won’t help, what can you do to get in front of companies? We’re in the midst of a brutal market, the likes of which we haven’t seen since the dot-com crash in 2000. According to anecdotes shared in our Discord community, even engineering managers from FAANGs are getting something like a 10% response rate when they apply to companies online. If that’s true, what chance do the rest of us have?

We strongly encourage anyone looking for work in this market, especially if you come from a non-traditional background, to stop spending energy on applying online, full stop. Instead, reach out to hiring managers. The numbers will be on your side there, as relatively few candidates are targeting hiring managers directly. We plan to write a full blog post on how to do this kind of outreach well, but this CliffsNotes version will get you started:

- Get a LinkedIn Sales Navigator account
- Make a target list of hiring managers at the companies you’re interested in
- Figure out their emails (you can use a tool like RocketReach), and send them something short and personalized. Do not use LinkedIn. The same way that you don’t live in LinkedIn, eng managers don’t either. Talk about the most impressive thing you’ve built. Ask them about their work, if you can find a blog post they’ve written or a project they’ve worked on publicly. Tie those two things together, and you’ll see a much higher response rate. Writing these personalized emails takes time, of course, but in this market, it’s what you need to do to stand out.

### Advice for recruiters

We know that recruiting is a tough job, especially in the current climate, where there are more applicants than ever and fewer recruiters to parse through them. So, it rationally makes sense to us that a recruiter would spend no more than 30 seconds per resume and focus primarily on looking for top brands.

We hope, though, that this piece may have given a measure of pause about your approach, and we’d like to leave you with two actionable pieces of advice. First, if you do nothing else, please slow down. As you saw above, taking just 15 extra seconds to read a resume could improve your accuracy by 34%.[11](#user-content-fn-11)

Our second piece of advice is this. Freada Kapor Klein from Kapor Capital coined the term “distance traveled” more than two decades ago. It refers to what someone accomplished, in the context of where they started. For instance, Kapor Klein recommends that, in their admissions processes, universities should consider not just the number of AP tests a candidate has passed but the number of AP tests divided by the total number offered at their high school. For example, if an applicant took 5 AP tests and their school offered 27, that paints a very different picture from another applicant who also took 5 AP tests when that’s the total number offered at their school. Kapor Capital uses distance traveled as one of their metrics for determining which entrepreneurs to fund. One can easily apply this concept to hiring as well.

Take a look at the resume below. "John" (name has been changed; scrubbed resume shared with permission) studied chemical engineering and worked his way into software engineering by starting as a service engineer focused on pen testing. In the meantime, he completed a bootcamp, attended the Bradfield School of Computer Science (a school dedicated to teaching computer science at a depth beyond what many university programs, and certainly most bootcamps, offer), and ended up with a senior title in just three years.

John was consistently rated poorly by recruiters but is one of the top performers on interviewing.io.

![anonymized resume](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fanonymized_resume_55980a6854.png&w=1200&q=75)

It takes just a bit more time, so please spend a little longer reading resumes, and evaluate candidates’ achievements in the context of where they came from. Think about the denominator. But don’t think for a moment that we recommend that you lower the bar — absolutely not. On interviewing.io, we regularly see candidates like John objectively outperforming their FAANG counterparts.

### What this means for our industry

The last time I did this research, I wrote about how being bad at judging resumes isn’t anything to be ashamed about and that comes down to the resume itself being a low-signal and not-very-useful document.

I held that same opinion for the last decade (and even wrote a [recent post about how AI can’t do recruiting](https://interviewing.io/blog/why-ai-cant-do-hiring))… right up until we ran this study and successfully built two ML models that outperformed recruiters.

So, I stand corrected.

As you saw above, both models were limited – they were looking at the same types of features that recruiters do when they quickly scan a resume, certainly fewer features than recruiters have access to. But, despite that, the AI models still outperformed humans. What happens then, if you can build a model that behaves like a recruiter who really slows down and reads everything? These results make me believe that resumes do carry some signal, and you can uncover it if you carefully read what people write about their jobs and themselves and also analyze how they write it. Unfortunately, this takes more time and effort to uncover than most human recruiters are able to devote. And, in retrospect, that’s a good task for AI. Though we haven’t built a model like that for this post, I’m optimistic that we may be able to do it in the future.

As I said in the AI piece I linked above, in order for AI to do useful recruiting work, rather than just perpetuating the biases that human recruiters hold, it needs a data set that contains some objective measure of performance. Most recruiting AI models today do one of three things: glorified keyword matching, training on what recruiters prefer (the outcome is whether a recruiter would want to talk to the candidate, NOT whether the candidate is good), or live on top of existing tools like ChatGPT (which [we recently showed doesn’t perform very well and is biased against non-traditional candidates](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)). These three approaches just result in the wrong thing being done, faster.

I hope that, in the not too distant future, we can use AI to make less-biased decisions, using meaningful performance data. And I hope that this type of AI solution can get adoption among the recruiting community.

1. Participating technical recruiters were paid a base rate and then received additional $$ for each accurate prediction. [↩](#user-content-fnref-1)
2. Different roles have different requirements. To correct for that, we asked each candidate to specify which eng role they were applying for: Software Engineer (back-end or full-stack), Mobile Engineer, Front-end Engineer, ML Engineer, Data Engineer, or Engineering Manager. Then we prompted recruiters to evaluate them specifically for that role. If no role was specified by the candidate, the default role to evaluate for was Software Engineer (back-end or full-stack). [↩](#user-content-fnref-2)
3. Top firms = Airbnb, Amazon, Anthropic, AWS, Apple, Asana, Atlassian, Bloomberg LP, Checkr, Coinbase, Coursera, Cruise, Dropbox, Etsy, Facebook, Flexport, GitHub, Google, Gusto, HashiCorp, Instacart, Instagram, Jane Street, Jump Trading, Khan Academy, LinkedIn, Lyft, Medium, Microsoft, Mozilla, Netflix, Oculus, OpenAI, Palantir, Peloton, Pinterest, Postmates, Quora, Reddit, Robinhood, Roblox, Salesforce, Segment, Slack, Snap, Snowflake, SpaceX, Spotify, Square, Stripe, Tesla, Thumbtack, TikTok, Twilio, Twitch, Twitter, Two Sigma, Uber, Udemy, Waymo, Whatsapp, Yelp, and Zoom. [↩](#user-content-fnref-3)
4. We corrected by FAANG & FAANG-adjacent experience (and all of our other variables) before making this statement, i.e., the effect existed for engineers from underrepresented backgrounds who did not have FAANG/FAANG-adjacent companies on their resumes. We expect that recruiters favor underrepresented minority candidates because of guidelines from their employers to focus on sourcing these types of candidates, as part of DEI initiatives. Discussion about the magnitude of this effect and its implications is out of scope of this piece. [↩](#user-content-fnref-4)
5. Interestingly, recruiters might penalize, for example, alternative education. Candidates with only alternative education pathways post-high school — coding bootcamps or digital certifications — appeared to be penalized by recruiters in this study. However, with limited observations (n=11), it’s inconclusive without further study. [↩](#user-content-fnref-5)
6. That field was optional, so most of the reasons recruiters provided were in cases when they said no — presumably because the reasons for saying yes may have seemed self-evident. [↩](#user-content-fnref-6)
7. It’s not that recruiters who generally take their time make more accurate judgements. Any recruiter slowing down might make them better at judging resumes! [↩](#user-content-fnref-7)
8. It’s important to stress that neither algorithm was custom-built. The models, one using a Random Forest algorithm and the other an XGBoost algorithm, are distinct but interrelated approaches akin to [Decision Tree algorithms](https://medium.com/@brandon93.w/decision-tree-random-forest-and-xgboost-an-exploration-into-the-heart-of-machine-learning-90dc212f4948). Decision trees sort data into groups based on features. Random forest algorithms combine multiple decision trees to improve predictions. XGBoost builds multiple decision trees one after another, with each new tree focusing on prediction errors from the previous trees. [↩](#user-content-fnref-8)
9. Training data excluded data in this study. We take user privacy very seriously, and we want to stress that all models were local and anonymized and that no data in this study was shared with cloud LLMs. [↩](#user-content-fnref-9)
10. To see a particularly egregious example of recruiters favoring brands over substance, take a close look at [this fake resume that got a bunch of recruiter responses](https://www.reddit.com/r/recruitinghell/comments/qhg5jo/this_resume_got_me_an_interview/). [And this one too](https://twitter.com/JerryJHLee/status/1778484920593055763). [↩](#user-content-fnref-10)
11. We haven’t proven causality here, but when we just scoped our analysis to the same person, it appeared that taking more time did help (in other words, it’s not just that recruiters who spend more time usually are more accurate; it’s the added time). Still, this is something that merits more work, and we'll try to investigate it causally in the future. [↩](#user-content-fnref-11)

[I love meritocracy, but all the recent anti-DEI rhetoric is bad](/blog/i-love-meritocracy-but-all-the-recent-anti-dei-rhetoric-is-bad)


# [Refuting Bloomberg's analysis: ChatGPT isn't racist. But it is bad at recruiting.](https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist)

By Aline Lerner | Published: April 10, 2024; Last updated: April 16, 2024

*Note: We’ve reached out to Bloomberg, asking them to share their data and clarify their findings, ahead of publishing this piece. If it turns out that they used a different method for statistical significance testing or if we missed something, we’ll gladly retract the part of this post that’s about their results.*

Recently, Bloomberg published an article called “[OpenAI’s GPT is a recruiter’s dream tool. Tests show there’s racial bias](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/).” In this piece, the Bloomberg team ran a clever test where they had ChatGPT review nearly identical resumes with just the names changed to include typically Black, White, Asian, and Hispanic names. Their analysis uncovered racial bias.

Bloomberg had published their numbers on GitHub, so we were able to check their work. When we re-ran the numbers, we saw that they hadn’t done statistical significance testing and that there was, in fact, no evidence of racial bias in Bloomberg's data set. However, when we ran our own tests, we discovered that ChatGPT is indeed bad at judging resumes. It’s not bad because it’s racist. It’s bad because it’s prone to a different kind of bias, the same kind of bias as human recruiters — over-indexing on candidates’ pedigrees: whether they’ve worked at a top company and/or whether they attended a top school. Pedigree can be somewhat predictive (especially where people worked), but ChatGPT is significantly overestimating its importance and doing a disservice to candidates from non-traditional backgrounds as a result.

## The Bloomberg study

Here’s what the team at Bloomberg did (taken verbatim from their piece):

> We used demographically-distinct names as proxies for race and gender, a common practice used to audit algorithms… Altogether we produced 800 demographically-distinct names: 100 names each for males and females who are either Black, White, Hispanic or Asian…
>
> To test for name-based discrimination, Bloomberg prompted OpenAI’s GPT-3.5 and GPT-4 to rank resumes for a real job description for four different roles from Fortune 500 companies: HR specialist, software engineer, retail manager and financial analyst.
>
> For each role, we generated eight nearly-identical resumes using GPT-4. The resumes were edited to have the same educational background, years of experience, and last job title. We removed years of education, as well as any objectives or personal statements.
>
> We then randomly assigned a distinct name from each of the eight demographic groups [Black, White, Hispanic, Asian, and men and women for each] to each of the resumes.
>
> Next, we shuffled the order of resumes, to account for order effects, and asked GPT to rank the candidates.

The authors reported that ChatGPT shows racial bias across all groups, except for retail managers ranked by GPT-4.

More specifically:

> [We] found that resumes labeled with names distinct to Black Americans were the least likely to be ranked as the top candidates for financial analyst and software engineer roles. Those with names distinct to Black women were top-ranked for a software engineering role only 11% of the time by GPT — 36% less frequently than the best-performing group.
>
> The analysis also found that GPT’s gender and racial preferences differed depending on the particular job that a candidate was evaluated for. GPT does not consistently disfavor any one group, but will pick winners and losers depending on the context. For example, GPT seldom ranked names associated with men as the top candidate for HR and retail positions, two professions historically dominated by women. GPT was nearly twice as likely to rank names distinct to Hispanic women as the top candidate for an HR role compared to each set of resumes with names distinct to men. Bloomberg also found clear preferences when running tests with the less-widely used GPT-4 — OpenAI’s newer model that the company has promoted as less biased.

The team also, commendably, [published their results on GitHub](https://github.com/BloombergGraphics/2024-openai-gpt-hiring-racial-discrimination), so we tried our hand at reproducing them. What we found was starkly different from what Bloomberg reported.

Before we get into what we found, here’s what we did.

In their results, Bloomberg published the rates at which both GPT-3.5 and GPT-4 chose each demographic as the top candidate. The Bloomberg analysts ran a lot of trials for each job: ChatGPT was asked 1,000 times to rank 8 resumes for the HR specialist job, for example. And if ChatGPT had gender or racial bias, each group should in general be the top pick 125 times (or 12.5% of the time, given that there were 1000 data points).

Where are we going with this? Stats nerds might have noticed a glaring omission from the Bloomberg piece: statistical significance testing and p-values. Why is that important? Even with 1,000 trials, a perfectly unbiased resume sorter would not give you exactly equal proportions, picking each group precisely 125 times. Instead, purely random variation could mean that one group is picked 112 times and another is picked 128 times, without there actually being any bias. Therefore, you need to run some tests to see if the results you got were by chance or because there truly is a pattern of some kind. Once you run the test, the p-value tells you the probability that a certain set of selection rates are consistent with chance, and in this case, consistent with a random (and, therefore, unbiased) sorting of resumes.

We calculated the p-values for each group[1](#user-content-fn-1). What we found was starkly different from what Bloomberg reported.

## Where the Bloomberg study went wrong

Given the nature of our business, we looked at software engineers first. Here are the results of Bloomberg having run software engineering resumes through GPT-4 for all 8 groups (the column titled “obsfreq”) as well as our calculated p-value.

![](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbloomberg_chi2_gpt4_software_78ea5edfd6.png&w=1200&q=75)

A\_M = Asian man, A\_W = Asian woman, etc. 12.5% would be the expected rate at which a candidate from each group would be the top choice (“expperc”). Out of 1000 candidates, that’s 125 each (“expfreq”). Finally, “obsfreq” is the observed frequency of the top candidate being from each group, taken from Bloomberg’s results.

It’s convention that you want your p-value to be less than 0.05 to declare something statistically significant – in this case, that would mean less than 5% chance that the results were due to randomness. This p-value of 0.2442 is way higher than that. As it happened, we couldn’t reproduce statistical significance for software engineers when using GPT-3.5 either. **Using Bloomberg’s numbers, ChatGPT does NOT appear to have a racial bias when it comes to judging software engineers’ resumes.**[2](#user-content-fn-2) **The results appear to be more noise than signal.**

We then re-ran the numbers for the eight race/gender combinations, using the same method as above. In the table below, you can see the results. TRUE means that there is a racial bias. FALSE obviously means that there isn't. We also shared our calculated p-values. **The TL;DR is that GPT-3.5 DOES show racial bias for both HR specialists and financial analysts but NOT for software engineers or retail managers. Most importantly, GPT-4 does not show racial bias for ANY of the race/gender combinations.**[3](#user-content-fn-3)

| Occupation | GPT 3.5 (Statistically significant? ‖ p-value) | GPT 4 (Statistically significant? ‖ p-value) |
| --- | --- | --- |
| Financial analyst | TRUE ‖ 0.0000 | FALSE ‖ 0.2034 |
| Software engineer | FALSE ‖ 0.4736 | FALSE ‖ 0.1658 |
| HR specialist | TRUE ‖ 0.0000 | FALSE (but it’s close) ‖ 0.0617 |
| Retail manager | FALSE ‖ 0.2229 | FALSE ‖ 0.6654 |

That’s great, right? Well, not so fast. Before we deemed ChatGPT as competent at judging resumes, we wanted to run a test of our own, specifically for software engineers (because, again, that’s our area of expertise). The results of this test were not encouraging.

## How we tested ChatGPT

interviewing.io is an anonymous mock interview platform, where our users get paired with senior/staff/principal-level FAANG engineers for interview practice. We also connect top performers with top-tier companies, regardless of how they look on paper. In our lifetime, we’ve hosted >100k technical interviews, split between the aforementioned mock interviews and real ones. In other words, we have a bunch of useful, indicative historical performance data about software engineers.[4](#user-content-fn-4) So we decided to use that data as a sanity check.

### The setup

We asked ChatGPT (GPT4, specifically *gpt-4-0125-preview*) to grade several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. For each profile, we asked ChatGPT to give the person a coding score between 1 and 10, where someone with a 10 would be a top 10% coder. In order to improve the quality of the response, we asked it to first give its reasoning followed by the coding score.

We want to be very clear here that we did not share any performance data with ChatGPT or share with ChatGPT any information about our users — we just asked it to make value judgments about publicly available LinkedIn profiles. Then we compared those value judgments with our data on our end.

## How ChatGPT performed

There is a correlation between what ChatGPT says and how the coder performed on a real technical screen. The tool performs better than a random guess… but not by much. **To put these results in perspective**, overall, 47% of coders pass the screen. The ChatGPT score can split them into two groups: one that has a 45% chance and one with a 50% chance. So it gives you a bit more information on whether someone will succeed, but not much.

Below are two more granular ways of looking at ChatGPT’s performance. The first is a modified calibration plot, and the second is a ROC curve.

### Calibration plot

In this plot, we take each predicted probability from ChatGPT (e.g., 0.4112) and assign it to one of 10 equally-spaced deciles. Decile 1 is the 10% of profiles with the lowest probability. Decile 10 is the 10% of people with the highest probability.

Then, for each decile, we plot the actual probability of those candidates performing well in interviews (i.e., what portion of them actually passed interviews on interviewing.io). As you can see, the plot is something of a mess — for all deciles that ChatGPT came up with, those candidates actually passed about half the time. The ideal plot (“An excellent model”) would have a much steeper slope, with way fewer passing in the bottom decile than the top decile.

![How ChatGPT predictions compare to actual performance on interviewing.io](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_decile_calibration_overall_added_lines_674eab2b9f.png&w=1200&q=75)

We asked GPT-4 to judge several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. Then we split up its predictions into deciles — 10% buckets and compared to how those users actually performed. A great model would have bad performance in the first decile and then improve sharply and steadily.

### ROC curve

Another way to judge ChatGPT’s performance at this task is to look at a ROC curve. This curve graphs the true positive rate of a model against the false positive rate. It’s a standard way of judging how accurate an ML model is because it lets the viewer see how it performs at different acceptable false positive rates — for cancer diagnostics, you may be OK with a very high false positive rate, for instance. For eng recruiting, you likely will not be!

Related to ROC curves is the AUC, or the area under the curve. A perfect model would have a 100% true positive rate for every possible false positive rate, and so the area under the curve would be 1. A model that’s basically the same as guessing would have a true positive rate that equals the false positive rate (AUC = 0.5). With that in mind, here’s the ROC curve and the AUC for ChatGPT judging resumes — with an overall AUC of about 0.55, it’s only barely better than random guesses.

![When asked to judge resumes, ChatGPT doesn't perform better than chance](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_oneshot_roc_cca5b11e77.png&w=1200&q=75)

We asked GPT-4 to judge several thousand LinkedIn profiles belonging to people who have practiced on interviewing.io before. It performed barely better than guessing.

So, no matter how you measure it, while ChatGPT does not appear to have racial bias when judging engineers’ profiles, it’s not particularly good at the task either.

## ChatGPT is biased against non-traditional candidates

Why did ChatGPT perform poorly on this task? Perhaps it’s because there may not be that much signal in a resume in the first place. But there’s another possible explanation as well.

Years ago, I ran an experiment where I [anonymized a bunch of resumes and had recruiters try to guess which candidates were the good ones](https://interviewing.io/blog/resumes-suck-heres-the-data). They did terribly at this task, about as well as random guessing. Not surprisingly, they tended to over-index on resumes that had top companies or prestigious schools on them. In my data set of candidates, I happened to have a lot of non-traditional, good candidates — people who were strong engineers but didn’t attend a highly ranked school or work at a top company. That threw recruiters for a loop.

It looks like the same thing happened to ChatGPT, at least in part. We went back and looked at how ChatGPT treated candidates with top-tier schools on their LinkedIn profiles vs. those without. It turns out that ChatGPT consistently overestimates the passing rate of engineers with top schools and top companies on their resumes. We also saw that ChatGPT consistently underestimates the performance of candidates without those elite “credentials” on their resumes. Both of these differences are statistically significant. In the graph below, you can see exactly how much ChatGPT overestimates and underestimates in each case.

![GPT-4's bias against non-traditional engineers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fchatgpt_school_and_company_bias_818bd5dfc7.png&w=1200&q=75)

To ChatGPT’s credit, we did not find the same bias when it came to top companies, which is funny because, in our experience, having worked at a top company carries some predictive signal, whereas where someone went to school does not carry much.

## ChatGPT likely isn't racist, but its biases still make it bad at recruiting

In recruiting, we often talk about unconscious bias. Though it’s no longer en vogue, companies have historically spent tens of thousands of dollars on unconscious bias trainings designed to stop recruiters from making decisions based on candidates’ gender and race. At the same time, recruiters were trained to exhibit a different, conscious bias: to actively select candidates from elite schools and top companies.

The same conscious bias against candidates who didn’t attend a top-tier school appears to be codified in ChatGPT.

That decision is rational — in the absence of a better signal, you have to use proxies, and those proxies seem as good as any. Unfortunately, as you can see from these results (and from other studies we’ve done in the past; see the footnote for the full list[5](#user-content-fn-5)), it’s not particularly accurate… and it’s definitely not accurate enough to codify into our AI tools.

In a market where [recruiter jobs are tenuous](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024), where the [dwindling number of recruiters is dealing with more applicants than before](https://www.ashbyhq.com/talent-trends-report/reports/2023-recruiter-productivity-trends-report) and are pressured, more than ever, to make the aforementioned rapid decisions, and where companies are embracing AI as a tempting and productive cost-cutting measure[6](#user-content-fn-6), we’re in rather dangerous territory.

A few months ago, we published a long piece called, “[Why AI can’t do hiring](https://interviewing.io/blog/why-ai-cant-do-hiring)”. The main two points of the piece were that 1) it’s hard to extract signal from a resume because there’s not much there in the first place, and 2) even if you could, you’d need proprietary performance data to train an AI — without that data, you’re doing glorified keyword matching.

Unfortunately, most, if not all, of the AI tools and systems that claim to help recruiters make better decisions do not have this kind of data and are 1) either built on top of GPT (or one of its analogs) without fine-tuning or 2) are glorified keyword matchers masquerading as AI, or both.

Though human recruiters aren’t particularly good at judging resumes, and though we, as a society, don’t yet have a great solution to the problem of effective candidate filtering, it’s clear that off-the-shelf AI solutions are not the magic pill we’re looking for — they’re flawed in the same ways as humans. They just do the wrong thing faster and at scale.

1. We used a Chi-squared goodness of fit test, a type of statistical significance test that you use for discrete data, like yes or no votes on resumes. [↩](#user-content-fnref-1)
2. Another way to see this is to simulate the same process with a perfectly unbiased resume sorter, i.e., a bot that picks from the 8 resumes at random. If you run 1,000 imaginary versions of the Bloomberg experiment, it’s pretty common for a particular group to have a round where they’re the top pick just 11% of the time. This distribution is in the histogram below. This is another way of saying what the p-values are saying: the disparities are consistent with random chance. [INSERT HISTOGRAM] [↩](#user-content-fnref-2)
3. A caveat is that these tests might show evidence of bias if the sample size were increased to, say, 10,000 rather than 1,000. That is, with a larger sample size, the p-value might show that ChatGPT is indeed more biased than random chance. The thing is, we just don’t know from their analysis, and it certainly rules out extreme bias. In fact, the [most recent large-scale resume audit study](https://academic.oup.com/qje/article/137/4/1963/6605934) found that resumes with distinctively Black names were 2.1 percentage points less likely to get callbacks from human recruiters. Based on Bloomberg’s data, ChatGPT’s was less biased against resumes from Black candidates than human recruiters — according to our calculations, in the Bloomberg data set, there was 1.5 percentage point drop. [↩](#user-content-fnref-3)
4. Mock interview performance, especially in aggregate, is very predictive of real interview performance. We have data from both mock and real interviews, spanning ~6 years, and the candidates who’ve done well in mock interviews on our platform have consistently been 3X more likely to pass real interviews. [↩](#user-content-fnref-4)
5. Here is a list of our past studies that show how top schools are not particularly predictive and top companies are only somewhat predictive:

- [Lessons from a year’s worth of hiring data](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data)
   - [We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)
   - [Lessons from 3,000 technical interviews… or how what you do after graduation matters way more than where you went to school](https://interviewing.io/blog/lessons-from-3000-technical-interviews)[↩](#user-content-fnref-5)
6. A significant portion of employers, ranging from 35-55% depending on the source (Zippia, Forbes, USC), are currently using AI to screen job candidates. The adoption of AI in hiring appears to be particularly high among large enterprises and recruiting firms. Given that large enterprises see a disproportionately high volume of candidates, the % of candidates who are screened by AI is likely much higher than the 35-55% number. [↩](#user-content-fnref-6)

### [Partition Equal Subset Sum](/questions/partition-equal-subset-sum)

[Given an array of positive numbers, determine if the array can be split such that the two partition sums are equal.](/questions/partition-equal-subset-sum)


# [After a lot more data, technical interview performance really is kind of arbitrary.](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary)

By Aline Lerner | Published: October 12, 2016; Last updated: March 28, 2024

[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because [resumes suck](https://interviewing.io/blog/resumes-suck-heres-the-data) and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.

In February of 2016, we published a [post about how people’s technical interview performance, from interview to interview, seemed quite volatile](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data/). At the time, we just had a few hundred interviews to draw on, so as you can imagine, we were quite eager to rerun the numbers with the advent of more data. **After drawing on over a thousand interviews, the numbers hold up. In other words, technical interview outcomes do really seem to be kind of arbitrary.**

When an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.

After every interview, interviewers rate interviewees on a few different dimensions, including technical ability. Technical ability gets rated on a scale of 1 to 4, where 1 is “poor” and 4 is “amazing!” ([you can see the feedback form here](https://strapi-iio.s3.us-west-2.amazonaws.com/9fdaa_new_interviewer_feedback_circled_2d4ba08149.png)). On our platform, a score of 3 or above has generally meant that the person was good enough to move forward.

## Performance from interview to interview really is arbitrary

If you’ve read our [first post](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data) on this subject, you’ll recognize the visualization below. For the as yet uninitiated, every tiny human icon represents the mean technical score for an individual interviewee who has done 2 or more interviews on the platform. The y-axis is standard deviation of performance, so the higher up you go, the more volatile interview performance becomes. If you hover over each , you can drill down and see how that person did in each of their interviews. Anytime you see bolded text with a dotted underline, you can hover over it to see relevant data viz. Try it now to expand everyone’s performance. You can also hover over the labels along the x-axis to drill into the performance of people whose means fall into those buckets.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_22_at_11_55_47_2x_1ec095b1a8.png)

As you can see, roughly 20% of interviewees are consistent in their performance (down from 25% the last time we did this analysis), and the rest are all over the place. If you look at the graph above, despite the noise, you can probably make some guesses about which people you’d want to interview. **However, keep in mind that each represents a *mean*. Let’s pretend that, instead, you had to make a decision based on just one data point. That’s where things get dicey**.[1](#user-content-fn-1) For instance:

- Many people who scored at least one 4 also scored at least one 2.
- And as you saw above, a good amount of people who scored at least one 4 also scored at least one 1.
- If we look at high performers (mean of 3.3 or higher), we still see a fair amount of variation.
- Things get really murky when we consider “average” performers (mean between 2.6 and 3.3).

## What do the most volatile interviewees have in common?

In the plot below, you can see interview performance over time for interviewees with the highest standard deviations on the platform (the cutoff we used was a standard dev of 1 or more, and this accounted for roughly 12% of our users). Note that the mix of dashed and dotted lines is purely visual — this way it’s easier to follow each person’s performance path.

**So, what do the most highly volatile performers have in common? The answer appears to be, well, nothing.** About half were working at top companies while interviewing, and half weren’t. Breakdown of top school was roughly 60/40. And years of experience didn’t have much to do with it either — a plurality of interviewees having between 2 and 6 years of experience, with the rest all over the board (varying between 1 and 20 years).

**So, all in all, the factors that go into performance volatility are likely a lot more nuanced than the traditional cues we often use to make value judgments about candidates.**

## Why does volatility matter?

I discussed the implications of these findings for technical hiring at length in the last post, but briefly, a noisy, non-deterministic interview process does no favors to either candidates or companies. Both end up expending a lot more effort to get a lot less signal than they ought, and in a climate where software engineers are at such a premium, noisy interviews only serve to exacerbate the problem.

But beyond micro and macro inefficiencies, I suspect there’s something even more insidious and unfortunate going on here. Once you’ve done a few traditional technical interviews, the volatility and lack of determinism in the process is something you figure out anecdotally and kind of accept. And if you have the benefit of having friends who’ve also been through it, it only gets easier. What if you don’t, however?

In a previous post, we talked about how [women quit interview practice 7 times more often](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) than men after just one bad interview. It’s not too much of a leap to say that this is probably happening to any number of groups who are underrepresented/underserved by the current system. In other words, though it’s a broken process for everyone, the flaws within the system hit these groups the hardest… because they haven’t had the chance to internalize just how much of technical interviewing is a game. More on this subject in our next post!

## What can we do about it?

So, yes, the state of technical hiring isn’t great right now, but here’s what we can say. **If you’re looking for a job, the best piece of advice we can give you is to really internalize that interviewing is a numbers game.** Between the kind of volatility we discussed in this post, [impostor syndrome](https://interviewing.io/blog/own-interview-performance), [poor evaluation techniques](https://interviewing.io/blog/resumes-suck-heres-the-data), and how hard it can be to get meaningful, realistic practice, it takes a lot of interviews to find a great job.

And if you’re hiring people, in the absence of a radical shift in how we vet technical ability, we’ve learned that drawing on aggregate performance is much more meaningful than a making such an important decision based on one single, arbitrary interview. Not only can aggregative performance help correct for an uncharacteristically poor performance, but it can also weed out people who eventually do well in an interview by chance or those who, over time, simply up and memorize *Cracking the Coding Interview*. At interviewing.io, **even after just a handful of interviews, we have a much better picture of what someone is capable of and where they stack up than a single company would after a single interview, and aggregate data tells a much more compelling, repeatable story than one, arbitrary data point**.

*Huge thanks to [Ian Johnson](https://twitter.com/enjalot), creator of [d3 Building Blocks](http://blockbuilder.org/), who made the graph entitled "Standard Dev vs. Mean of Interviewee Performance" (the one with the icons) as well as all the visualizations that go with it.*

1. At this point you might say that it’s erroneous and naive to compare raw technical scores to one another for any number of reasons, not the least of which is that one interviewer’s 4 is another interviewer’s 2. For a comprehensive justification of using raw scores comparatively, please check out the [appendix](https://interviewing.io/blog/technical-interview-performance-is-kind-of-arbitrary-heres-the-data) to our previous post on this subject. Just to make sure the numbers hold up, I reran them, and this time, our R-squared is even higher than before (0.41 vs. 0.39 last time). [↩](#user-content-fnref-1)

### [Longest Substring Without Repeating Characters](/questions/longest-substring-without-repeating-characters)

[Given a string s, find the length of the longest substring without repeating characters.](/questions/longest-substring-without-repeating-characters)


# [How much have 2022 layoffs affected engineers vs. other departments? We dug into the data to find out.](https://interviewing.io/blog/2022-layoffs-engineers-vs-other-departments)

By Aline Lerner | Published: October 12, 2022; Last updated: December 7, 2024

Over the past few months, I’ve seen a number of fear-mongering pieces in the press about how the recession is driving tech layoffs and how tech employees (and engineers specifically) are losing their leverage as a result. Here are a few recent examples:

- [The balance of power is shifting in the tech industry](https://www.cnn.com/2022/07/07/tech/tech-layoffs-workers-silicon-valley/index.html) (CNN)
- [The hot market for tech employees is ending, giving the power back to companies like Meta and Google to set and reduce pay](https://www.businessinsider.com/tech-layoffs-salary-cuts-workers-2022-8) (Business Insider)
- [Snap’s Web3 team among 20% workforce reduction after first single-digit-growth quarter since going public](https://fortune.com/crypto/2022/09/01/snap-shutters-web3-team-layoffs-tech-evan-spiegel/) (Fortune)

Articles like these drive engineers to speculate [on Reddit](https://www.reddit.com/r/ExperiencedDevs/comments/um58oq/have_any_engineers_been_affected_by_recent_layoffs/), in the internet equivalent of hushed whispers, about whether they’re next. Unfortunately, none of the pieces I’ve seen have bothered to draw a distinction between “tech” and “software engineering,” and many even use these terms interchangeably (like the CNN piece above).

The problem is that “tech” can mean anyone working at a tech company. You’re an engineer? Of course, you’re tech. You do ops? Great, you’re tech. You do marketing? You, too, are tech! These are all critical roles, and what I take umbrage with isn’t the decision to label non-engineers as tech employees. It’s deliberately misleading your audience by implying that “tech” refers to engineers specifically.

I don’t like imprecision, and I *really* don’t like fear-mongering. So, we at interviewing.io decided to dig into the data to see if engineers do indeed have a reason to fear. The TL;DR is that while, yes, tech layoffs have affected engineers somewhat, they are one of the least affected departments. While the existence of layoffs, in concert with eng hiring slowing down in general, does mean that engineers are losing some leverage in the market, it’s nowhere near as dire as the press makes it out to be.

## The best source for layoff data

At some point in the last 2.5 years, you’ve probably visited layoffs.fyi. It was launched by [Roger Lee](https://www.rogerlee.com/) in February 2020, just when concerns that this COVID-19 thing *might* affect the economy went mainstream. The site does exactly what it sounds like – it tracks layoffs at tech companies. Every time a company conducts a public round of layoffs, it gets added to a growing list. Each entry includes a layoff count, and a small subset of entries include a list of actual people who were let go, as well as some info about them (name, LinkedIn, geography, title, and so on). These layoff lists are usually in a Google doc but sometimes in something more fancy, like this [dedicated website for Shopify layoffs](https://weworkedatshop.com).

![Table of companies and number of people laid off](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FLayoff_lists_on_layoffs_fyi_1_2ad6abbed9.png&w=1920&q=75 "# of people laid off by company")

As far as I know, even with its limitations, layoffs.fyi is the best source for layoff data, and because the individual layoff lists actually break out layoffs by function, it’s the natural place to go if you want to disambiguate “tech” into specific departments.

So that’s where we looked. Before we talk about what we did, I’d like to call out one big limitation of the layoff lists on layoffs.fyi.

## One big limitation of the data: layoff lists are opt-in

From what we can tell, the layoff lists on layoffs.fyi are opt-in. In other words, former employees decide if they want to be included. Sometimes the difference between the layoff count reported in the press and the length of the list is substantial. Peloton, for instance, had 2800 layoffs, but their list only had 400 people.

If job losers’ willingness to opt in depends on their role, they might be overrepresented in the layoff data. Recruiters, for instance, might be more willing to sign on since they know the value of getting their names out there. Our fix is to calculate opt-in rates for each role and use these as weights in the analysis – just like when the Census upweights groups who are less likely to respond to a survey. For more detail on how we did that, please see the section called “Appendix: Methods” below.

## The questions we wanted to answer

To disambiguate “tech layoffs” into actual departments and break out how each department was affected, we needed to answer the following questions:

1. **What % of employees got laid off overall?**
2. **What’s the breakdown of layoffs by department?**
3. **How hard was each department hit?**

In the next section, you can see what we learned. If you’re curious how we got to those results, see the section called “Appendix: Methods” at the bottom of the post.

## The results, or layoffs by the numbers

### What % of employees got laid off overall?

Based on the companies we looked at, at companies where layoffs happened, about 19% of employees were laid off.

### What’s the breakdown of layoffs by department?

Below, you can see the % of total layoffs that each department constituted. In other words, of the layoffs that happened in 2022, 20% were salespeople, 5% were engineers, 3% were finance, and so on … and that a third of newly unemployed tech workers come from sales and product.

![Chart showing % of total layoffs by department in 2022](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fof_total_layoffs_each_department_constituted_in_2022_68b8b4e6d6.png&w=1920&q=75 "Layoffs by department")

### How hard was each department hit?

The graph above doesn’t give the risk of getting laid off, however. All departments don’t have the same headcount. For instance, according to LinkedIn data, engineers make up about 20% of a company’s total headcount on average, whereas recruiters only make up 2%, about 10X less. In other words, if each department were to be hit equally, we’d expect engineers to be laid off about 10X more often than recruiters. That’s clearly not what’s going on.

To get the full picture, we need to correct for department size and see how hard each department was actually hit, which you can see in the graph below.

![Chart showing % of workers laid off, by department](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2022_layoffs_department_007b58958a.png&w=1920&q=75 "Percentage of workers laid off, by department")

These results show exactly why using the label “tech” isn’t good enough – there is a big disparity between different departments. According to our data, almost *half* of HR people and recruiters got laid off, as compared to 10% of engineers and only 4% of salespeople. The salespeople percentage shows the need to correct for department size, given that salespeople constituted the biggest % of overall layoffs.

## What does this mean for engineers?

Although engineers didn’t emerge from this year’s layoffs unscathed, our data shows that lumping everyone together under the “tech” umbrella is misleading. These insights don’t change the fact that the people who lost their jobs, regardless of department, are hurting, and, unfortunately, layoffs are likely far from over.

That said, HR teams and recruiters were hit the hardest by far. Sadly these teams are often the proverbial canary in the coal mine – when a recession hits, hiring, and especially outbound hiring, is the first to get cut. Engineering, marketing, product, and design were all affected somewhat, and per capita, sales was affected least.

So what does this all mean for engineers? In our minds, the kind of anxiety produced by the news articles I referenced in the beginning is unjustified. Moreover, though hiring has clearly slowed down in the past quarter, after surveying our users, we saw that [a healthy amount of companies are still hiring engineers](https://interviewing.io/blog/companies-hiring-engineers-2022).

However, the reality is that engineers are losing some of the leverage they had previously enjoyed. Given this climate, it’s reasonable to expect that more engineers will be competing for fewer open positions than before, which means that:

1. Candidate experience is going to degrade somewhat. Why? Sadly, companies caring about candidate experience are not motivated by doing the right thing. Rather, it’s largely a function of how much leverage labor has in the market. As such, we’re likely to see…
   - The return of homework assignments and coding quizzes early on in the process, even for senior candidates. When good candidates don’t drop out of your process despite having to jump through hoops, the hoops, which are cheaper than interviewing with a human, will return.
   - Interview questions will degrade in quality – when selling in an interview process is less important, companies will put less effort into designing bespoke questions that give candidates a preview of what it’s like to work there and will fall back to doing the easy thing, which is probably leaning more on cookie-cutter Leetcode questions.
2. The bar will go up. Because more candidates are competing for fewer positions, companies will be able to get more picky while still hitting their hiring goals.

<https://interviewing.io/> is both a mock interview platform and an eng hiring marketplace – engineers use us for [technical interview practice](https://interviewing.io/mocks), and top performers get fast-tracked at companies. Companies actually interview our top performers anonymously, right on our platform, and leave feedback after each interview. If the candidate passes, they unmask and move to the next step (typically an onsite). Feedback is both quantitative and qualitative, and in addition to telling us if the candidate passed, companies also rate them on technical ability, communication ability, and problem solving ability. Technical ability tends to be most predictive and weighed the most heavily.

As such, to check our read on the market, we averaged the technical scores for successful interviews over the last few quarters to see where the bar has been and where it is now. The results are… compelling.

![Chart showing how the engineering 'bar', in real interviews on interviewing.io, has risen over time](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FThe_engineering_bar_over_time_on_interviewing_io_1_cb7162b724.png&w=1920&q=75 "The engineering bar")

## Appendix: Methods

Below is the list of all companies (either US-based or with a significant number of US employees) with layoff lists, through 7/26/22 with Snap (8/31/22) added on. Sadly, there have been more layoffs in the last few months, but at some point we made the call to stop adding new ones so we could actually get this post published. That said, we made the call to include Snap (8/31/22) as a late addition because they had enough layoffs to potentially change the results. Based on layoffs.fyi data, these companies comprise about 15% of tech companies who employed people in the U.S. and had at least one round of layoffs in 2022.

| Date | Company |
| --- | --- |
| 8/31 | Snap |
| 7/26 | Shopify |
| 7/20 | Capsule |
| 7/19 | Olive |
| 7/14 | The Mom Project |
| 7/13 | Tonal |
| 7/7 | Cedar |
| 7/7 | Next Insurance |
| 6/24 | Feather |
| 6/23 | Netflix |
| 6/22 | MasterClass |
| 6/21 | Tray.io |
| 6/16 | JOKR |
| 6/15 | Weee! |
| 6/14 | Redfin |
| 6/14 | Coinbase |
| 6/13 | Studio |
| 6/13 | Automox |
| 6/9 | Stitch Fix |
| 6/7 | ID.me |
| 6/6 | Dutchie |
| 6/3 | Coterie |
| 6/3 | Policygenius |
| 5/13 | Replicated |
| 5/31 | Tomo |
| 5/31 | BookClub |
| 5/27 | Terminus |
| 5/25 | Bolt |
| 5/25 | PeerStreet |
| 5/23 | Klarna |
| 5/12 | Section4 |
| 5/4 | Cameo |
| 5/4 | Colossus |
| 5/4 | Ideoclick |
| 4/27 | Robinhood |
| 4/25 | Clyde |

For each of the companies above, we did the following to get layoff counts by department:

1. Used LinkedIn to get the total number of U.S. employees at each company. In cases where a company had both U.S. and non-U.S. employees (e.g. Shopify), we looked at what portion of total employees were located in the U.S. and then adjusted the layoff count proportionately.
2. Went through each company’s layoff list and tagged all employees with their department. We included the following departments: Software Engineering, Data Science & Analysis, Product & Design, Marketing, Sales/Account Management/Customer Success, Recruiting, HR (Non-Recruiting), Finance, and Operations.
3. Because layoff lists are opt-in (see the section above called “One big limitation of the data: layoff lists are opt-in” for more detail), we needed another source of truth, and LinkedIn seemed like the best bet. As such, for each department:
   - For a set of representative companies, we searched LinkedIn for the list of people who had left the company in 2022. We chose a subset of companies that included public companies, large startups, and small startups.
   - We compared that count to the count for that department from (2) and used the difference for each department as a multiplier on the number of people in (2). When we did this, we ended up with the numbers below – each bar is our calculated probability that someone from this department would actually opt in to be included on a layoff list.

![Chart showing the likelihood of people listing themselves on a layoff list, by department](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FLikelihood_of_people_listing_themselves_on_a_layoff_list_by_department_de35c9da76.png&w=1920&q=75 "How likely is it that people self-report being laid off?")

Once we had layoff counts by department, we also needed to know how many people worked in each department so we could figure out how hard each department was hit. For instance, Stitch Fix has about 4500 employees in the U.S. Of those, 238 are engineers. Therefore, at Stitch Fix, engineers comprise about 5% of employees. To do that, we used the same representative subset of companies as above and cross-referenced LinkedIn to see how many people worked in each department and what % of total headcount that department constituted. After we got all the numbers for the engineering department at our slice of companies, we saw that, on average, engineers constitute about 11% of employees. We repeated this process for each department. This is important information – without knowing the relative size of each department, we wouldn’t be able to reason about how each department was affected.

*Thank you to Sam Jordan, Liz Graves, Santiago Munoz, the broader interviewing.io team, David Holtz, and Maxim Massenkoff for their help with this piece.*

### [Right View Of Binary Tree](/questions/right-view-of-binary-tree)

[Given the root of a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.](/questions/right-view-of-binary-tree)


# [Exactly what to say when recruiters ask you to name the first number… and other negotiation word-for-words](https://interviewing.io/blog/negotiate-salary-recruiter)

By Aline Lerner | Published: August 15, 2018; Last updated: November 2, 2024

There are a lot of resources out there that talk about salary negotiation but many tend to skew a bit theoretical. In my experience, one of the hardest things about negotiating your salary is knowing what to say in tough, ambiguous situations with a power balance that’s not in your favor. What’s OK? What’s rude? What are the social norms? And so on.

Before I started [interviewing.io](https://interviewing.io/), I’ve worked as a software engineer, an in-house recruiter, and an agency recruiter, so I’ve literally been on all sides of the negotiating table. For the last few years, I’ve been guest-lecturing MIT’s 6.UAT, a class about technical communication for computer science majors. Every semester, negotiation is one of the most-requested topics from students. In this post, I’m sharing the content of that lecture, which is targeted toward students, but has served seasoned industry folks just as well. You’re never too young or too old to advocate for yourself.

Btw, if you don’t like reading and prefer long, rambly diatribes in front of an unsightly glass wall, I covered most of this material (and other stuff) in a webinar I did with the fine people at Udacity (where I used to run hiring) a few months ago. So, pick your poison.

## Why negotiate my salary at all, especially if I’m junior?

If you’re early in your career, you might say that negotiation isn’t worth the hassle — after all, junior roles have pretty narrow salary bands. There are a few reasons this view is short-sighted and wrong. First, though it’s pretty unlikely in the grand scheme of things, if you’re applying to a startup, there might come a magical day when your equity is worth something. This is especially true if you’re an early employee — with a good exit, a delta of a few tenths of a percent might end up being worth a down payment on a home in San Francisco.

But, let’s get real, your equity is likely worthless (except interviewing.io’s equity… that’s totes gonna be worth something), so let me give you a better, more immediate reason to learn to haggle early in your career, precisely because that’s when the stakes are low. Humans are frighteningly adaptable creatures. Scared of public speaking? Give 3 talks. The first one will be gut-wrenchingly horrific, the stuff of nightmares. Your voice will crack, you’ll mumble, and the whole time, you’ll want to vomit. The next one will be nerve-wracking. The last one will mostly be OK. And after that, you’ll be just fine. Same thing applies to approach anxiety, mathematical proofs, sex, and, you guessed it, salary negotiation!

So, make all the awkward, teeth-cringing mistakes now, while it doesn’t matter, and where failure will cost you $5K or $10K a year. Because the further along you get in your career, the bigger the upside will be… and the bigger the downside will be for not negotiating. Not only will the salary bands be wider for senior roles, but as you get more senior, more of your comp comes from equity, and equity has an even wider range for negotiating. Negotiating your stock well can make 6-figure differences and beyond (especially if you apply some of these same skills to negotiating with investors over term sheets, should you ever start your own company)… so learn these skills (and fail) while you’re young, because the older you get, the harder it’s going to be to start and the more high-stakes it’s going to be.

So, below, as promised, I’ll give you a few archetypal, stress-inducing situations and what to say, word-for-word in each one. But first, let me address the elephant in the room…

## Will my offer be rescinded if I try to negotiate my salary?

As I mentioned earlier, this blog post is coming out of a lecture I give at MIT. Every semester, I start the negotiation portion of the lecture with the unshakeable refrain that no one will ever rescind your offer for negotiating. Last semester was different, though. I was just starting to feel my oats and get into my talk (the negotiation piece comes about halfway through) and smugly recited the bit about offers never being rescinded, followed by my usual caveat… “unless you act like a douche while negotiating.” Then, a hand shot up in the back of the room. Ah ha, I thought to myself, one of the non-believers. Ready to placate him, I called on the gentleman in the back.

“My offer got rescinded for negotiation.”

The class broke out into uproarious laughter. I laughed too. It was kind of funny… but it was also unnerving, and I wanted to get to the bottom of it.

“Were you a giant jerk when you negotiated?”

“Nope.” Shit, OK, what else can I come up with…

“Were you applying at a really small company with maybe one open role?” I asked, praying against hope that he’d say yes.

“Yes.”

“Thank god.”

So, there’s the one exception I’ve found so far to my blanket statement. After working with hundreds and hundreds of candidates back when I was still a recruiter, I had never heard or seen an offer get rescinded (and none of my candidates acted like douches while negotiating, thank god), until then. So, if you’re talking to a super small company with one role that closes as soon as they find someone, yes, then they might rescind the offer.

But, to be honest, and I’m not just saying this because I was wrong in front of hundreds of bloodthirsty undergrads, an early startup punishing a prospective employee for being entrepreneurial is a huge red flag to me.

OK, so, now onto the bit where I tell you exactly what to say.[1](#user-content-fn-1)

## What to say when asked to name the first number

There will come a time in every job search where a recruiter will ask you about your compensation expectations. This will likely happen very early in said search, maybe even during the first call you’ll ever have with the company.

I think this is a heinous, illaudable practice fraught with value asymmetry. Companies know their salary ranges and roughly what you’re worth to them before they ever talk to you (barring phenomenal performance in interviews which kicks you into a different band). And they know what cost of living is in your area. So they already have all the info they need about you, while you have none about them or the role or even your market value. Sure, there are some extenuating circumstances where you are too expensive, e.g. you’re like an L6 at Google and are talking to an early stage startup that can only afford to pay you 100K a year in base, but honestly even in that situation, if the job is cool enough and if you have the savings, you might take it anyway.

So, basically, telling them something will only hurt you and never help you. So don’t do it. Now, here’s exactly what to say when asked to name the first number.

At this point, I don’t feel equipped to throw out a number because I’d like to find out more about the opportunity first – right now, I simply don’t have the data to be able to say something concrete. If you end up making me an offer, I would be more than happy to iterate on it if needed and figure out something that works. I also promise not to accept other offers until I have a chance to discuss them with you.

TADA!

## What to say when you’re handed an exploding offer

Exploding offers, in my book, are the last bastion of the incompetent. The idea goes something like this… if we give a candidate an aggressive deadline, they’ll have less of a chance to talk to other companies. Game theory for the insipid.

Having been on the other side of the table, I know just how arbitrary offer deadlines often are. Deadlines make sense when there is a limited number of positions and applicants all come in at the same time (e.g. internships). They do not make any sense in this market, where companies are perpetually hiring all the time — therefore it’s an entirely artificial construct. Joel Spolsky, the creator of Trello and Stack Overflow, had something particularly biting to say on the matter of exploding offers many years ago (the full post, [Exploding Offer Season](https://www.joelonsoftware.com/2008/11/26/exploding-offer-season/), is really good):

*“Here’s what you’re thinking. You’re thinking, well, that’s a good company, not my first choice, but still a good offer, and I’d hate to lose this opportunity. And you don’t know for sure if your number one choice would even hire you. So you accept the offer at your second-choice company and never go to any other interviews. And now, you lost out. You’re going to spend several years of your life in some cold dark cubicle with a crazy boss who couldn’t program a twenty out of an ATM, while some recruiter somewhere gets a $1000 bonus because she was better at negotiating than you were.”*

Even in the case of internships, offer deadlines need not be as aggressive as they often are, and I’m happy to report that many college career centers have taken stands against exploding offers. Nevertheless, if you’re not a student or if your school hasn’t outlawed this vile practice, here’s exactly what to say if it ever happens to you.

I would very much appreciate having a bit more time. I’m very excited about Company X. At the same time, choosing where I work is extremely important to me. Of course, I will not drag things out, and I will continue to keep you in the loop, but I hope you can understand my desire to make as informed of a decision as possible. How about I make a decision by…?

## The reverse used car salesman… or what to say to always get more

At the end of the day, the best way to get more money is to have other offers. I know, I know, interviewing sucks and is a giant gauntlet-slog, but in many cases, having just one other offer (so, I don’t know, spending a few extra days of your time spread over a few weeks) can get you at least $10K extra. It’s a pretty rational, clear-cut argument for biting the slog-bullet and doing a few more interviews.

One anecdote I’ll share on the subject goes like this. A few years ago, a close friend of mine who’s notoriously bad at negotiation and hates it with a passion was interviewing at one of the big 4 companies. I was trying to talk him into getting out there just a little bit, for the love of god, and talk to at least one more company. I ended up introducing him to a mid-sized startup where he quickly got an onsite interview. Just mentioning that he had an onsite at this company to his recruiter from the bigco got him an extra $5K in his signing bonus.

Offers are, of course, better than onsites, but in a pinch, even onsites will do… because every onsite increases your odds of not accepting the offer from the company you’re negotiating with. So, let’s say you do have some offers. Do you reveal the details?

The answer is that it depends. If the cash parts of the offers you have are worth more than the one you have in hand, then you can reveal the details. If they’re worth more in total but less in cash, it’s a bit dicier because equity at smaller companies is kind of worthless… you can still use it as leverage if you tell the story that that equity is worth more to YOU, but that’s going to take a bit more finesse, so if you’ve never negotiated before, you might want to hold off.

If the cash part of your equity is not worth more, it’s sufficient to say you have offers and when pressed, you can simply say that you’re not sharing the details (it’s ok not to share the details).

But whether you reveal details or not, here’s the basic formula for getting more. See why I call it the reverse used car salesman?

I have the following onsites/offers, and I’m still interviewing at Company X and Company Y, but I’m really excited about this opportunity and will drop my other stuff and **SIGN TODAY** if…

So, “if” what? I propose listing 3 things you want, which will typically be:

- Equity
- Salary
- Signing/relocation bonus

The reason I list 3 things above isn’t because I expect you’ll be able to get all 3, but this way, you’re giving the person you’re negotiating with some options. In my experience, you’ll likely get 2 out of the 3.

So, what amounts should you ask for when executing on the reverse used car salesman? It’s usually easier to get equity and bonuses than salary (signing bonus is a one-off rather than something that repeats every year). Therefore, it’s not crazy to ask for 1.5X-2X the equity and an extra 10-15% in salary. For the bonus portion, a lot depends on the size of the company, but if you’re talking to a company that’s beyond seed stage, you can safely ask for at least 20% of your base salary as a signing bonus.[2](#user-content-fn-2)

What if the company says no to all or most of these and are a big enough brand to where you don’t have much of a leg to stand on? You can still get creative. One of our users told me about a sweet deal he came up with — he said he’d sign today if he got to choose the team he could join and had a specific team in mind.

## Other negotiation resources

As I mentioned at the beginning of this post, there are plenty of blog posts and resources on the internets about negotiation, so I’ll just mention two of my favorites. The first is a riveting, [first-hand account of negotiation adventures](https://www.freecodecamp.org/news/how-not-to-bomb-your-offer-negotiation-c46bb9bc7dea) from one of my favorite writers in this space, Haseeb Qureshi. In his post, Haseeb talks about how he negotiated for a 250K (total package) offer with Airbnb and what he learned along the way. It’s one of the most honest and thoughtful accounts of the negotiation process I’ve ever read.

The second post I’ll recommend is a [seminal work in salary negotiation by Patrick McKenzie](https://www.kalzumeus.com/2012/01/23/salary-negotiation/) (patio11 on Hacker News, in case that’s more recognizable). I read it back when I was still an engineer, and it was one of those things that indelibly changed how I looked at the world. I still madly link anyone and everyone who asks me about negotiation to this piece of writing, and it’s still bookmarked in my browser.

If you’re an interviewing.io user and have a job offer or five that you’re weighing and want to know exactly what to say when negotiating in your own nuanced, unique situation, please email me, and I’ll whisper sweet, fiscal nothings in your ear like a modern-day Cyrano de Bergerac wooing the sweet mistress that is capitalism.[3](#user-content-fn-3)

1. If you’re interviewing at interviewing.io, USE THESE ON ME. IT'LL BE GREAT. And while you’re at it, [use these on me](https://blog.alinelerner.com/how-to-interview-your-interviewers/) as well. [↩](#user-content-fnref-1)
2. Some of the larger tech companies offer huge signing bonuses to new grads (~100K-ish). Obviously this advice is not for that situation. [↩](#user-content-fnref-2)
3. An increasing number of our customers pay us on subscription, so we don't get more money if you do.[4](#user-content-fn-4) And for the ones who don't, salary and recruiting fees typically come out of a different budget. [↩](#user-content-fnref-3)
4. In the early days of interviewing.io, we tried to charge a flat per-hire fee in lieu of a percentage of salary, precisely for this reason -- we wanted to set ourselves up as an entirely impartial platform where lining up with our candidates' best interests was codified into our incentive structure. Companies were pretty weirded out by the flat fee, so we went back to doing percentages, but these days we're moving over as many of our customers to subscription as possible -- it's cheaper for them, better for candidates, and I won't lie that I like to see that recurring revenue. [↩](#user-content-fnref-4)


# [The technical interview practice gap, and how it keeps underrepresented groups out of software engineering](https://interviewing.io/blog/technical-interview-practice-gap)

By Aline Lerner | Published: March 8, 2021; Last updated: May 23, 2024

I’ve been hiring engineers in some capacity for the past decade, and five years ago I founded interviewing.io, a technical recruiting marketplace that provides engineers with anonymous mock interviews and then fast-tracks top performers—regardless of who they are or how they look on paper—at top companies.

We’ve hosted close to 100K technical interviews on our platform and have helped thousands of engineers find jobs. Since last year, we’ve also been running a [Fellowship program specifically for engineers from underrepresented backgrounds](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship).

All that is to say that even though I have strong opinions about “diversity hiring” initiatives, I’ve acquired them the honest way, through laboratory experience.

Though I find it problematic to label any type of hiring as “diversity hiring” —primarily because it tacitly implies to candidates and everyone else involved that they are getting the job not because of their merits but because of their demographics or to fill some quota—nomenclature is not my main critique of these initiatives. Though there are a number of big issues with how employers approach hiring, in this post I’ll talk about the one that’s at once the most subtle and the most damaging: **systematically ignoring the technical interview practice gap between traditional and non-traditional candidates**. I’ll explain exactly what the interview practice gap is and why it’s so bad in a moment, but first, allow me to tell you a little story.

## The benefit of attending an elite CS university

I was fortunate enough to attend MIT for undergrad. Probably the greatest gift that MIT gave me was a big stamp on my forehead that, to this day, makes strangers think I’m smart. But there was another, more relevant gift that gave me a serious advantage over students who did not attend an elite computer science institution: boundless access to technical interview practice.

Not only was there a multi-week course during the month-long break between Fall and Spring semesters that was dedicated exclusively to passing technical interviews, but all of my peers were going through exactly the same thing at the same time. Everyone was interviewing at FAANG for internships and new grad positions, which meant that we could all practice with each other, share our successes and failures, and, over time, internalize just how much of technical interviewing is a numbers game. **We learned that bombing a Google interview did not mean that you weren’t meant to be an engineer. It just meant that you needed to work some more problems, do more mock interviews, and try again at Facebook.**

## Practice, practice, practice

My anecdotal experience aside, we at interviewing.io have actual data. As I mentioned earlier, we’ve hosted close to 100K interviews on our platform. It has taught us two very important things. This interview style is controversial, in part because it’s not entirely similar to the work software engineers do every day but also because 1) like standardized testing, it’s a learned skill, and 2) unlike standardized testing, interview results are not consistent or repeatable—the same candidate can do well in one interview and fail another one the same day.

**First, regardless of how strong you are technically, practice really matters.** In a recent study, [we looked at how people who got jobs at FAANG performed](https://interviewing.io/blog/how-know-ready-interview-faang) in practice vs. those who did not. Below is a graph of average technical score in interviews and the portion of people with each score who passed Facebook’s interview process.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_21_52_2x_592c564655.png)

You can see that technical ability did not obviously associate with interview success. So what did? It turned out that the number of practice interviews people completed (either on interviewing.io or off) had a much bigger bearing, as you can see below. Surprisingly, no other factors we included in our model (seniority, gender, degree, etc.) mattered at all.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_22_05_2x_1974c0c5ce.png)

**Secondly, technical interview performance from interview to interview is fairly inconsistent, even among strong candidates.** Notably, as above, consistency appears to have nothing to do with seniority, pedigree, or anything else. **In fact, only about 20% of interviewees perform consistently from interview to interview. Why does this matter? Once you’ve done a few traditional technical interviews, the volatility and lack of determinism in the process is something you figure out anecdotally and kind of accept.** And if you have the benefit of having friends who’ve also been through it, it only gets easier. But what if you don’t?

In a previous post, we talked about how [women quit interview practice 7 times more often than men](https://interviewing.io/blog/voice-modulation-gender-technical-interviews) after just one bad interview. It’s not too much of a leap to say that this is probably happening to any number of groups who are underrepresented/underserved by the current system. **In other words, though it’s a broken process for everyone, the flaws within the system hit these groups the hardest—just because they haven’t had the chance to internalize exactly how much of technical interviewing is a game.** Sadly, since we’ve started running our Fellowship program for engineers from underrepresented backgrounds, we’ve seen this effect firsthand.

## The technical interview practice gap

So what does this have to do with the practice gap? As you can see in the graph above, **there was a pretty meaningful bump in performance** (almost 2X as likely to pass!) **after at least five practice interviews.** Five interviews is not a lot for someone who’s actively practicing and knows how the interview prep game works. **But it’s a far cry from what companies who are looking to boost their diversity numbers offer their candidates, and it’s equally far from what bootcamps, an increasingly important source of candidates from underrepresented backgrounds, provide their students.**

### What employers are doing wrong

I won’t name any specific companies here, but tech giants (many with good intentions and with marching orders to boost their diversity numbers) often do a considerable amount of outreach and pre-interview engagement with candidates from underrepresented backgrounds. This outreach is usually an info session where one engineer speaks to a virtual room of candidates from underrepresented backgrounds. The engineer tells them what to expect in technical interviews, encourages them to learn how to articulate their thought process out loud while solving a problem, and recommends resources like *Cracking the Coding Interview.* Some companies even go so far as to offer their underrepresented candidates a mock interview or two.

Unfortunately, for candidates who are unfamiliar with the process, neither of these interventions is nearly enough.

### What bootcamps are doing wrong

Because they’re cheaper and much faster than a four-year program, bootcamps seem like a rational choice when compared to the price of attending a top university. Since 2013, bootcamp enrollment has grown 9X, with a total of 20,316 graduates in 2018. Though these numbers represent enrollment across all genders and the raw number of grads lags behind CS programs, the portion of women graduating from bootcamps is also on the rise and graduation from online programs has actually reached gender parity (as compared to only 20% in traditional CS programs).

Despite these encouraging numbers, things start to go sideways when it’s time for students to start interviewing for jobs, and disappointingly, what bootcamps provide their students isn’t much better than the tech giants with good intentions.

Again, without naming names, I’ve interacted in some capacity with almost every major bootcamp over the last few years, trying to sell them on providing interview practice for their students, and I’ve also witnessed how recent bootcamp grads perform in mock interviews on interviewing.io. Based on these observations, bootcamps seem to have two big limitations: not nearly enough time spent on data structures and algorithms in the curriculum and lack of interview prep support both during and after the program.

Discussing bootcamp curriculum is out of the scope of this post, but I will say a few words about how bootcamps handle interview preparation. Bootcamps, by and large, have pretty thin margins and can’t invest much in paying for interview prep or hiring engineers to do it. As such, if they do offer mock interviews with professionals, just like tech companies, they tend to cap them at one or two, which we know isn’t nearly enough.

If they do not offer mock interviews with professionals (and most do NOT), they instead tend to focus on peer to peer practice (students interviewing other students), or they solicit volunteers from their pool of recent alums. While peer interviews are not entirely useless, they’re a far cry from working with someone who knows what the interview process is like. For the price tag that bootcamps charge their students, it’s appalling that they’re unable to invest resources into the last mile: actually getting their students ready for interviews.

That’s not to say that bootcamp grads are not viable employees. If that were true, I wouldn’t be advocating bootcamps as a means of increasing diversity in software engineering. What’s heartbreaking is that bootcamp students have all the makings of good engineers—they just need to spend an extra 3-6 months after graduation drilling by themselves on the skills their programs should have offered them.

While I have less direct experience with curriculum and career centers at non-elite universities (another contributor to the practice gap), from speaking with students who attend these schools, it’s clear that access to consistent interview prep is hard to come by, especially when compared to what’s available at their more elite counterparts.

## The cost of falling short

Why are bootcamps, universities, and employers all falling short and exacerbating the practice gap? With both employers and bootcamps, the teams responsible for facilitating interview info sessions or mock interviews typically consist of career coaches who don’t come from a technical background and therefore don’t have a good grasp of the difference between preparing for behavioral interviews and technical ones … and the big gulf between the two.

Behavioral interviews aren’t easy, sure, but they don’t require you to get used to a completely different mode of communication while simultaneously solving hard problems and turning them into code as someone breathes down your neck. Nor do behavioral interviews require hours and hours of grinding and drilling.

Preparing for technical interviews (and other analytical, domain-specific interviews… see the Tweet about management consulting below) is much more like studying for a math test than it is about practicing presenting yourself and telling your story. But until you’ve done it, you won’t really get it.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Clean_Shot_2022_12_16_at_19_22_19_2x_5d1b0fcf7f.png)

The most heartbreaking effect of the practice gap is this. Companies, under fire for hiring exclusively from top schools, expand their recruiting efforts and start targeting a broader range of schools and programs. One of the boldest programs of this type interviewed every CS student at each HBCU to develop a pipeline of African American candidates. Sadly, because of the interview practice gap, good-intentioned programs like this are destined to fail. A 2016 piece in Bloomberg (“[Why Doesn’t Silicon Valley Hire Black Coders?](https://www.bloomberg.com/tosv2.html?vid=&uuid=324fb7d1-d308-11ed-a129-7179624d4b55&url=L2ZlYXR1cmVzLzIwMTYtaG93YXJkLXVuaXZlcnNpdHktY29kZXJzLw==)”) covered this problem extensively:

> *When they started interviewing seniors, companies found—as Pratt did at Howard—that many were underprepared. They hadn’t been exposed to programming before college and had gaps in their college classes. The companies were coming into the process too late. So many of them have created programs geared toward freshmen and sophomores.*

**Then, when the initiative inevitably fails, companies walk away with their conscious biases about the “superiority” of top schools confirmed. Or even worse, this confirms their unconscious biases about race and/or gender, when the real problem isn’t the students but their lack of access to interview practice.**

## How you can (actually!) help

A few years ago, we ran a study that examined interview performance by school tier. Among students who were actively and regularly practicing technical interviewing, [there was no difference in performance between elite schools and non-elite schools](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt)!

![Chart showing new grad interview performance by school tier](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fnew_grad_interview_performance_008804d4ed.png&w=1920&q=75 "New Grad Interview Performance by School Tier")

All that is to say that **with at least 5 professional mock interviews per candidate, we could close the practice gap, level the playing field in software engineering, and hit our representation goals.** This is a bold statement, but in my decade plus of operating in this space, I have not seen anything as effective at closing the gap in tech as getting everyone technical interview practice. If you’re an employer, I urge you to consider the following:

- **Create a program where you sponsor interview practice for your candidates**, especially those who aren’t socialized in technical interviews. I don’t care if it’s through interviewing.io or through someone else. Just do it.
- One mock interview is not enough. An info session is not enough. **You need at least five interviews to move the needle**, and they need to happen systematically and on a schedule. But you know what? It’s a few hundred dollars per candidate. Compared to the cost of sourcing and interviewing them, it’s peanuts.
- You may want to think about routing some of your D&I budget toward this purpose. **If you’re spending any appreciable amount of money on initiatives like unconscious bias training**, [which has been shown over and over not to work](https://www.mckinsey.com/featured-insights/gender-equality/focusing-on-what-works-for-workplace-diversity), **you could make a much bigger dent by relocating that budget toward interview practice for your candidates**.

There is a lot of loud conversation in Silicon Valley about fixing representation in tech. **It’s time to stop talking and instead do something that actually moves the needle. If interviewing.io had the means to give five mock interviews to every student in the US who doesn’t have the ability to pay for them, we’d do it right now.**

My years of experience have shown me that there are countless members of underrepresented groups who are ready and able to be outstanding engineers, but who have been unable to pass their technical interview. We at interviewing.io are determined to make a difference, and we want to give people the same gift that MIT once gave me: the knowledge that interview practice is everything and that failing an interview doesn’t mean you’re not cut out for this career.

If you want to help us, [get in touch](mailto:hello@interviewing.io).


# [It's OK to postpone your interviews if you're not ready](https://interviewing.io/blog/its-ok-to-postpone-your-interviews-if-youre-not-ready)

By Aline Lerner | Published: March 24, 2025; Last updated: April 27, 2025

At interviewing.io, we’ve seen hundreds of thousands of engineers go through job searches, and the biggest mistakes we see people make are all variations on the same theme: not postponing their interview when they aren’t ready.

I’ve found myself repeating that it’s OK to postpone interviews so often that I finally broke down and decided to make it a blog post. It’s very simple advice, so the bulk of this post will be spent trying to convince you that it’s fine to postpone. And then of course I’ll tell you what to say.

Do any of these situations sound familiar?

- A recruiter from a FAANG (or other top-tier) company contacts you out of the blue and invites you to interview. You do the recruiter call, and it goes well. The recruiter reaches out to schedule your technical phone screen. You haven’t practiced enough, and you know it, but you are scared to tell the recruiter that you want to postpone the interview by a few months (months?!) because the position may no longer be there. So, you plow ahead, do the phone screen, fail it, and then you’re frozen out for a year.
- You were able to postpone your technical phone screen and take the time to study up on data structures & algorithms problems. You do well in the interview. But then your recruiter schedules your onsite the following week. Because you were so focused on DS&A prep, you haven’t had much time to study up on system design. You fumble the system design interview during the onsite, and you either get rejected or get down-leveled.

Both of these situations are extremely common, but they’re both preventable. You can just ask to postpone. There are a few edge cases where that’s not a good idea, but in most situations, it’s the right thing to do.

## When postponing is OK, and when you *shouldn’t* take our advice

If you’re applying to a large company with a centralized process (in other words, a process where you interview first and get matched with a team later), postponing is almost always OK. These companies are perpetually hiring, and their open roles are evergreen.[1](#user-content-fn-1)

Even if you’re applying to a large company with a decentralized process (where you interview for a specific team), we recommend postponing unless you’re extremely excited about the team you’re talking to. In that scenario, it’s possible that if you postpone, the slot will be filled. But if it’s not a perfect fit and you’d be OK with another team, we recommend postponing — in the worst case, you’ll simply get slotted into a different team.

The only time when postponing isn’t a good idea is when you’re applying to a very small company that has just one open headcount. In that scenario, it is possible that postponing will cost you the opportunity because they’ll choose another candidate. However, you can ask how likely that is to happen, up front.

With that edge case out of the way, here’s a little-known fact about how timing works at large companies: Recruiters don’t really care when you interview. Though they’d prefer that you interview sooner rather than later so they can hit their numbers, at the end of the day, they’d rather be responsible for successful candidates than unsuccessful ones.

Every recruiter, in every job search, will tell you that time is of the essence because of all the other candidates in the pipeline. Most of the time, that is irrelevant and just something they say to create an artificial sense of urgency. There are always other candidates in the pipeline because the roles are evergreen. But they have nothing to do with your prospects.

## Exactly what to say to postpone your interviews

You can use this text verbatim when postponing your interviews, and, with some small edits, you can even use it several times (e.g., before the phone screen and then again before the onsite).

> *I’m really excited about interviewing at [company name]. Unfortunately, if I’m honest, I haven’t had a chance to practice as much as I’d like. I know how hard and competitive these interviews are, and I want to put my best foot forward. I think I’ll realistically need a couple of months to prepare. How about we schedule my interview for [date]?*

One important thing to remember is to be conservative about how long it will take. You’ve probably heard the adage about how, when you have to estimate the time an engineering task will take, you should think of a number and double it. Here, you may even want to triple it.

I’ve seen many candidates ask for 2 weeks because that feels like a reasonable thing to ask for, only to have it blow up in their face when they realize they need to ask for another extension (which is still worth doing but harder because the company may think you’re taking them for a ride). If you need two months, ask for two months.

## Postponing can also be a good way to control the timing of your job search

This section is extra credit, but once you get comfortable with postponing your interviews when you’re not prepared, you can use the same skills to batch your interviews and ultimately control the timing of your job search.

What does it mean to control the timing of your job search? Ideally, you want all of your offers to come in at the same time, both because it maximizes optionality (one company that arbitrarily offers first doesn’t rush you into making a decision) and maximizes leverage (you can negotiate from a position of power).

If you want to dive deeper into this process, take a look at a book I recently co-wrote, *[Beyond Cracking the Coding Interview](https://www.amazon.com/dp/195570600X)*. It includes a full chapter about how to manage your job search, which covers everything from determining the order in which to approach companies to how to speed them up and slow them down once you’re in process. There’s a lot more detail than I can touch in this post, and much of it depends on your specific circumstances, but you can probably get 50% of the way there just by postponing your interviews in batches.

The big insight here is that, except for the edge cases we discussed above, a recruiting process can be paused at any point.

In other words, you can do a bunch of outreach to companies, then do a bunch of enthusiastic recruiter calls, and THEN pause all the processes until you’re prepared to do technical phone screens.

Then, you batch the phone screens.

Finally, if needed, you pause again to give yourself time to prepare for onsites. Onsite interviews require a different skill set than technical phone screens. The technical phone screen isn’t about depth or fit — it’s just a way to cut people who aren’t likely to pass the onsite.

The onsite, on the other hand, isn’t just meant to cut poor performers. It’s at once a deeper dive into your technical ability and a way to gauge fit. If you’ll be interviewing with your future team (typical at companies with a decentralized process), it’s also meant to assess your ability to work together, collaborate on hard things, complement the team's existing skillset, and so on. It usually has some coding (to verify that your technical phone screen wasn’t a fluke), but the focus is usually on system design and behavioral interviews[2](#user-content-fn-2), which are also the interviews commonly used for leveling decisions.

So, onsite prep is much more about system design and getting your stories right for your behavioral interviews. Some people can pull off prepping for both coding and sys design/behavioral at the same time. For many, depending on their existing familiarity with the material, it’s a tall order. So, it’s wise to take the time you need and prepare.

Then, once you’re ready, you batch the onsites.

When your offers come in, you should ask for extensions as needed, speed companies up, and start [negotiating](https://interviewing.io/blog/category/salary-negotiation) (which we’ve written about in the past and which, of course, is covered at great depth in the book).

1. Yes, it’s true that in 2022, we saw several FAANGs and many other companies freeze hiring, and if you had postponed your interviews, you’d have been left out in the cold. Despite how devastating these freezes were to affected candidates (and to the tech economy as whole), they are extremely rare, and in our humble opinion, not worth optimizing for or worrying about. You are very unlikely to have to deal with an impending freeze. [↩](#user-content-fnref-1)
2. Some companies will also have technical deep dives, project presentations, assessments of niche skills, and so on. [↩](#user-content-fnref-2)


# [The unwritten rules (till now) of negotiating with Meta](https://interviewing.io/blog/how-to-negotiate-with-meta)

By Aline Lerner | Published: February 26, 2024; Last updated: February 24, 2025

*If you don't like reading, here's me presenting the contents of this blog post in a video. Pick your poison.*

**EDIT**: This post is based on stories from users who were interviewing for E4-E6 SWE and MLE roles at Meta. Other, more specialized roles may have different processes than what's described here.

**EDIT 2**: As of Q1 2025, it looks like Meta is sometimes foregoing team matching in favor of driving candidates to their Monetization org (which apparently has a lot of open headcount). If this happens to you, after you pass the hiring committee, instead of entering team matching, your recruiter will tell you that you’ve been assigned to this particular org and, if you proceed, you’ll just get an offer. You won’t get to talk to your future manager, and you’ll find out team details a week after you join. From what we know, it’s possible to insist on team matching instead, with the downside that it’ll take an unknown amount of time rather than certainty.

Why does this matter?

If you’re looking to use your Meta offer primarily as leverage with other companies, this is a great thing because you’ll get numbers quickly. If you’re seriously interested in Meta, then this could be good or bad and depends where you are in the process and how much you want to work on monetization versus other things/how much you want to meet your manager before you work for them.

So if you’re seriously interested in Meta, whether this is good or bad depends on where you are in the process. If you need them to move quickly, it’s great because you know you won’t be stuck in team matching. However, if you need them to slow down because you aren’t as far along with other companies or because you want to work in a different org, you may want to ask your recruiter to stick with the old team matching process. In our experience, it’s ok to ask this. Your recruiter may not tell you outright that that’s an option, but if you ask, they will probably do it. Of course, you may be stuck longer than you want. But at least then you will have more of a say in what you work on.

At interviewing.io, one of the services we offer our users is salary negotiation. Even though I’m the founder, I still do many of the sessions myself because they give me an invaluable insider’s perspective on what’s actually going on in the engineering market, what different companies’ offers look like, how companies extend offers, what kinds of deadlines they give, and how much they go up in compensation, under what circumstances.

Access to this kind of data is great because it helps me make better business decisions. But sometimes I see questionable patterns of behavior among companies. Recently, I’ve observed a string of practices at Meta that I find reprehensible, and that’s what this post is about. I’ve seen the same practices with enough candidates, and across enough different titles and positions, that it’s become clear to me that they are not isolated incidents or a rogue recruiter’s doing but rather a consistent implementation of a deliberate strategy that comes from the top.

I’m writing about this for two reasons. First, if you’re negotiating with Meta, you need to know how they operate and understand the unwritten rules of the game. If you do not know the rules, you will fail — long before you even start negotiating.

Second, I’m hoping that someone at Meta sees this post and that maybe it’ll spark an internal discussion about changing the rules.

*By the way, if I’m wrong, I will gladly issue a retraction and a public apology. Please contact me if you’re a recruiter at Meta and find something incorrect in this post. My email is [aline@interviewing.io](mailto:aline@interviewing.io).*

Lastly, if you’re about to interview there or are interviewing there already, please [read our free, long-form guide to their interview process and questions](https://interviewing.io/guides/hiring-process/meta-facebook#meta).

## Meta basically has a monopoly on FAANG hiring right now

I mentioned above that we do salary negotiation, but our main business is mock interviews. We offer anonymous mock interviews in the style of a bunch of different companies (mostly FAANGs). This means we know how many people are practicing for interviews at Google vs. Meta vs. other FAANGs, and that lets us guess (pretty accurately) how much hiring is actually happening at these companies.

You can read in way more detail about how all the FAANGs are doing in our [recent blog post where we made 2024 predictions](https://interviewing.io/blog/when-is-hiring-coming-back-predictions-for-2024) based on our proprietary data. But while I was writing that post, I noticed something odd. Meta was hiring way more engineers than any of the other FAANGs. In fact, Meta hiring is up more than 10X since January of last year.
![Mock interview purchases for FAANG companies on interviewing.io (as a proxy for hiring volume) in 2023](https://strapi-iio.s3.us-west-2.amazonaws.com/Mock_interview_purchases_for_FAANG_companies_on_interviewing_io_as_a_proxy_for_hiring_volume_in_2023_c7dbe9a6d7.png)
You can see that more recently Amazon has picked up a bit, but it’s very recent and not enough to drive major change in other companies’ behaviors (at least not yet). And, yes, Netflix is hiring too, but Meta’s eng team is more than 10X the size of Netflix’s, so in the absolute, Netflix’s hiring volume isn’t enough to balance Meta out. For all intents and purposes, Meta’s the only FAANG that’s really hiring at scale — and they’re currently getting away with treating candidates really poorly as a result.

## How Meta negotiates, given their effective monopoly on eng hiring

Here’s how Meta runs their hiring process. These practices have been consistent across every negotiation client we’ve had in the past 6 months or so.

Meta’s hiring is centralized, which means that you enter one big interview process that’s completely divorced from which team you might end up on and you interview with people whom you might never work with again. If you do well, there will be a team matching component after you pass the onsite but before you get an offer[1](#user-content-fn-1). With that in mind, here’s how they run their process, once you get the green light.

1. **Team matching**. This can take days or weeks, depending on how many teams you speak to and how many conversations you have with the people on each team. You'll speak with hiring managers to gauge fit and chemistry, and if you’d like, you can also talk to peers. We've heard that sometimes you get the chance to talk to a handful of teams, and sometimes it's over 10. After your team-matching conversations, your recruiter will ask you to choose a team. In order to move forward, both you and the hiring manager have to opt in[2](#user-content-fn-2).
2. **Likely down-level you**. Sometime during team matching, you’ll probably find out that you’ve been down-leveled. Often, your recruiter will cite your performance in the system design portion (and sometimes the behavioral portion). According to a recent survey we did, something like 55% of Meta candidates get down-leveled (more likely for generalist SWE roles, less likely for more niche roles like ML).
3. **Make a lowball offer with just a few days to make a decision**. Once you’re done with team matching, things get dicey. Your recruiter will make you a lowball offer that’s often $50k or more (!!) below the average TC on levels.fyi. Moreover, you usually just get a couple of days to make a decision. If you were down-leveled, your lowball offer may include a small signing bonus as a consolation prize.
4. **Refuse to negotiate unless you can show them other offers from comparable companies**. Your recruiter will say something like, “If you’d like to increase your offer, I can take this to the compensation committee, but I need a compelling reason [i.e., another offer]."
5. **If you have other offers, they will apologize for the lowball offer, citing that it’s “automatic numbers from our computer” and raise the numbers by $100K or more (in first year’s TC)**. If you do not, you will be stuck with a lowball offer, though you may be able to negotiate a small signing bonus if the offer didn’t come with one already.

## How to negotiate with Meta

Below are the steps for negotiating with Meta in a hard climate where they have a monopoly on hiring. We hope that most of these won’t be necessary in the future. They are:

- Don’t share anything with your recruiter
- Make sure you have other offers
- Slow-play team matching
- Build rapport with every hiring manager you talk to
- Actually negotiate (this is the easy part)

### Don’t share anything with your recruiter

We wrote a whole post about how to avoid sharing information with your recruiter and why this is so vital. If you share where you’re interviewing or how far along you are, or if you start negotiating prematurely, the strategies below won’t work.

Please [read our post on not shooting yourself in the foot during negotiations](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) before you continue!

You should also be aware of a few tricks specific to Meta recruiters. If you're not forthcoming about where else you're interviewing, they may say a few sneaky things.

First, they may say something like, "Well, we just want to know where you're interviewing so we can intro you to others who interviewed at those companies but ultimately chose Meta." Don't fall for that early in the process. It's a trap to get information out of you. You can always ask for those intros later, when you're negotiating and it's the right time information about your other offers.

The other thing they do is fish, saying something like, "Well, in case you're interviewing at {Google, Netflix, some other big company they don't want to lose candidates to}, just so you know, they move kind of slowly, so we may not be able to wait." There, the Meta recruiter's goal is to get you to say, "Oh, no, don't worry, I'm not interviewing at Google!"

Now, you've lost leverage in their eyes because that's one less competitive company they might lose you to... and they'll be more confident about lowballing you later on.

### Make sure you have other offers

As you may have guessed from reading the previous section, it’s critical to have other offers, and not just any offers but ones from top-tier companies who pay very competitively[3](#user-content-fn-3).

We realize that saying “have other offers” doesn’t capture the blood, sweat, and tears that go into months of interview prep, applications, emails, recruiter calls, and interviews. We know it’s hard, but as you’ll see, it makes a huge difference in your compensation.

Getting those offers doesn’t start when you’ve received your Meta offer. It starts months before. Make sure that you get enough initial conversations with other FAANGs, FAANG-adjacent companies, and late-stage sexy startups to end up with at least one other offer, ideally at least two. Depending on your interview performance, this might be anywhere from 4 to 10 initial conversations.[4](#user-content-fn-4)

### Slow-play team matching

Having your offers come in around the same time is critical for [any negotiation](https://interviewing.io/blog/negotiate-salary-recruiter), but it’s especially important with Meta because they take such a hard line — without other offers, they will not meaningfully budge.

Obviously, you’ll want to start your conversations with other companies well in advance of your Meta interviews and do everything you can to make sure they all come in at the same time. However, even with your best efforts, it’s not guaranteed that your timing will match up. **Here’s how to make sure that your offers come in at the same time: slow-play Meta’s team-matching process**.

Team matching is actually the part of your Meta journey where you have the most leverage and power. Why leverage? At this point, they know they want you, but they can’t yet hold an offer deadline over you. We’ve already mentioned that once they make the offer, your recruiter is going to push very hard to have you accept, often giving you a deadline of something like two days. In your recruiter’s eyes, you’re a ticking time bomb, where for every day you don’t sign, the deal loses momentum, and your odds of signing drop off. Recruiters are also evaluated on how many candidates they close, so it’s in their interest to create a false sense of scarcity in order to rush you and to use high-pressure sales tactics to get you to seal the deal.

And what power do you have? It turns out you can really control how long team matching takes, within reason. If you’re still wrapping phone screens with other companies, slow-playing is the best thing you can do. Here’s how to do it.

**We’ve recently heard that Meta is now insisting that hiring manager conversations happen in series, but even if Meta lets you talk to multiple hiring managers concurrently, try to serialize those conversations as much as possible**. For instance, if you hear from your recruiter that you’re going to start team matching on a Monday, and they offer to set up some calls for Wednesday, ask to do the first call on a Friday and the next call the following Tuesday.
When we advise our users to do this, we often get pushback because they’re worried that slowing things down will make them look disinterested/not serious. We promise you that’s not the case. The biggest risk you run when you slow-play team matching scheduling like this is losing the chance to work on a specific team. If you find that your recruiter has proposed what seems like the perfect team for you, you can and should prioritize doing that call as soon as possible.

**In addition to serializing your hiring manager conversations, for each team, ask to talk to a few individual contributors on teams that you’re serious about**. This isn’t just a stalling tactic. These are the people you’ll be working closely with every day, and they’re the ones doing the job you may have in the future. They’re also less likely to do hard sells, and if you ask thoughtful questions, you’ll learn a lot about what to expect. We’re always surprised by how few candidates ask to speak to their future peers, out of a mistaken concern that asking for too much will make them look disinterested or unengaged.
Just like with hiring manager calls, if you need to slow things down, we recommend scheduling calls with your peers a few days apart.

### Situations where slow-playing may bite you, and how to know the difference

We have seen two instances when our advice about slow-playing could backfire. You probably remember when companies started to freeze hiring aggressively in mid-2022 — if you didn’t get matched before the ax came down, you were left out in the cold. Much more recently, we heard from some of our users that Meta put a pause on team matching for E4 roles (largely outside the Bay Area), and many candidates were stuck in a holding pattern (while Meta figured out headcount constraints, though it looks like it’s since been resolved, and picking back up in earnest. Slow-playing and then getting stuck is obviously an unfortunate situation, as is being on the wrong end of a hiring freeze, but these situations are rare, and in our humble opinion, not worth optimizing for — in most cases, you will not be dealing with an impending freeze or stalled matching. If you’re unsure about team supply or the state of hiring, you can do two things:

1. Always ask your recruiter up front to share the number of teams that candidates have had the chance to talk to, on average, recently.
2. Look at our [real-time graph of FAANG hiring volume](https://interviewingio-metabase.herokuapp.com/public/dashboard/11313b19-b9fd-4532-9a3f-d2aad7e027c3) (as opposed to the graph above, which is a monthly snapshot). In this real-time graph, you can see what portion of our mock interview volume is dedicated to practice in the style of Meta, Amazon, and Google. Historically, our purchasing behavior has lined up very well with what’s actually going on in the market, and the rate of change in this graph should give you an idea of whether Meta is slowing down. As you can see, there was a dip in February (and one in December, but that almost always happens because hiring slows down over the holidays). If you see dips like these, you’ll want to make sure that you do (1) and ask your recruiter about the situation inside. You may also choose not to slow-play for too long.

There's one more situation where slow-playing may bite you. If you match with a team and really click well with the manager, to the point where you have your heart set on it, it may be wise to accept that team instead of trying to drag things out. We've heard of times where, even if the candidate asked the manager about open headcount and confirmed they didn't have to rush, the role got filled from under them (they were able to talk to other teams afterwards but missed the chance to be on that specific team). If a specific team feels irreplaceable to you and you'd be crushed if you didn't get it, then that may be more important than maximizing your negotiation.

### Build rapport with every hiring manager you talk to

Outside of using the team-matching process to control your timeline, there is one other important tactical piece of advice: Do your best to build rapport with hiring managers.

**As we said above, recruiters are trying to close the deal. That’s their job. Hiring managers, on the other hand, are trying to lay the groundwork for a good working relationship with you**. As a result, their interests are much more aligned with yours. Of course, they still want to close you, but it’s not worth it to them to employ high-pressure tactics, and it’s not something they’re trained in or comfortable with (in fact, many of them hated these tactics when they were on the other end of it while looking for work)[5](#user-content-fn-5).

As such, hiring managers will generally be a lot more transparent with you about how much time you actually have to make a decision, and their answers will likely be very different from the ones you get from recruiters.

We’ve advised all of our Meta candidates to ask their prospective hiring managers about when they realistically have to make a decision by, and the differences between what the hiring manager has told them (“Take your time; you have a few weeks at least.”) and what their recruiter has told them (“We’re talking to a lot of candidates for that team. To ensure your spot, you should make a decision in the next few days.”) are stark.

There’s simply no downside to building rapport with hiring managers. At worst, you make a professional connection. At best, you get a head start on a great working relationship with your new boss.

**One practical note: Always ask your hiring manager for their email address in case you have more questions later**. This way, if your recruiter starts telling you that you need to make a decision by Friday, you can ping your hiring manager, explain that you’re still thinking, and ask if it’s OK to take a few more days. Almost always they will say yes.

### Actually negotiate (this is the easy part)

If you’ve done everything else in this post, the negotiation is the easy part. By now, you’ve wrapped up team matching, chosen a team, and have likely gotten an aggressive offer deadline.

You have also not shared any offer details till now. As we said at the beginning of this post, the success of your strategy hinges on the recruiter not being aware of the other companies you’re interviewing with. This will be the first time they find out about it, and that will put them on their proverbial back foot.

![](https://64.media.tumblr.com/4e9087066942c43406fd478dcc34ba90/f2c80ff8d660cda3-62/s540x810/9f0a2c2a8f29b0782c59e0551444d001ec1a7a26.gif)

To respond to the offer, you can send an email[6](#user-content-fn-6) that looks something like the below. The details will differ, and how much you reveal about the other offers will vary, but here’s the general idea.

*Hey [Recruiter name],*

*Thank you so much for the offer, for working with me throughout this process, and for all your help with team matching. I wanted to share some details with you. I currently have offers from {Company 1}, {Company 2}, and {Company 3}.

{Company 1} has offered me a significantly higher base salary of $220k. {Company 2} has a comparable base but has offered me significantly more equity: $500k. I know {Company 2} is a startup, but they’re late stage and handing out RSUs. Those RSUs are as valuable to me as public company equity.

Finally, {Company 3} has thrown in a meaningful signing bonus, and their performance bonus is actually at 25%, not 15%.

I’m very excited about the opportunity to work at Meta and about the team. {Insert some authentic reasons why you’re excited about the company, the team, your new boss, etc.} It’d be great to see a meaningful increase in compensation to make my decision easier.*

*Thank you, and I look forward to hearing from you.*

By the way, this isn’t the only approach you can take, and with other companies, you might have better luck with the [Reverse Used Car Salesman](https://interviewing.io/blog/negotiate-salary-recruiter). However, in our experience, if you don’t share offer details, your Meta recruiter will immediately ask you to share, so you might as well control the flow of information.

In the template above, I’ve assumed that not all of your offers are stronger than Meta’s across the board, which is why I’ve cherry-picked which pieces to share. Sometimes, if you’re lucky, you’ll have multiple offers that have a higher base, more equity, and a higher signing bonus. In that case, it’s less of a game of skill — just throw the numbers at them, and they’ll exceed the other offers without much prompting.

If you run this play, your recruiter will apologize for low-balling you, blaming the “computer” for giving them those numbers. Then, like clockwork, you will see a $50k to $150k jump in your offer (precisely how much depends on where your other offers are from and how strong they are).[7](#user-content-fn-7)

Now, whether you take that offer is up to you.

Some closing thoughts. I’m a capitalist. Meta’s behavior here is aggressively capitalistic, if short-sighted – once other FAANGs start meaningfully hiring again, and Meta employees figure out that there’s a $150k comp differential between people with the same job title, they’re going to pay the piper and likely see a bunch of attrition. Ultimately, the market will correct these practices. However, I also believe that individuals have the right and duty to be as informed as possible and to wield whatever weapons in their arsenal to advocate for themselves, rather than waiting on the mercy of slow, indifferent market forces.

As such, we hope this post has given you some ammunition in your negotiations and helped reduce the information asymmetry between you and Meta, a huge, aggressive player with basically a monopoly on eng hiring at the moment. And we hope that if anyone from Meta is reading this, it’ll spark some internal conversations about what’s right. And if they don’t, other FAANGs’ recruiters will swoop in soon enough.

1. How team matching works changed fairly recently. In the past, you’d get an offer before you matched with a team and do a 6-week “bootcamp” where you’d get up to speed on Meta’s tech stack, infrastructure, and systems, followed by a multi-week “speed dating”-esque team matching cycle. Bootcamp still exists, but now it's much shorter (2-4 weeks), and the focus is getting new engineers ramped up on generic tools. After that, new engineers continue to ramp up on their specific teams. [↩](#user-content-fnref-1)
2. We’ve recently heard that Meta may now be insisting that team matching conversations happen in serial, i.e., you can only do one at a time. However, this doesn’t meaningfully change our advice. [↩](#user-content-fnref-2)
3. You might say, “Aline, why can’t I just make up offers?” We could never, in good conscience, advise that. It’s unethical, and though I’d argue that while Meta’s negotiation practices are also unethical, that’s not the way to win. Outside of ethical considerations, while the risks of getting caught are low, they’re not zero. Lying about offers, in our mind, is the last refuge of the incompetent. [↩](#user-content-fnref-3)
4. The advice in this post is orthogonal from your career goals and what you want to work on. This blog post is about navigating an unfair system filled with opaque rules while maximizing your cash. It is not about self-actualization, though we’d argue that creating the most optionality for yourself helps with self-actualization as well. You can also talk to smaller companies and use your big-co offers as leverage to increase your startup equity. There’s nothing wrong with that, but more detail on optionality and self-actualization is outside the scope of this post. [↩](#user-content-fnref-4)
5. Of course, some hiring managers will use high-pressure sales tactics or create false timelines to try to close you. But that’s the exception rather than the rule. You can decide if that’s something that you want to weigh when judging whether or not you want to work for them. [↩](#user-content-fnref-5)
6. We strongly urge you to avoid negotiating over the phone and over text, whenever possible. Your recruiter does 5 of these calls a day. You might do one of these calls every few years. Do the hard part over email. It’s the best way to level the playing field. [Read this post](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start) to learn how to avoid synchronous phone negotiations (just look for “phone”). [↩](#user-content-fnref-6)
7. One advanced maneuver is to pit all your other companies against each other and raise up their initial offers BEFORE talking to Meta. We’ll likely write about how to run this play in a future post. [↩](#user-content-fnref-7)


# [6 red flags I saw while doing 60+ technical interviews in 30 days](https://interviewing.io/blog/6-red-flags-i-saw-while-doing-60-technical-interviews-in-30-days)

By Uduak Obong-Eren | Published: September 14, 2020; Last updated: July 14, 2023

*Hey, Aline (founder of interviewing.io) here. We’re trying something new. Up till now, all posts on this blog have been written by interviewing.io employees or contractors. Why? Frankly, it’s hard to find great content in the recruiting space. There’s so much fluff and bad advice out there, and we didn’t want any part of that.*

*The other day though, I was reading Hacker News and saw* [*an article by Uduak Obong-Eren about how he did over 60 technical interviews in 30 days*](https://medium.com/@meekg33k/14-lessons-i-learned-from-doing-60-technical-interviews-in-30-days-7732e4f7608d?source=---------4------------------) *and what he learned from that gauntlet of an experience. I thought it was honest, vulnerable, well-written, and brimming with actionable advice. So, I reached out to him to see if he’d want to write something else. Fortunately, he did, and the article below is the inaugural post in what I hope will become our Guest Author series. You can read more about Uduak in the bio below.*

*A quick note because this is the first time we’re doing this. One of the things I’m most excited about with this new Guest Author series is the diversity of opinions it will bring to our blog. Technical interviewing and hiring is fraught with controversy, and not everything these posts contain will be in line with my opinions or the official opinions of interviewing.io. But that’s what’s great about it. After over a decade in this business, I \*still\* don’t think there’s a right way to conduct interviews, and I think hiring is always going to be a bit of a mess because it’s a fundamentally human process. Even if we don’t always agree, I do promise that the content we put forth will be curated, high quality, and written by smart people who are passionate about this space.*

*Now, off we go!*

![Author avatar](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUduak_Obong_Eren_0a6ec8d658.jpg&w=384&q=75 "Uduak Obong-Eren")

Uduak Obong-Eren

Uduak Obong-Eren is a Software Engineer based in the San Francisco Bay Area who is passionate about architecting and building scalable software systems. He has about five years of industry experience and holds a Masters in Software Engineering from Carnegie Mellon University. He is also an open source enthusiast and writes technical articles – you can view some of his writings at [https://meekg33k.dev](https://meekg33k.dev/). He especially enjoys conducting free mock technical interviews to help folks get better at technical interviewing. You can follow him on Twitter [@meekg33k](https://twitter.com/meekg33k).

What is the one thing you would look out for if you had to join a company?

Sometime between January and February 2020, I wanted to change jobs and was looking to join a new company. This, among other reasons, led me to embark on a marathon of technical interviews – [60+ technical interviews in 30 days](https://www.freecodecamp.org/news/what-i-learned-from-doing-60-technical-interviews-in-30-days/).

Doing that many number of interviews in such a short time meant I had an interesting mix of experiences from the various companies I interviewed with, each with their unique culture and values that often reflected in the way their interviews were conducted, intentionally or not.

In this article, I will be sharing some of the red flags I observed while I was on this marathon of technical interviews. I will not be mentioning names of any companies because that’s not the intent behind this article.

The goal of this article is also not to make you paranoid and be on the hunt for red flags in your next interview, far from it. Rather the goal is to equip you with knowledge to help you immediately identify exactly the same or similar red flags in your next interview\*,\* and hopefully identifying them will set you up to better handle them.

Even though the stories I’ll be sharing come from my marathon of *technical* interviews, these red flags do not apply only to technical interviews. They apply to all kinds of interviews and so there’s a lot to learn here for everyone.

## The Red Flags

### Your interviewer is only open to solving the problem ONE way

In the world of computing and in life generally, for any given problem, there is typically more than one way to solve that problem. For example, given a sorting problem, you could solve it using a [merge-sort](https://www.programiz.com/dsa/merge-sort) algorithm or a [heap sort](https://www.programiz.com/dsa/heap-sort) algorithm.

Having this rich number of techniques to solve a problem makes it even more interesting and the general expectation in technical interviews is that you should have the flexibility to solve a problem using your preferred technique.

I had an interview where the interviewer asked me to solve an algorithmic problem. I had started solving the problem using a specific technique when the interviewer stopped me in my tracks and asked that I use another technique.

When I probed a bit further to know why, it appeared that the reason he asked that I used the second wasn’t to test my knowledge of that second technique; it was because he was more ‘comfortable’ with that approach.

It is different if the interviewer wants to test your knowledge of something very specific. For example, given a problem that can be solved using iteration and [recursion](https://interviewing.io/recursion-interview-questions), the interviewer may want to test your knowledge of recursion and can ask you to solve the problem recursively. That wasn’t the case here.

I ended up using both techniques and discussed the trade-offs but frankly that experience left a bad taste in my mouth especially because that interview was with the hiring manager — my would-be manager, who is someone that can significantly influence your career growth and trajectory.

### Undue pressure to accept an offer letter

It’s very exciting and fulfilling when you go through all preliminary stages of a technical interview, through to the onsite interview (remote or in-person) and then you receive that “*Congratulations <insert name here>, we are pleased to offer you…*” email.

However, that excitement often becomes short-lived when there is some form of pressure from your soon-to-be employer to accept the offer. It’s a bit more manageable when the pressure comes from someone in HR or the recruiter, but when it’s from the hiring manager, that can be harder to manage.

That was the case for me when I interviewed with a startup based in Palo Alto. They were a small company in terms of staff strength. My onsite interview with them had gone quite well. I had a good conversation with the hiring manager and an even better conversation with the VP of Engineering, so much so that I could tell that I was going to be extended an offer. I asked to know how long I had to accept the offer letter and I was told seventy-two hours.

The offer letter arrived later that evening, and it looked great — a six-figure offer definitely didn’t seem like a bad start. I was also at the final stage of the interview process with other companies too and thankfully, I had enough time to negotiate and accept the offer, or so I thought.

Then the pressure started, incessant calls from the hiring manager and the VP of Engineering, back-to-back emails, all within the allotted time. So much was the pressure that it got to a point where I wasn’t sure I wanted to negotiate the offer anymore. I turned down the offer.

I turned down the offer because the experience got me thinking about the company’s work culture. Were the methods employed by the company to get me to accept the offer indicative of their work culture? If they needed to get something done, how far would they go?

Now don’t get me wrong, yes the company wants to employ you, yes the recruiting team wants to ‘close the deal’ however, it’s very important to pay attention to how the company does this. Do they remain professional about it?

A company’s values go beyond what they say, it shows in what they do and how they do it.

### Not enough clarity about your role

Among the many reasons why you would join a company is your desire to be involved in valuable work. I had the opportunity to join a US company based in Boulder, Colorado. They had contracted a recruiting agency to help them find someone to fill a Software Engineer position in their firm.

The hiring process started with an exploratory interview with the recruiting agency closely followed by a second interview with a recruiter from the company. In both interviews, I couldn’t get a clear sense of the details of my specific role was — what team I would be on, what kinds of projects I’d be working on, what the career growth pathway was, etc.

I understand that sometimes companies can be going through restructuring, but that didn’t seem to be the case here. It seemed more like the company was focused on completing their headcount. Even though there’s nothing wrong with completing a headcount, I think there is everything wrong with not having a clear purpose for a role for a couple of reasons:

- It means the role may not be critical to the company’s core business.
- If the role isn’t that important, it may mean when a layoff comes, your position may be impacted.

On a more personal note, I don’t want to be just a number. I want to work at a place where I have the opportunity to contribute in an impactful way, and I like to believe you would too. So it’s important to get clarity about your role, for where you are today and for future career growth.

### Consistent lack of interest or low morale from interviewers

When looking to join a company, one of the things you simply must care about is the team you will end up working on. At least 25% of your waking hours will be spent interacting with that team whether in-person or virtually.

Interviews offer you an opportunity to experience firsthand what it will look like to work with your prospective teammates, especially since, unless you’re interviewing at a huge company, your interviewers are likely to become your teammates.

If through all the different stages of the interview process, you experience a **consistent** lack of interest or low morale from your interviewers, you might want to pay attention .

When I experienced that during one of my interviews, I couldn’t exactly tell what the cause was, but I knew something just wasn’t right. After some internal tussle, I decided to trust my gut feelings and ended the interview process with the company.

Fast forward to two months after, two of my interviewers (would-be teammates) had left the company and joined another company (no I wasn’t stalking, I just checked on LinkedIn).

Now I’m not saying that during the interview process, there won’t be one or two people, who because of their busy schedules, would have preferred to be doing something else rather than interviewing. Yet, when all of the interviewers don’t want to be there, you certainly want to pay attention to that.

A lack of interest or low morale could be pointers to a combination of any of the following:

- Your prospective team-mates may be experiencing burnout.
- Some internal dissatisfaction with company — culture, policies, something, anything.
- The team isn’t that interested in you (hard pill to swallow?), maybe they don’t see you as a long-term hire.

Or it could be for reasons that I have not included here, but I implore you to not ignore this red flag if you see it in your next interview.

### Your interviewers aren’t prepared for the interview

Have you been in an interview before where the interviewer doesn’t seem to have any questions to ask you? Trust me it can get really awkward.

That was my experience during a technical phone-screen interview with an educational technology company based in California. The interviewer wasn’t prepared for our interview and didn’t have any questions at hand. He wasn’t even sure of who he was interviewing and what role I was interviewing for. It wasn’t a pleasant experience.

I understand that there are a myriad of reasons why interviewers may not be prepared for an interview. Some of which include:

- Lack of proper planning by the HR/recruiting team.
- Last-minute changes on the interviewee.
- Busy schedules for the interviewer.
- The interviewer just wasn’t prepared.

I typically won’t act on this red flag in isolation. I will be looking for other red flags in a bid to form a cluster of patterns before making any decision.

### Lack of a clear direction on where the company is headed

It’s fulfilling to be a part of a company that is involved in meaningful work that creates value for its users. Joining such a company would mean you have the desire to contribute in helping the organization meet its goals. This invariably means the organization must have some goals right?

I was contacted by a startup based in San Francisco via [AngelList](https://wellfound.com/). I had a first introductory call with a recruiter from the company, closely followed by a phone screen technical interview.

In both interviews, even though the interviewers shared some details about the company, there was a lot of vagueness and about the company’s direction and where the company was headed.

I particularly remember that one question I asked at the time, was about how the company would deal with its growing competition. Sadly, the answers I got didn’t seem convincing and the company later got acquired by the competition.

When you are interviewing to join a company, you are selling more than just your skills, but also yourself — your unique experience. While it’s important to do that, I think it’s equally important that the company should be able to sell you on its vision and what it hopes to achieve.

When I think of joining a company, I picture myself in that company for the next 2–5 years. If my vision for where I want to be in my career doesn’t align with the company’s vision, that is a mismatch that shouldn’t be ignored.

We sometimes focus more on securing the job and even though that is very important, even more important than getting the job is staying fulfilled on the job. **For me, fulfillment meant joining a company that had a clear vision of where they were headed, working in a role that was critical to the company’s business while being equipped with a lot of growth opportunities**.

Hopefully, these red flags I have shared will equip you to make better decisions on what companies you choose to grow your career with. I would generally not advise making a decision based on one or two red flags, but if you see a cluster of red flags, you shouldn’t ignore them. I wish you the best in your career journey.

If you ever need someone to do a mock interview with you, feel free to schedule one [here](https://interviewing.io/) or you can reach out directly to me on Twitter [@meekg33k](https://twitter.com/meekg33k).

And if you’d like a list of things to ask companies while you’re interviewing that may help you identify these red flags (and others!) sooner, [take a look at this one](https://blog.alinelerner.com/how-to-interview-your-interviewers/).

*If you have something to say about your adventures in interviewing or hiring, write a guest post on our blog! Please email me at [aline@interviewing.io](mailto:aline@interviewing.io) to get started.*


# [How to get in the door at top companies: cold outreach to hiring managers. Part 2 of 2.](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2)

By Aline Lerner | Published: July 30, 2024; Last updated: November 2, 2024

In [part 1 of this post](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-part-1), we talked about which channels are most effective for getting in the door and did an analysis of those channels along two axes: effectiveness and how much control you actually have. Here’s a quick summary.

In the quadrant above, you can see that while getting contacted by an in-house recruiter is very effective, whether you get contacted or not is largely out of your hands. The channel that maximizes both effectiveness and control is cold outreach to hiring managers (not recruiters!) “done right”. What does “done right” mean? That’s what we’ll talk about in this post (part 2 of 2). Most people do this type of outreach incorrectly. Here, we’ll get very tactical and tell you exactly what to say and do to reach out to hiring managers at the companies you’re interested in and actually get responses.

Here’s our recommended, hyper-practical approach.

## Prerequisites/tooling

- Buy a month or two of LinkedIn Sales Navigator. This will run you a few hundred dollars, but it’s worth it.
- Get an account with an email discovery tool like [RocketReach](https://rocketreach.co/) (an excellent email discovery tool).
- Get [Streak](https://www.streak.com/mail-merge-gmail), which lets you do mail merges in Gmail. You create an email template, with variables for everything from recipient name to long snippets of personalized text, and then you upload a CSV with all the values. The resulting emails feel personalized but get sent to hundreds of people at once.

## Treat your job search like a sales funnel

If you’re an engineer, chances are you haven’t ever done sales (maybe you had a job in high school selling Cutco knives or magazines, in which case what we’re about to say will resonate). But if you do sales for any appreciable amount of time, you’ll start thinking about everything in life as a funnel.

Funnels are wide at the top and narrow at the bottom. That’s why they’re such an apt metaphor for the sales process — you do a lot of outreach, and you don’t get many responses. Of the responses you do get, relatively few will do the thing you want them to do. And even fewer will ultimately “close” (aka, buying — or, in this case, hiring).

In your engineering career, you’ve intellectually mastered many abstract concepts that are much more complex than a funnel. Despite its simplicity, however, the funnel is one of the hardest concepts to internalize emotionally, especially for people who are used to having control over outcomes. When you write code for *n* hours, you can expect that you will build *m* features.

In sales though, you do a lot of work, very little of it will pan out, and when it does pan out, it can feel almost random; an impersonal, mediocre email gets a response while your beautifully targeted email is met with deafening silence.

And then there’s rejection. When you apply to jobs online and don’t hear back, it stings, but the sting is softened by the possibility that a human never even saw your application. You’re not reaching out to people when you apply online; you’re dealing with a bureaucratic machine.

On the other hand, when you email a real human and they don’t respond, that hurts: you put yourself out there, someone made a value judgment about you, and you lost.

**The good news is that, after a while, the pain lessens, and you build up some useful emotional calluses and acquire the thousand-yard stare of someone who’s been rejected a million times for a million reasons, ranging from soul-crushingly legitimate to incontrovertibly random.** Sadly, there’s no shortcut. You’ve got to do the reps, you’ve got to get the rejections, and you’ve got to pick yourself up again. You get used to it, and then it doesn’t hurt as much, because experience has taught you that if you keep going, you will eventually get to a yes.

## What to actually do

**First, come up with a target list of companies.** How to do that is out of scope for this post, but we may write about it in the future. For now, we’ll assume you have a list.

Once you have your list of companies, use LinkedIn Sales Navigator to find hiring managers at those companies (or founders or directors or VPs, as above). Below is an example query where we look for Google hiring managers.

You might think that Google is so big that sifting through all their various hiring managers will be intractable. Fortunately, you can whittle down the list to a pretty manageable size by applying some filters.

![linkedin_sales_navigator.png](https://strapi-iio.s3.us-west-2.amazonaws.com/linkedin_sales_navigator_1543713d2a.png)

Here are our filters:

- **Just targeting managers**, not directors or VPs. Google is a huge organization. You want the people who are most likely to help, and they’re the ones who are struggling to hire for their teams.
- **In position for less than 2 years:** These are the people who are still trying to prove themselves and who are less likely to have a long-standing relationship with their recruiter to the point where they only rely on internal recruiting and overlook other sources of candidates.
- **Geography:** Let's focus on the places we most want to work.
- **1st- or 2nd-degree connection:** This way, when they look you up, they’ll see some social proof. You can expand this to 3rd-degree connections, if needed.

**Once you have your list, put their LinkedIn URLs into a spreadsheet. Then, do a pass through your targets’ profiles and see if any of them link to personal websites, social media accounts, blogs, or anything else that will help you find common ground with them.** Add any useful links in your spreadsheet because we’ll be mining them when we actually write our emails.

### Look up their email addresses

**Once you have your list of LinkedIn URLs, use a tool like RocketReach to look up their emails.**

Why not reach out on LinkedIn? While recruiters live on LinkedIn, managers generally do not. Possibly, they don't even like or check LinkedIn much. They live in their emails, so that's where you want to target them.

RocketReach is a nice tool for email discovery because 1) it takes LinkedIn URLs as inputs and 2) its email database is generally up-to-date and correct.[1](#user-content-fn-1)

If RocketReach fails or you don't wish to pay for it, you might just be able to guess their email address, as email addresses tend to follow common forms: [aline@interviewing.io](mailto:aline@interviewing.io) (my actual email address), [alerner@interviewing.io](mailto:alerner@interviewing.io), or [aline.lerner@interviewing.io](mailto:aline.lerner@interviewing.io).

Where possible, contact managers via their work email address.[2](#user-content-fn-2) In some cases, you won’t be able to find their work email, in which case it’s acceptable to fall back to their personal email.

### Write succinct, highly personalized emails

**Next, compose a fairly personalized, yet short, email.** All too often, candidates write a long, generic cover letter that’s obviously been sent to a ton of people. I get many emails that look like this:

![Example 1 of a bad cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/bad_cold_email_8ed5f88af2.png)

Don’t do this!

![Example 2 of a bad cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/bad_cold_email_2_cd282fccc0.png)

Don’t do this either! There is nothing here about why this candidate is a good fit for interviewing.io, and the bullets aren’t compelling enough on their own. Note that this particular email is from a marketer, not an engineer, but the anti-patterns are the same.

Emails like the above are impersonal, but worst of all, they have a poor signal-to-noise ratio — I want to find a reason to say yes and to invest my valuable time into this person. But they’re not giving me one, and they’re making me work for it in the process.

- **Don't open email with how they found you.** This is a big pet peeve of ours. I don’t care how you found me! I know I’m on LinkedIn. What I care about is why talking to you will add value for me or why you’re interesting. Use the most significant real estate in the email, the first sentence, to tell me that!
- **Don't be overly formal in how you address the person. Use their first name.**
- **Don't get their gender wrong** (e.g., referring to a woman as "sir" — you’d be surprised how often this happens).
- **Don't paste in a generic cover letter.** These are sure to get ignored immediately — if you’re not going to put in the effort to write to me personally, why would I put in the effort to read your email?
- **Don't forget to include a link to a LinkedIn or a personal website.** We don’t recommend attaching your resume, though. It can seem overly formal/somewhat presumptuous if you're trying to build rapport.

More broadly, if you want someone to go out on a limb for you, make it dead simple for them to justify expending their social/political capital on you. Hiring managers, as a rule, want to help. Make it a no-brainer for them.

There are three components to a great cold email:

1. Common ground with your target
2. Proof that you’re worthy of their time
3. A strong call to action

Not every cold email will have (1) because you won’t always be able to find common ground with everyone — there’s simply not enough information out there about some targets to be able to craft a compelling narrative that’s highly personalized to them.

But every cold email you write should have (2). It is your job to sell yourself quickly and succinctly. You want your target to feel like they’d be an idiot to pass up the chance to talk to you.

#### Finding common ground

The email below is personal, succinct, and finds common ground. Not only that, but it conveniently finds common ground that *benefits the candidate* (a soft-spot for non-traditional candidates, like himself!).

![Example 1 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_2_8ecdbfe927.png)

To find common ground, reference something your target cares about. Then either show them that you care about it too or that helping you would fit into their worldview and further that cause.

As we mentioned above, finding common ground may be tough because there might not be enough information available about your target, but it’s important to do the work before you give up on this route — finding common ground is the tactic that’s going to get you the highest response rates.

Here are some examples of great ways to build common ground:

- Reference a project they worked on (maybe they wrote a blog post about it, mentioned it in a comment on Hacker News, or are a contributor to some open source project). Then…
  - If possible, talk about relevant work you’ve done. It’s important not to make this connection too tenuous. If you do, this approach might backfire because they’ll start to get excited about you, only to be let down and ultimately feel tricked.
  - If you do not have relevant work to share, ask a thoughtful question or two about theirs.
- Reference a controversial point of view that they hold, and affirm it in an authentic way.
- In the absence of something technical, it’s okay to reference something non-technical you've seen on their public profiles. We've seen candidates connect with strangers based on a shared love of Star Wars or Hearthstone.

We understand that you won't always be able to find common ground. But if you can, it'll help you a lot, especially if you’re light on social proof or accomplishments.

#### Selling yourself

Selling yourself is usually about one of two things:

- Accomplishments: What have you built or created?
- Social proof: Have you worked at a top company or attended a top school?

Some people are fortunate enough to have both, but many will have just one. That’s okay. We’ll work with what you have!

#### Accomplishments

What have you done that most other people haven’t? What have you done that, if you were to tell it to a stranger, would cause them to pause and think you're special or interesting?

Below are some examples:

- You’ve had a blog post about a technical topic or a personal project do well[3](#user-content-fn-3) on Hacker News, Reddit, or social media.
- Something you built at work got some great press when your company announced its last funding round.
- You refactored a piece of code at work, and now it runs 100X faster.
- You won a company hackathon.
- You’re a core contributor to a notable open-source project.
- Something you built is being used by a number of other people.

#### Social proof

**Social proof is more about your pedigree. If you attended a top school or worked at a company known for having a high engineering bar, you should absolutely mention it!** People won't click on links or open your resume until *after* they're interested, so you need to get them interested right away. That is: you should spoon feed them the most impressive-sounding things about you out of the gate. This may feel strange and uncomfortable, like you’re bragging. We assure you, however, that it’s necessary to get your target’s attention. They’re not thinking you’re bragging. They’re thinking, “Is this worth my time?” Your job is to convince them that it is.

Also, don’t forget to link to your LinkedIn or personal website. Attaching a resume may feel too heavy-handed for a first conversation, as we discussed above.

Here's an example of a prospective intern, leveraging both social proof and accomplishments, to write a compelling email. His email isn't super personalized, but he did make some effort to say that what we do at interviewing.io is important.

![Example 2 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_3_393cc556e3.png)

#### Formulating a strong call to action

**A call to action is an invitation for the recipient to do something. You can go one of two ways with your call to action: ask for a job interview or start a conversation.** Which you do should be a function of how much firepower you have in the way of social proof and accomplishments. It’s not fair, but if you can get your target’s attention with one or both of those, being bold and asking for a job interview makes sense. This approach can be effective, but it won’t work for most people… because most people don’t have enough social proof or accomplishments to justify this type of request.

If you can’t leverage social proof or accomplishments, you’re going to have to work harder and bank entirely on building common ground, which will likely take some time and effort and involve a live conversation before they’re convinced to expend their social capital on you.

If you’re asking for an interview, just come right out and say it. You can use the intern candidate’s email from earlier as a guide. However, this isn’t our preferred way to do it, and we really recommend starting a conversation instead.

Take a look at the email below.

![Example 3 of a good cold email](https://strapi-iio.s3.us-west-2.amazonaws.com/good_cold_email_1_38095f2d40.png)

In this email, the candidate doesn’t ask me about jobs — he just asks to meet to discuss a topic. Indeed, he’s done his research. I write a *ton* about judging resumes, and it’s a topic I could go on about for hours if you’ll let me. His email read like he’s genuinely interested in the subject and that we’d have a good conversation, so of course I responded. You’d be surprised how rare emails like this are. If you can find the topic your target cares about and write something that shows earnest, genuine interest, they’ll respond.

**With these emails, you’re asking for a conversation, not a job interview… because the conversation is what will hopefully prove to the hiring manager that you’re worth interviewing. *Then*, once you have a conversation, the hiring manager will walk away with the impression that you’re a competent, thoughtful human being who’s interested in this sort of work. From there, getting a job interview will feel like an afterthought.**

As such, don’t talk about jobs at all in this type of email, and in this particular case, don’t attach your resume — that will feel out of place and transactional. You can and should link to your LinkedIn so they know who you are and have some context. But spend the bulk of the email building common ground and coming up with an interesting reason for the two of you to talk.

**This approach is much more effective than asking for an interview out of the gate!** You’re not going to land a job from one email, so, as with any seemingly insurmountable goal, it’s important to think of your outreach as a series of steps where you put one foot in front of the other. Like in sales, all you need is to get to a conversation.

If your call to action is to set up a time to talk (which it probably should be because it’s specific), we recommend providing them with a time window. "Would you want to meet up sometime?" puts the burden on the recipient to pose a time, while "Can we talk next Monday at 3pm?" is problematic because, most likely, they aren't free then. Instead, try something like the candidate above did: "Would you be available sometime within the next two weeks for a thirty-minute call? I'm free most weekdays between X and Y and can pretty much do any time on weekends if those are better for you."

## Two templates for you to use

**Below are two templates you can use for cold outreach. The first one is ideal but requires more effort and can't always be used. The second one is weaker but more generic.** You can choose what fits your needs best. We expect both of these templates to be far more effective than throwing your resume into the blackhole of online portals.

### Template #1: Use this template if your target has an online presence

This template includes common ground, accomplishments/social proof, and a call to action. It will get you the highest response rates, possibly anywhere from 25-50%. However, it can be challenging to use because it requires you to 1) do a deep dive into their online presence and 2) tie what you find back to something you’re doing. Sometimes, that tie-in might be tenuous or non-existent (in which case, maybe skip it).

I’ve read your work on {insert some details about their writing}, and I {insert your thoughts on the work}.

{If you can make the connection between their work and yours, talk about something similar you’ve been working on.}

{If you cannot, ask them a specific, thoughtful question about your work. Don’t worry about making it “the perfect question” like you might when you attend a talk and want to sound smart. Any earnest question will do. You don’t have to use this as a chance to show off!}

{Finally, close with a sentence or two about you, if you have some social proof or impressive accomplishments you can share.}

Note that in this template, we leave some places for you to insert some social proof and your accomplishments. Even though this email is primarily about them and their work, and your references to yourself are primarily through that lens, it never hurts to drop in a few pieces of evidence that you’re someone who’s accomplished things and/or someone who looks good on paper.

### Template #2: Use this template if you don’t have anything except a LinkedIn profile for your target

The reality is that you won’t always have enough information about your target to find common ground. In this case, you’ll lead with accomplishments/social proof and a strong call to action. We expect this template will get you response rates anywhere from 5-25%, depending on the strength of your achievements and pedigree. That said, we recommend treating this template as a last resort. Using it means you’ve exhausted any possibility of writing something personal.

{List 2 things about you. They can be impressive accomplishments of yours or social proof, as above.}

I’m really interested in the work you’re doing at {Company Name}. {If you know what team they’re on and are interested in that specific team or are familiar with that team’s accomplishments, great! If not, just write a few earnest sentences about why the company is interesting to you.}

Keep your note short. The intent here is to make your target believe you’re an entity worth paying attention to, rather than them doing the easy thing: deleting your email.

Regardless of which template you use, just like you have to manage your psychology when you prepare for technical interviews, you have to manage your psychology when doing outreach like this. You have to:

- Mentally prepare yourself for the slog of writing personalized emails and doing the requisite research.
- Get used to rejection. If you do write good emails and target the right people, you’ll have a much better hit rate than when you apply online, but you will still get ghosted a lot, and it will sting much more because, this time, you actually tried. But you know what? If you stick with it and do this right, within a few months, you’ll have a connection to a top-tier company.

Now that you’ve girded your proverbial loins, it’s time to do the work. If you follow our advice, you’ll get 1-2 orders of magnitude more responses than from applying online, and with this approach, you’ll have at least a hiring manager at that company rooting for you!

1. RocketReach also has a LinkedIn-like faceted search you can use to find engineering managers, but we’ve found that it’s not nearly as reliable or rich as LinkedIn, which is why we recommend using LinkedIn for search and then RocketReach for email discovery. [↩](#user-content-fnref-1)
2. Recruiters should not contact candidates on their work email address, but that's because they're trying to make the candidate leave their job. You are trying to join the manager, which is why it's okay to use their work email address. [↩](#user-content-fnref-2)
3. Many people think that for something to be worth mentioning, it has to have gone viral. That’s simply not correct — in our niche space, a few hundred likes or a few thousand upvotes is already really impressive. [↩](#user-content-fnref-3)

### [Lucky Numbers in a Matrix](/questions/lucky-numbers-in-a-matrix)

[Given an m x n matrix of distinct numbers, return all lucky numbers in the matrix in any order.](/questions/lucky-numbers-in-a-matrix)


# [LinkedIn endorsements are dumb. Here’s the data.](https://interviewing.io/blog/linkedin-endorsements-useless)

By Aline Lerner | Published: February 26, 2017; Last updated: May 1, 2023

If you’re an engineer who’s been endorsed on LinkedIn for any number of languages/frameworks/skills, you’ve probably noticed that something isn’t quite right. Maybe they’re frameworks you’ve never touched or languages you haven’t used since freshman year of college. No matter the specifics, you’re probably at least a bit wary of the value of the LinkedIn endorsements feature. The internets, too, don’t disappoint in enumerating some [absurd potential endorsements](https://www.socialtalent.co/blog/endorsement-bombing) or in [bemoaning the lack of relevance of said endorsements](https://news.ycombinator.com/item?id=6292348), even when they’re given in earnest.

Having a gut feeling for this is one thing, but we were curious about whether we could actually come up with some numbers that showed how useless endorsements can be, and we weren’t disappointed. If you want graphs and numbers, scroll down to the “Here’s the data” section below. Otherwise, humor me and read my completely speculative take on why endorsements exist in the first place.

## LinkedIn endorsements are just noisy crowdsourced tagging

Pretend for a moment that you’re a recruiter who’s been tasked with filling an engineering role. You’re one of many people who pays LinkedIn ~$9K/year for a recruiter seat on their platform. That hefty price tag broadens your search radius (which is otherwise artificially constrained) and lets you search the entire system. Let’s say you have to find a strong back-end engineer. How do you begin?

Unfortunately, LinkedIn’s faceted search (pictured below) doesn’t come with a “can code” filter.

So, instead of searching for what you really want, you have to rely on proxies. Some obvious proxies, [even though they’re not that great](https://interviewing.io/blog/lessons-from-3000-technical-interviews), might be where someone went to school or where they’ve worked before. **However, if you need to look for engineering ability, you’re going to have to get more specific. If you’re like most recruiters, you’ll first look for the main programming language your company uses** (despite [knowledge of a specific language not being a good indicator of programming ability](https://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/) and despite most hiring managers not caring which languages their engineers know) and then go from there.

**Now pretend you’re LinkedIn. You have no data about how good people are at coding, and though you do have a lot of resume/biographical data, that doesn’t tell the whole story.** You can try relying on engineers filling in their own profiles with languages they know, but given that engineers tend to be pretty skittish about filling in their LinkedIn profile with a bunch of buzzwords, what do you do?

**You build a crowdsourced tagger, of course! Then, all of a sudden, your users will do your work for you.** Why do I think this is the case? Well, if LinkedIn cared about true endorsements rather than perpetuating the skills-based myth that keeps recruiters in their ecosystem, they could have written a weighted endorsement system by now, at the very least. That way, an endorsement from someone with expertise in some field might mean more than an endorsement from your mom (unless, of course, she’s an expert in the field).

But they don’t do that, or at least they don’t surface it in candidate search. It’s not worth it. Because the point of endorsements isn’t to get at the truth. It’s to keep recruiters feeling like they’re getting value out of the faceted search they’re paying almost $10K per seat for. In other words, improving the fidelity of endorsements would likely cannibalize LinkedIn’s revenue.

You could make the counterargument that despite the noise, LinkedIn endorsements still carry enough signal to be a useful first-pass filter and that having them is more useful than not having them. This is the question I was curious about, so I decided to cross-reference our users’ interview data with their LinkedIn endorsements.

So, what data do we have? First, for context, interviewing.io is a platform where people can practice technical interviewing anonymously with interviewers from top companies and, in the process, find jobs. Do well in practice, and you get guaranteed (and anonymous!) technical interviews at companies like Uber, Twitch, Lyft, and more. Over the course of our existence, we’ve amassed performance data from close to 5,000 real and practice interviews.

When an interviewer and an interviewee match on our platform, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question. Interview questions on the platform tend to fall into the category of what you’d encounter at a phone screen for a back-end software engineering role. Some examples of these interviews can be found on our [public recordings](https://interviewing.io/mocks) page.

![Screenshot of Interviewing.io interview feedback form highlighting the question: how were their technical skills?](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ftech_skills_109973cdf5.png&w=1920&q=75 "Interviewing.io interview feedback form")

As promised, I cross-referenced our data with our users’ LinkedIn profiles and found some interesting, albeit not that surprising, stuff.

## Endorsements vs. what languages people actually program in

The first thing I looked at was whether the programming language people interviewed in most frequently had any relationship to the programming language for which they were most endorsed. It was nice that, across the board, people tended to prefer one language for their interviews, so we didn’t really have a lot of edge cases to contend with.

**It turns out that people’s interview language of choice matched their most endorsed language on LinkedIn just under 50% of the time.**

Of course, just because you’ve been endorsed a lot for a specific language doesn’t mean that you’re not good at the other languages you’ve been endorsed for. To dig deeper, I took a look at whether our users had been endorsed for their interview language of choice at all. It turns out that people were endorsed for their language of choice 72% of the time. This isn’t a particularly powerful statement, though, because most people on our platform have been endorsed for at least 5 programming languages.

That said, even when an engineer had been endorsed for their interview language of choice, that language appeared in their “featured skills” section only 31% of the time. This means that **most of the time, recruiters would have to click “View more”** (see below) **to see the language that people prefer to code in, if it’s even listed in the first place**.

![Screenshot showing a list of Featured Skills & Endorsements](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Flinkedin_endorsement_270b56e616.png&w=1920&q=75 "LinkedIn: Featured Skills & Endorsements")

So, how often were people endorsed for their language of choice? Quantifying endorsements is a bit fuzzy, but to answer this meaningfully, I looked at how often people were endorsed for that language relative to how often they were endorsed for their most-endorsed language, in the cases when the two languages weren’t the same (recall that this happened about half the time). Perhaps if these numbers were close to 1 most of the time, then endorsements might carry some signal. As you can see in the histogram below, this was not the case at all.

The x-axis above is how often people were endorsed for their interview language of choice relative to their most-endorsed language. The bars on the left are cases when someone was barely endorsed for their language of choice, and all the way to right are cases when people were endorsed for both languages equally as often. All told, the distribution is actually pretty uniform, making for more noise than signal.

## LinkedIn endorsements vs. interview performance

The next thing I looked at was whether there was any correspondence between how heavily endorsed someone was on LinkedIn and their interview performance. This time, to quantify the strength of someone’s endorsements, I looked at how many times someone was endorsed for their most-endorsed language and correlated that to their average technical score in interviews on interviewing.io.

Below, you can see a scatter plot of technical ability vs. LinkedIn endorsements, as well as my attempt to fit a line through it. **As you can see, the R^2 is piss-poor, meaning that there isn’t a relationship between how heavily endorsed someone is and their technical ability to speak of.**

## LinkedIn Endorsements vs. no endorsements… and closing thoughts

Lastly, I took a look at whether having any endorsements in the first place mattered with respect to interview performance. If I’m honest, I was hoping there’d be a negative correlation, i.e. if you don’t have endorsements, you’re a better coder. After running some significance testing, though, **it became clear that having any endorsements at all (or not) doesn’t matter.**

So, where does this leave us? As long as there’s money to be made in peddling low-signal proxies, endorsements won’t go away and probably won’t get much better. It is my hope, though, that any recruiters reading this will take a second look at the candidates they’re sourcing and try to, where possible, look at each candidate as more than the sum of their buzzword parts.

*Thanks to Liz Graves for her help with the data annotation for this post.*


# [We co-wrote the official sequel to Cracking the Coding Interview!](https://interviewing.io/blog/we-co-wrote-the-official-sequel-to-cracking-the-coding-interview-introducing-beyond-ctci)

By Aline Lerner | Published: January 26, 2025; Last updated: April 27, 2025

**EDIT 2:** [Read 9 chapters of the book for free!](https://bctci.co/free-chapters) These include:

- The first seven chapters of the book, covering topics such as why technical interviews are broken, what recruiters won't tell you, why not to spend a lot of time on resumes, and how to get in the door at companies without a referral.
- Two technical chapters: Sliding Windows and Binary Search. Our new take on Binary Search teaches one template that works for every binary search problem on LeetCode, with only a single-line change you need to remember. The Sliding Windows chapter features 6 unique sliding window templates that make off-by-one errors a thing of the past.

**EDIT:** [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) is out now! Here's the table of contents:

![Beyond CtCI table of contents](https://strapi-iio.s3.us-west-2.amazonaws.com/cover_toc_b77bec4593.png)

I have some exciting news. Along with Gayle Laakmann McDowell, Mike Mroczka, and Nil Mamano, I’m writing the official sequel to *Cracking the Coding Interview* (often called the bible of technical interview prep). It's fittingly called [*Beyond Cracking the Coding Interview.*](https://www.amazon.com/dp/195570600X)

I’ve always wanted to write a book about technical interviewing. And this is it. And of course it'll draw on all the hiring data we've collected over the past decade at interviewing.io.

Technical interviews are much harder today than they used to be. Engineers study for months and routinely get down-leveled despite that. [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X), in addition to covering a bunch of new questions and topics, teaches you how to think instead of memorizing. Grinding and memorization isn’t the way in this market (though in fairness, it’s never really the way). With us, you’ll still have to do the work, of course, but we’ll teach you to work smarter.

[![Beyond Cracking the Coding Interview book cover](https://strapi-iio.s3.us-west-2.amazonaws.com/book_cover_87f67665fd.png)](https://www.amazon.com/dp/195570600X)

We added at least thirteen new technical topics (I say “at least” because we’re still writing, and it might be more like twenty)—and over 150 new problems. Each problem includes step-by-step walkthroughs, and you can work each problem with our (actually good) AI Interviewer. And of course this book was written in partnership with interviewing.io. We’ve pulled in data from over 100k FAANG mock interviews on interviewing.io, and we include hundreds of curated interview replays from interviewing.io (shared with permission of course) – watch people make mistakes and learn so you’re not doomed to repeat them.

But it’s not *just* about interview prep. In today’s job market, the bar is higher but it’s also harder than ever to get noticed and run your job search end-to-end. My excellent co-authors killed it on the technical chapters. I focused on writing the job search stuff, including, but not limited to:

- How to negotiate, exactly what to say, and how to not screw up your negotiations before they even start
- How to manage your job search, end to end, and balance interview prep with applications and outreach
- A worksheet to help you figure out what order you need to engage with the companies you’re targeting to ensure that all your offers come in at the same time
- How to get in the door at top companies without relying on referrals, including email templates and examples of good and bad outreach
- An internal look at FAANG (and other) company rubrics to help understand what interviewers really care about, no matter what company you're applying to
- What you need to know about behavioral interviews, whether you want to or not, and how to avoid the mistakes that even great engineers make
- A list of very specific questions to ask your interviewers (not just to look smart but to learn useful things)
- How technical interviews got to be so broken and how to get over hating them so you can win

I also spend some time on owning and sharing data on how flawed technical interviewing is and, most importantly, how to manage your psychology so you can get past that. I see so many engineers opting out of this interview style, arguably for good reason. But you’re leaving a lot of good opportunities on the table, and it doesn’t have to be like that.

This book is so much of what I’ve blogged about for the last 15 years, but it’s fleshed out with much more detail and actionable advice. If you read it, let me know what you think. Technical interviewing sucks (and so does looking for a job). But this book will help you do it well and get out alive.

**Purchases of [*Beyond Cracking the Coding Interview*](https://www.amazon.com/dp/195570600X) get a $50 discount for interviewing.io. The book costs $45, so it’s not a bad deal. The book is out in January of 2025, and you can [get it on Amazon](https://www.amazon.com/dp/195570600X).**


# [Why you shouldn’t list certifications on LinkedIn](https://interviewing.io/blog/why-you-shouldnt-list-certifications-on-linkedIn)

By Aline Lerner | Published: March 13, 2023; Last updated: May 15, 2023

People often suggest that interviewing.io should create a certification that our users can post on their LinkedIn profile, e.g., something like “Top 10% performer on interviewing.io”. Presumably, these certifications would signal to recruiters that this person is a good engineer and worth reaching out to and should carry more signal than where said person went to school or worked previously.

I think certifications are a terrible idea, and I’ve resisted building them. Simply put, the incentives they create for engineers and recruiters are all wrong. To explain what I mean, let’s split engineers into two distinct personas:

- **The engineer who looks good on paper. This person will simply not list the certification on their profile – they have no reason to!** They’re already getting contacted by at least 10 recruiters a day. Chances are, they’re actively ignoring LinkedIn unless they’ve decided that they’re looking and want to respond to a handful of the hundreds of InMails they’ve gotten already to give themselves more optionality during their job search.
- **The engineer who doesn’t look good on paper. This person will likely add a certification to their profile. It’s rational – anything that helps them stand out and legitimize their profile, in the absence of traditional pedigree, is a win. That sounds great, right? Unfortunately, here’s the rub. Unless your certification is respected by recruiters and well-established, they will not take it seriously… because it runs counter to everything they’ve been tuned to look for**. If your profile doesn’t have a reputable school or a top company on it, then an unknown certification won’t save you, and over time, listing it will do harm. Because the certification will almost always go hand in hand with lack of pedigree (as you saw above, people who look good on paper will have no reason to list it on their profiles), recruiters will start to develop negative associations with it. Because most recruiters will not engage with these candidates for long enough to find out if they’re good or not, this will happen even if the certification actually carries a positive signal. In other words, recruiters will learn, over time, to associate the presence of the certification with a “no-go” because it will only be on profiles that have previously been trained not to reach out to.

That’s the theory of why certifications are bad. They’re bad for the individuals listing them, and they’re bad for the industry as a whole because, ironically, they make it harder to find good candidates. But what happens when you look at the data?

Engineers use interviewing.io for anonymous mock interviews. If things go well, they skip right to the technical interview at real companies (which is also fully anonymous). We started interviewing.io because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.

At this point, we’ve hosted over 100k technical interviews, split between mocks and real ones.

Regardless of the interview type, when an interviewer and an interviewee match on our platform, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/unnamed_1_d9f00a2248.png)
**In this post, we aggregated scores from these interviews for each interviewee and then cross-referenced how many certifications they listed on their LinkedIn profiles.**

You might say that an engineer’s performance in interviews on our platform isn’t the canonical source of truth for their engineering ability, and you’d certainly be right. In the absence of holistic performance review data about our users, which is pretty much impossible to get, we decided running this study was still worthwhile. For what it’s worth, I have pretty high conviction that performance in interviews on our platform correlates very strongly with performance in interviews in the real world – our candidates tend to pass real interviews 3X better than candidates from other sources. Fully closing that loop with on-the-job performance data is the holy grail of any recruiting enterprise, and I hope we can do it one day.

Caveats aside, before doing the analysis, our hypothesis was that having one or more certifications on your profile would have a strong negative correlation with your interview performance. Why?

1. We’ve seen in the past that though [having attended a top school doesn’t correlate with interview performance](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt), having worked at a top employer does. We saw above that people who look good on paper aren’t going to be incentivized to put certifications on their profile, which means that we’ve just cut a lot of top performers from the pool.
2. This leaves people who don’t look good on paper. Because top performers aren’t evenly distributed between pedigreed and unpedigreed candidates, there will be fewer top performers in this pool. But even if they were, by definition, only a small part of this population will be top performers. Therefore, most of the people in this pool will not be top performers, which means that most of the people who are incentivized to list certifications on their profile will not be top performers.

**Basically, people who look good on paper are disincentivized to list certifications. People who don’t look good on paper are incentivized to list them, but most of them are not top performers. Therefore, most of the people listing certifications are not top performers.**

If we’re right, not only is creating certifications not useful, but doing so will also have the unfortunate side effect of making recruiters dig in their heels about the importance of pedigree, will, over time, reduce the marginal utility of any new certifications and will ultimately harm attempts at making technical recruiting more fair or meritocratic.

## What the data actually says

In this analysis, we took a list of our users for whom we had interview data and, where possible, analyzed their LinkedIn profiles. **We ended up analyzing about 20K LinkedIn profiles, 28% of which had some kind of certification. We then pulled out the top 10 most frequent certification authorities, so we could break them out and do some more granular analysis.** These were (in order of frequency, i.e. Coursera had the most hits):

- Coursera
- LinkedIn
- Triplebyte
- Microsoft
- Amazon Web Services (AWS)
- Oracle
- Udacity
- Udemy
- HackerRank
- Cisco

Because people usually do multiple interviews on our platform, we ended up with about 40k observations (i.e., interviews) in each regression.

Our first result is that people with certifications do worse in interviews, as shown in the bar chart below. **People with certifications on their LinkedIn profiles pass interviews on our platform about 53% of the time versus 57% of the time for people without certifications, a very statistically significant difference (p < 0.00001).** Remember that these interviews are completely anonymous. The interviewer isn’t basing their ratings on the person’s LinkedIn—just their interview performance.

Want to make sure your interview performance is in tip-top shape, regardless of your LinkedIn? Sign up for anonymous mock interviews with engineers from top companies.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_eb27700235.png)

**Notably, the certification “penalty” is equally large whether people had the certifications up in 2021 or 2023. So there’s no sign that today’s depressed labor market changed the nature of the signal.**

How much of this difference is due to the fact that people with certifications do different kinds of interviews? We next adjusted for the language (e.g., Python, Java) and focus (e.g., frontend, machine learning) of the interview and only compare Java coders to Java coders, Python to Python, etc. This control ensures that the results are not driven by broad patterns at the group level, asking whether certifications are predictive of performance compared only to candidates coding in the same language. If anything, the “Interview language & focus controls” bar shows that this makes certifications look slightly worse. When you compare certified people to non-certified people within the narrow types of interviews they typically choose, they lag just a bit further behind.

Next, we wanted to know how much of this difference is explainable by the attributes of the person. For example, people who seek out certifications may have a non-quantitative background. Perhaps they majored in communications rather than computer science. Or they are a paralegal trying to switch career paths. **Indeed, we observe this pattern in the data: people with non-traditional backgrounds are about 30% more likely to have a certification.**

To account for this selection, we just compared people with similar backgrounds. For instance, does a Harvard graduate with a certification do better or worse than a Harvard graduate without one? This correction shrinks the gap by about 40 percent (see the “Pedigree controls” bar).

![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_controls_592ab3c690.png)

Note: All differences between the certified and uncertified users are statistically significant (p < .01 or smaller).

**So, LinkedIn certifications are indeed a negative tag for candidates on our platform. This isn’t explained by the kinds of interviews they do. But we can show that part of it is due to the fact that certified people tend to have non-traditional backgrounds. The remainder of the gap is probably due to similar dynamics: you get certified if you have something to prove.**

### Not all certifications are created equal

This analysis treats certifications as binary. Either you have it or you don’t. But there are a range of authorities out there that give certifications: are any of them a positive tag?

We did similar regression analysis for the top ten certifiers in the data. The results are below:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/certification_effect_by_company_47d9163301.png)

The one standout is Triplebyte. Their graduates are 6 percentage points more likely to pass interviews —a serious boost, albeit not enough to dispel the negative signal that all the others carry. The worst is the Cisco badge, with a 10 percentage point drop in performance.

When we dug into the data, we saw that people from non-traditional backgrounds do indeed list certifications on their LinkedIn profile more often than their well-pedigreed counterparts. People whose most recent schooling is a web development certificate or associate's degree score are about 30% more likely to display a certification.

We also saw that, generally speaking, certifications carry a negative signal and that these results hold up even in the increasingly employer-favorable 2023 job market (in other words, good candidates haven’t suddenly started listing certifications on their profiles to get noticed).

As we expected, these realities create an unfortunate feedback loop. Recruiters tend to value pedigree above all else, which means they’re less likely to talk to non-traditional candidates. When they see non-traditional candidate profiles with certifications, because they weren’t going to talk to them anyway, over time they’ll develop a negative association with those certifications.

Furthermore, given that people who list certifications are more likely to perform worse in interviews, when they choose pedigreed candidates who have a certification and those candidates perform worse, that negative association will be strengthened.

Because of these mechanics, certifications get reinforced as bad in recruiters’ minds, and listing them on your profile ends up being a counterproductive strategy for diamonds in the rough, the very candidates whom certifications were supposed to help in the first place.

### [LRU Cache](/questions/lru-cache)

[Implement an LRU Cache
LRU = Least recently used cache](/questions/lru-cache)


# [How to get in the door at top companies: a data-driven, practical guide for software engineers. Part 1.](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-part-1)

By Aline Lerner | Published: July 23, 2024; Last updated: October 28, 2024

*In this post (part 1 of 2), we’ll share some data about which channels are most effective for getting into the door at great companies and why. In [part 2](https://interviewing.io/blog/how-to-get-in-the-door-at-top-companies-cold-out-reach-to-hiring-managers-part-2), we get very tactical and tell you exactly what to say and do to get responses.*

interviewing.io is an anonymous mock interview platform — we help engineers prepare for technical interviews by pairing them with senior FAANG and FAANG-adjacent engineers for mock interviews and feedback. In this market, many of our users are struggling with getting in the door at companies, so we ran a survey to see what’s worked well for our users and what hasn’t, in today’s difficult climate.

In our survey, we gave people the following channels for getting into companies and asked them which were the most and least effective:

- In-house recruiters contact you
- Apply online
- Warm referrals
- Cold referrals
- Cold outreach to hiring managers
- Cold outreach to recruiters
- Agency recruiters contact you

We also asked them which types of companies they got responses from:

- FAANG
- FAANG-adjacent (e.g., Stripe, Dropbox, OpenAI, Uber)
- Large startups
- Small startups

We got ~500 responses. Among survey respondents, which channels were most effective was largely consistent, regardless of company type, but there were some twists depending on who the candidates were. More on that in a bit.

Below are the channels, ranked by effectiveness.[1](#user-content-fn-1) When more people found a channel effective than ineffective, it ended up in the first list. When more people found a channel to be ineffective than effective, it ended up on the second list.

**Recruiting channels that our users found to be effective (ranked from most to least effective):**

- Warm referrals
- In-house recruiters contact you
- Apply online
- Cold outreach to hiring managers

**Recruiting channels that our users found to be ineffective (ranked from least to most ineffective):**

- Cold outreach to recruiters
- Cold referrals (referrals from people you don’t know)
- Agency recruiters contact you

This data came primarily from surveying experienced engineers (4+ years), rather than juniors (we don’t have that many juniors on our platform; average years of experience is 8). If you’re a junior engineer in this market, you already know you’re in for a tough time, and we’d advise you to take your destiny into your own hands as much as possible by reaching out directly to hiring managers (the same advice we give many of our more experienced users). More on that later in the post.

Interestingly, these results were quite consistent between company types. In other words, channels that worked well for FAANGs tended to work well for startups and vice versa.

Overall, the most useful channels were in-house recruiters (when they reached out to you) and warm referrals. Unfortunately, both of these channels are somewhat out of your control. You have very little control over whether internal recruiters reach out to you. There are *some* things you can do to increase the chances, but they’re all tied up in your work history and identity, neither of which you can easily change. We’ll talk more about that later on.

Warm referrals (i.e., referrals from people you know), on the other hand, are very useful and are a *bit* more in your control, but they still depend on the quality of your network.

Below is a diagram comparing the utility of all the channels to how much control you have over them.

You might wonder why we speculatively singled out cold outreach to hiring managers as something that can be done right, versus the other channels. In our experience, that channel is both misused and underutilized and is the best bet for many candidates, provided they do it correctly. In our next post, we’ll tell you exactly what to do and what to say when you reach out to hiring managers — especially if you come from a non-traditional background and aren’t getting a lot of recruiter outreach/don’t have the network to get warm referrals, reaching out to hiring managers is your absolute best bet.
Now let’s look at each channel in detail.

### In-house recruiters contact you

This channel is one of the two where you have the least amount of control (the other is agency recruiters contacting you, though that one is way less useful).

So, how much control do you have over this channel? One bit of analysis we did on our survey data was to try to find patterns in the background of people who find in-house recruiters particularly useful. Not too surprisingly, some patterns did emerge.

In-house recruiters are most likely to contact you if:

- You look good on paper, i.e., you have top-tier companies and/or schools on your resume (in our experience, companies matter more)
- You belong to a group that’s been traditionally underrepresented in tech (i.e., you’re a woman or a person of color)
- To *some* extent, though less than the two bullets above, if you have niche skills (e.g., ML engineering)

These results aren’t unique to just this survey. We [recently did a study](https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes) where we asked a bunch of recruiters to look at resumes and tell us which candidates they’d want to interview. While the intent of the study was to see if recruiters are good at identifying talent (spoiler: they were barely better than a coin flip), we learned some other interesting things, including what recruiters actually look for when they read a resume.

The two most sought-after resume traits were: 1) experience at a top-tier tech company (FAANG or FAANG-adjacent) and 2) URM (underrepresented minority) status (in tech, this means being Black or Hispanic).

This mirrored what we saw in our user survey when we looked at commonalities among candidates who got value from in-house recruiters.

So how do you use this information to your advantage? You obviously can’t magic FAANG/FAANG-adjacent experience or URM status out of thin air[2](#user-content-fn-2), but if you do have either, our pragmatic advice is to highlight it and make it really easy for recruiters to spot. Of course, whether you *want* to lead with URM status is a personal decision. We’ve heard differing opinions on this and are not here to judge. All we can do is share the data — do with it what you will.

So, how do you make sure that, say, your FAANG experience stands out to recruiters? Take a look at the before and after screenshots of the resume below[3](#user-content-fn-3). This resume belongs to one of our users who was kind enough to let us share it. He actually has two of the three things that recruiters look for: FAANG experience and a niche title (ML engineer). But both are buried! And the section that gets the most attention is wasted on undergraduate awards.

#### Before

### Apply online

If you’ve ever applied to jobs online, then you know it’s kind of like screaming into a black hole. Though, according to our survey, some candidates (specifically people applying to FAANG/FAANG-adjacent companies and small startups) get some value out of this channel, it’s still a numbers game. And for large startups, it’s a losing proposition.

According to recruiting tool Gem, applicants that come from recruiter outreach (called “outbound” in recruiter lingo) are [6 - 10X more likely to get hired](https://www.gem.com/blog/outbound-candidates-more-likely-to-be-hired) than applicants who apply online (called “inbound”).

As Lyft recruiting manager Nate Wylie put it:

> Our data… showed higher pass-through rates for candidates [we reached out to] at each stage of the interview process vs. applicants via the careers page. It’s not that we want to ignore applicants; it’s just that historically we don’t get what we’re looking for — and with speed — through that channel.

Having been a recruiter myself, I can confirm that many companies do indeed ignore their online careers page. Many years ago, when I first joined the recruiting team at a top-tier startup, I spent my first few days going through the resumes of people who had applied online. I found a treasure trove of candidates, including very experienced applicants from top-tier companies.[4](#user-content-fn-4) But no one had seen these applicants because no one had been monitoring inbound activity for months!

The silver lining here is that when you don’t hear back from a company (or even when you get an automatic rejection email wishing you "the best in your future endeavors"), it’s not because a human looked at your resume and made a deliberate, thoughtful decision about you. It’s tempting to think that way because it plays so well into our insecurities. The reality is that a human probably never saw your resume in the first place.

So why do people apply online, despite knowing in their gut that it’s not an effective strategy? Simply put, it’s predictable and easy. You get into a routine, you upload your resume, you connect your LinkedIn, and you can knock out hundreds of applications in a matter of hours.

The other encouraging thing about this channel is that, when we analyzed specifically which types of candidates had success with it, we couldn’t find any patterns — the channel worked equally well (poorly?) for people who looked good on paper vs. not, and there was no preferential treatment for traditionally underrepresented groups in tech (e.g., women and people of color).

**TL;DR Applying online doesn’t hurt… provided that you don’t take rejection personally. If you do, it’ll wear you down over time.**

### Warm referrals

Warm referrals are, of course, excellent. That is, assuming it's a *real* referral — someone who can actually vouch for you, and ideally your work.

Per capita, referrals are most companies’ best source of candidates, and they were a great channel for our users across all company types (they were the best channel for FAANGs/FAANG-adjacents, as well as large startups, and second best for small startups, behind in-house recruiters reaching out).

If you have the network, you should absolutely use it. Of course, it’s unlikely that you’ll have meaningful connections at every company you want to work at. What do you do then?

### Cold referrals

Should you ask people you don't know to refer you? Our survey data says probably not. Cold referrals were net negative for both FAANG and small startups and neutral for large startups.

Years ago, trying to collect cold referrals was a decent strategy. You could track down someone at the company and ask them to toss your proverbial hat into the ring

Engineers were often happy to refer someone — even someone they didn't know — either to be kind, to avoid the awkwardness of declining, or to collect the potential referral bonus. They couldn't vouch for you, but the referral would ensure that a human looked at your resume.

This became so common that Blind actually spun out an entire referral marketplace called [Rooftop Slushie](https://theamericangenius.com/adj/rooftop-slushie/) (the link is to some press because the actual site is now defunct), where applicants would pay a small sum for a referral.

Then, companies wised up and realized that these referrals weren't all that different from normal in-bound applicants. So why treat them differently?
Many companies nowadays separate referrals into "true referrals" and "leads." It’s great for maintaining the delicate dance of social dynamics, but it’s completely useless for getting hired — dropping someone’s resume into the “leads” pile is basically the same as throwing it into the inbound black hole.

Given that cold referrals aren’t zero effort, our advice is to expend that energy elsewhere. More on that shortly.

### Agency recruiters

Agency recruiters were the worst channel overall, according to our survey, and were net negative for all company types.

FAANGs and FAANG-adjacent companies tend to rely less on agencies than startups, and when they do, it’s to fill some very specific need (rather than “Hey we need more SWEs”), so it’s not surprising that our users didn’t get much value from this channel when applying to FAANGs.

While both large and small startups use agencies liberally, clearly the value to candidates is limited.[5](#user-content-fn-5) Out of all of our survey respondents, only a handful of our users said that agencies were useful to them, and of those who mentioned agencies, the majority said that they were the worst channel.

We won’t belabor the point, but it’s probably not in your best interest to spend much time on working with agency recruiters. It has opportunity cost and not much upside. And you can [routinely get screwed in salary negotiations when you work with an agency recruiter](https://interviewing.io/blog/sabotage-salary-negotiation-before-even-start), if you even get that far.

### Cold outreach (to hiring managers vs. recruiters)

Not all cold outreach is created equal, for two reasons. First, there’s your audience: hiring managers vs. recruiters. And then there’s the quality of the outreach itself. We’ll come back to how to write the kinds of messages that will get you responses. First, let’s talk about the audience.

You can see in our survey results that cold outreach to hiring managers was net positive for FAANG/FAANG-adjacent companies and neutral for the other company types. Cold outreach to recruiters, on the other hand, was net negative for both FAANG/FAANG-adjacents and small startups and neutral for large startups.

Ignoring the quality of the outreach for a moment, which we expect is probably comparable for both types, why does this difference exist?

If you had to answer the question of who’s the right person to reach out to about jobs, your gut instinct might be to say it’s recruiters. After all, hiring is officially their job! However, that’s not strictly true. Recruiters are not incentivized to make hires, at least not directly. Just like everyone else, recruiters’ main priority is to keep their jobs.

#### Cold outreach to recruiters doesn’t work

How does a recruiter keep their job?[6](#user-content-fn-6) By *bringing in the types of candidates that their manager tasked them with*. How is that different from hiring? Hiring implies that you’re evaluated on whether the people you bring in actually get hired, but most in-house recruiters aren’t evaluated this way… because it takes too long.

Instead, recruiters are sometimes evaluated on what portion of their candidates get offers or get to onsite. However, because of drop-off and latency (getting an offer can still take months), your organization has to be pretty good at tracking. Many are not.

As such, many recruiting orgs prefer simpler, faster metrics:

- Of the candidates you reached out to, how many responded?
- Of those who responded, how many resulted in a first conversation?

The downside of measuring success in a single part of the funnel is that you don’t incentivize people to care about what happens downstream (that is, how many are hired). This would be like if marketers only paid attention to ad clicks, rather than actual purchases. But that’s how recruiting operates: individuals aren’t really incentivized to care what happens downstream.

So, if you are typically just measuring the response rates of your reports, as a recruiting manager, you have to set some guardrails for the types of candidates that you want your team to reach out to. If you don’t, they’ll end up just reaching out to people who are likely to respond instead of people who are a good fit for the job.

Unfortunately, you don’t know who is a good fit for the job. You can’t just say, “Go on LinkedIn, and find me good engineers.”

That doesn’t exist. So instead, you come up with some rules that look like this:

- Senior engineers
  - Juniors and intermediate engineers would answer outreach a lot, but they already apply online in droves, so we don’t need to pay people to go out and find them. Moreover, we have a whole university department that deals with college hires once a year in September.
- Went to a top school (in the absence of a better filter, this works OK)
- Worked at a top company

There may be a few other items on the list if the role requires specific skills (e.g., Android development), but by and large, that’s what recruiters are tasked with, and that’s what they’re focused on.

It seems counterintuitive, but if you’re either fairly junior (fewer than 4 years of experience) or you don’t have fancy brands and schools on your resume, recruiters are not incentivized to help you because you don’t meet their criteria, and they’re not incentivized to take risks on candidates because they’re not getting rewarded when the company makes hires (or punished when the company doesn’t).

**What does this mean for you? If you’re not the type of candidate that recruiters are reaching out to already (senior, well-pedigreed), they will not help you.**

With that sad reality in mind, here’s the good news: there *is* someone who’s actually incentivized to make hires and is much more open-minded: the hiring manager[7](#user-content-fn-7)!

#### Cold outreach to hiring managers is effective

At this point, you might be skeptical. After all, according to the graph comparing all channels, hiring manager outreach is the worst of the best. Sure, it’s net positive for FAANG/FAANG-adjacent companies, but it lags behind in-house recruiters, warm referrals, and online applications with respect to effectiveness.

Here’s the thing. Hiring manager outreach is the channel with the most untapped potential for effectiveness, while also being the one where you have the most control. Because companies often ignore them, online applications can’t come close to the same level of control, and warm referrals have a low ceiling. In-house recruiter outreach is largely out of your control (except for maybe making some limited profile tweaks, as we saw above).

Why is this the right channel?

Unlike recruiters, hiring managers are actually incentivized to make hires and tend to be more open-minded about candidate backgrounds, all because hiring managers are judged on results. Specifically, they’re judged on how quickly and effectively they’re able to build stuff, and are — directly or indirectly — incentivized to grow headcount. For hiring managers, it’s not about the appearance of doing the work. It’s about the cold, hard reality of whether the work got done. And because they’re judged on actually getting stuff done, hiring managers are also much more incentivized than recruiters to take risks.

Outside of needing more people to build things, hiring managers are also incentivized to hire for their teams because the better they are at recruiting and filling headcount, the more likely they are to get promoted.

As such, in our minds, when people say that hiring manager outreach hasn’t worked for them, it’s because they’re not doing it right. So, how do you do it?

In our next post, we’ll get very practical about outreach, provide a bunch of examples of good and bad outreach, and share two templates that you can steal.

1. First, here’s how we got to these rankings. We asked each engineer who took our survey to rank all the channels they used to get in the door, from best to worst. Then we tallied up the points (+1 for best two channels, -1 for worst two). We didn’t do a more granular point system (e.g., +2 and -2) because the difference between the top two channels wasn’t always 2X, and generally, from talking to our users, preferences were somewhat muddy. As such, these results are directionally correct, but we didn’t feel comfortable numerically comparing them to one another. Finally, we divided the total tally by the number of times that channel came up. As a result, we were able to rank channels from most effective to least effective. [↩](#user-content-fnref-1)
2. This is why I generally view resume writers as selling snake oil. Either you have the things recruiters are looking for or you don’t. If you don’t, no amount of wordsmithing your bullet points or reorganizing the page is going to make a significant difference. Sure, [check your resume for typos](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/), and make sure that it reads decently well. Any more time invested in your resume after those basic things will have diminishing returns. Beware of anyone who tells you otherwise, and beware of any products or services who charge for resume review. [↩](#user-content-fnref-2)
3. We realize that recruiters won’t always have access to your resume when doing outreach and are likely looking at your LinkedIn instead. The same advice stands. Make sure that your About section has all the most important tidbits about you, front and center. Also, even though we didn’t see the same strong preference for FAANGs and URM status when applying online (more on that in the next section), making these types of changes to your resume certainly won’t hurt. [↩](#user-content-fnref-3)
4. Of course we don’t share the point of view that you can only be a good candidate if you have a brand-name company on your resume. However, many recruiters do, and they are still ignoring this channel. [↩](#user-content-fnref-4)
5. We’d argue that the value to companies is limited as well. Though there are a handful of excellent agency recruiters out there, most are terrible. The hard thing is that, as an employer, you can’t immediately tell who’s terrible, and you end up wasting a bunch of time reviewing profiles of candidates who might look promising on the surface, but because of selection bias (these are the people who decided to work with bad agency recruiters, after all) are not a fit. That or they’re not interested in your company (and have possibly never even opted in to talk to you) or both. [↩](#user-content-fnref-5)
6. At larger companies, recruiting is usually split into two functions: sourcing (these are the people who reach out to candidates) and recruiting (these are the people who manage candidates’ journey through the process and extend offers). In this post, for simplicity, we’re lumping them together because separating them out would change some of the details but wouldn’t change the key takeaways. [↩](#user-content-fnref-6)
7. Note that if you’re interested in smaller startups (Series A and below), you can substitute “founder” for “hiring manager” in the steps below. Founders are the most incentivized to get shit done and take risks, regardless of company size and stage, but at larger startups, they may be less likely to read cold emails because they get bombarded with all manners of requests and sales pitches. At a Series B or C company or at public companies with fewer than, say, 3000 employees, in addition to hiring managers, you should also target Directors and VPs — they have the power to get things done and aren’t so far removed from feeling the pain of not filling roles that making an extra hire or two is out of their purview. At large public companies, targeting Directors and above doesn’t make much sense — they ARE too far removed from doing the work to make individual hires. If you do contact them, the best outcome is that they’ll pass you on to one of their direct reports. [↩](#user-content-fnref-7)


# [Resumes suck. Here's the data.](https://interviewing.io/blog/resumes-suck-heres-the-data)

By Aline Lerner | Published: November 10, 2014; Last updated: April 9, 2024

*Note: This post is syndicated from [Aline Lerner’s personal blog](https://blog.alinelerner.com). Aline is the CEO and co-founder of interviewing.io, and results like these are what inspired her to start this company.*

About a year ago, after looking at the resumes of engineers we had interviewed at TrialPay in 2012, I learned that the strongest signal for whether someone would get an offer was the [number of typos and grammatical errors on their resume](https://interviewing.io/blog/lessons-from-a-years-worth-of-hiring-data). On the other hand, where people went to school, their GPA, and highest degree earned didn’t matter at all. These results were pretty unexpected, ran counter to how resumes were normally filtered, and left me scratching my head about how good people are at making value judgments based on resumes, period. So, I decided to run an experiment.

In this experiment, I wanted to see how good engineers and recruiters were at resume-based candidate filtering. **Going into it, I was pretty sure that engineers would do a much better job than recruiters.** (They are technical! They don’t need to rely on proxies as much!) However, that’s not what happened at all. **As it turned out, people were pretty bad at filtering resumes across the board, and after running the numbers, it began to look like resumes might not be a particularly effective filtering tool in the first place.**

## Setup

The setup was simple. I would:

1. Take resumes from my collection.
2. Remove all personally identifying info (name, contact info, dates, etc.).
3. Show them to a bunch of recruiters and engineers.
4. For each resume, ask just one question: *Would you interview this candidate?*

![Screenshot of a resume filtering form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fsample_stimulus3_b5495fe25f.webp%3Fupdated_at%3D2022-11-21T20%3A18%3A21.787Z&w=1080&q=75 "Resume filtering")

Essentially, each participant saw something like this:

If the participant didn’t want to interview the candidate, they’d have to write a few words about why. If they did want to interview, they also had the option of substantiating their decision, but, in the interest of not fatiguing participants, I didn’t require it.

To make judging easier, I told participants to pretend that they were hiring for a full-stack or back-end web dev role, as appropriate. I also told participants not to worry too much about the candidate’s seniority when making judgments and to assume that the seniority of the role matched the seniority of the candidate.

**For each resume, I had a pretty good idea of how strong the engineer in question was, and I split resumes into two strength-based groups**. To make this judgment call, I drew on my personal experience — most of the resumes came from candidates I placed (or tried to place) at top-tier startups. In these cases, I knew exactly how the engineer had done in technical interviews, and, more often than not, I had visibility into how they performed on the job afterwards. The remainder of resumes came from engineers I had worked with directly. **The question was whether the participants in this experiment could figure out who was who just from the resume.**

At this juncture, a disclaimer is in order. Certainly, someone’s subjective hirability based on the experience of one recruiter is not an oracle of engineering ability — with the advent of more data and more rigorous analysis, perhaps these results will be proven untrue. But, you gotta start somewhere. That said, here’s the experiment by the numbers.

- I used a total of **51 resumes** in this study. 64% belonged to strong candidates.
- A total of **152 people** participated in the experiment.
- **Each participant made judgments on 6 randomly selected resumes** from the original set of 51, for a total of **716 data points**.[1](#user-content-fn-1)

If you want to take the experiment for a whirl yourself, you can do so [here](https://qtrial2014.az1.qualtrics.com/SE/?SID=SV_25Gmmef9ktW34A5).

Participants were broken up into engineers (both engineers involved in hiring and hiring managers themselves) and recruiters (both in-house and agency). There were 46 recruiters (22 in-house and 24 agency) and 106 engineers (20 hiring managers and 86 non-manager engineers who were still involved in hiring).

So, what ended up happening? Below, you can see a comparison of resume scores for both groups of candidates. A resume score is the average of all the votes each resume got, where a ‘no’ counted as 0 and a ‘yes’ vote counted as 1. The dotted line in each box is the mean for each resume group — you can see they’re pretty much the same. The solid line is the median, and the boxes contain the 2nd and 3rd quartiles on either side of it. **As you can see, people weren’t very good at this task — what’s pretty alarming is that scores are all over the place, for both strong and less strong candidates.**

Another way to look at the data is to look at the distribution of accuracy scores. *Accuracy* in this context refers to how many resumes people were able to tag correctly out of the subset of 6 that they saw. As you can see, results were all over the board.

**On average, participants guessed correctly 53% of the time.** This was pretty surprising, and at the risk of being glib, **according to these results, when a good chunk of people involved in hiring make resume judgments, they might as well be flipping a coin**.

![Cartoon coin toss](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Facb35_tie_coin_toss_1_68afd9383e.webp&w=640&q=75 "A coin toss")

Source: <https://what-if.xkcd.com/19/>

What about performance broken down by participant group? Here’s the breakdown:

- Agency recruiters – 56%
- Engineers – 54%
- In-house recruiters – 52%
- Eng hiring managers – 48%

None of the differences between participant groups were statistically significant. In other words, all groups did equally poorly. For each group, you can see how well people did below.

![Charts showing the accuracy of each group](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Faccuracy_charts_8792e468a0.webp%3Fupdated_at%3D2022-11-21T20%3A22%3A06.161Z&w=1920&q=75 "Accuracy of various groups")

To try to understand whether people really were this bad at the task or whether perhaps the task itself was flawed, I ran some more stats. One thing I wanted to understand, in particular, was whether *inter-rater agreement* was high. In other words, when rating resumes, were participants disagreeing with each other more often than you’d expect to happen by chance? **If so, then even if my criteria for whether each resume belonged to a strong candidate wasn’t perfect, the results would still be compelling** — no matter how you slice it, if people involved in hiring consistently can’t come to a consensus, then something about the task at hand is too ambiguous.

The test I used to gauge inter-rater agreement is called [Fleiss’ kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa). The result is on the following scale of -1 to 1:

- *-1* perfect disagreement; no rater agrees with any other
- 0 random; the raters might as well have been flipping a coin
- *1* perfect agreement; the raters all agree with one another

**Fleiss’ kappa for this data set was 0.13.** 0.13 is close to zero, implying just mildly better than coin flip. In other words, the task of making value judgments based on these resumes was likely too ambiguous for humans to do well on with the given information alone.

**TL;DR Resumes might actually suck.**

## Some interesting patterns

In addition to the finding out that people aren’t good at judging resumes, I was able to uncover a few interesting patterns.

### **Times didn’t matter**

We’ve all heard of and were probably a bit incredulous about [the study](https://info.theladders.com/our-team/you-only-get-6-seconds-of-fame-make-it-count) that showed recruiters spend less than 10 seconds on a resume on average. In this experiment, people took a lot longer to make value judgments. People took a median of 1 minute and 40 seconds per resume. In-house recruiters were fastest, and agency recruiters were slowest. **However, how long someone spent looking at a resume appeared to have no bearing, overall, on whether they’d guess correctly.**

### **Different things mattered to engineers and recruiters**

Whenever a participant deemed a candidate not worth interviewing, they had to substantiate their decision. Though these criteria are clearly not the be-all and end-all of resume filtering — if they were, people would have done better — it was interesting to see that engineers and recruiters were looking for different things.[2](#user-content-fn-2)

![Chart showing recruiter's top 5 rejection reasons](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Frecruiter_rejection_reasons_3fd67131ac.webp&w=1200&q=75 "Recruiter's top 5 rejection reasons")

Incidentally, *lack of relevant experience* didn’t refer to lack of experience with a specific stack. Verbatim rejection reasons under this category tended to say stuff like “projects not extensive enough”, “lack of core computer science”, or “a lot of academic projects around EE, not a lot on the resume about programming or web development”. *Culture fit* in the engineering graph denotes concerns about engineering culture fit, rather than culture fit overall. This could be anything from concern that someone used to working with Microsoft technologies might not be at home in a RoR shop to worrying that the candidate is too much of a hacker to write clean, maintainable code.

### **Different groups did better on different kinds of resumes**

First of all, and not surprisingly, engineers tended to do slightly better on resumes that had projects. Engineers also tended to do better on resumes that included detailed and clear explanations of what the candidate worked on. To get an idea of what I mean by detailed and clear explanations, take a look at the two versions below (source: [Lessons from a year’s worth of hiring data](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/ "Lessons from a year’s worth of hiring data")). The first description can apply to pretty much any software engineering project, whereas after reading the second, you have a pretty good idea of what the candidate worked on.

![A example of bad description](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fbad_description_5b87fee909.webp&w=1920&q=75 "Bad description")

A bad description

![An example of a good description](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fgood_description_67fd485b2f.webp&w=1920&q=75 "Good description")

A good description

Recruiters, on the other hand, tended to do better with candidates from top companies. This also makes sense. Agency recruiters deal with a huge, disparate candidate set while also dealing with a large number of companies in parallel. They’re going to have a lot of good breadth-first insight including which companies have the highest engineering bar, which companies recently had layoffs, which teams within a specific company are the strongest, and so on.

## Resumes just aren’t that useful

So, why are people pretty bad at this task? As we saw above, **it may not be a matter of being good or bad at judging resumes but rather a matter of the task itself being flawed — at the end of the day, the resume is a low-signal document.**

If we’re honest, no one really knows how to write resumes particularly well. Many people get their first resume writing tips from their university’s career services department, which is staffed with people who’ve never held a job in the field they’re advising for. Shit, some of the most fervent resume advice I ever got was from a technical recruiter, who insisted that I list every technology I’d ever worked with on every single undergrad research project I’d ever done. I left his office in a cold sweaty panic, desperately trying to remember what version of Apache MIT had been running at the time.

Very smart people, who are otherwise fantastic writers, seem to check every ounce of intuition and personality at the door and churn out soulless documents expounding their experience with the software development life cycle or whatever… because they’re scared that sounding like a human being on their resume or not peppering it with enough keywords will eliminate them from the applicant pool before an engineer even has the chance to look at it.

Writing aside, reading resumes is a shitty and largely thankless task. If it’s not your job, it’s a distraction that you want to get over with so you can go back to writing code. And if it is your job, you probably have a huge stack to get through, so it’s going to be hard to do deep dives into people’s work and projects, even if you’re technical enough to understand them, provided they even include links to their work in the first place. On top of that, spending more time on a given resume may not even yield a more accurate result, at least according to what I observed in this study.

## How to fix top-of-the-funnel filtering

Assuming that my results are reproducible and people, across the board, are really quite bad at filtering resumes, there are a few things we can do to make top-of-the-funnel filtering better. In the short term, improving collaboration across different teams involved in hiring is a good start. As we saw, engineers are better at judging certain kinds of resumes, and recruiters are better at others. If a resume has projects or a GitHub account with content listed, passing it over to an engineer to get a second opinion is probably a good idea. And if a candidate is coming from a company with a strong brand, but one that you’re not too familiar with, getting some insider info from a recruiter might not be the worst thing.

**Longer-term, how engineers are filtered fundamentally needs to change.** In my [TrialPay study](https://blog.alinelerner.com/lessons-from-a-years-worth-of-hiring-data/ "Lessons from a year’s worth of hiring data"), I found that, in addition to grammatical errors, one of the things that mattered most was how clearly people described their work. In this study, I found that engineers were better at making judgments on resumes that included these kinds of descriptions. Given these findings, relying more heavily on a writing sample during the filtering process might be in order. For the writing sample, I am imagining something that isn’t a cover letter — people tend to make those pretty formulaic and don’t talk about anything too personal or interesting. Rather, it should be a concise description of something you worked on recently that you are excited to talk about, as explained to a non-technical audience. I think the non-technical audience aspect is critical because if you can break down complex concepts for a layman to understand, you’re probably a good communicator and actually understand what you worked on. Moreover, recruiters could actually read this description and make valuable judgments about whether the writing is good and whether they understand what the person did.

Honestly, I really hope that the resume dies a grisly death. One of the coolest things about coding is that it doesn’t take much time/effort to determine if someone can perform above some minimum threshold — all you need is the internets and a code editor. Of course, figuring out if someone is great is tough and takes more time, but figuring out if someone meets a minimum standard, mind you the same kind of minimum standard we’re trying to meet when we go through a pile of resumes, is pretty damn fast. And in light of this, relying on low-signal proxies doesn’t make sense at all.

- All the engineers who let me use their resumes for this experiment
- Everyone who participated and took the time to judge resumes
- The fine people at [Statwing](https://www.statwing.com) and [Plotly](https://plot.ly)
- [Stan Le](https://www.linkedin.com/pub/stan-le/44/aa2/435) for doing all the behind-the-scenes work that made running this experiment possible
- All the smart people who were kind enough to proofread this behemoth

1. This number is less than 152\*6=912 because not everyone who participated evaluated all 6 resumes. [↩](#user-content-fnref-1)
2. I created the categories below from participants’ full-text rejection reasons, after the fact. [↩](#user-content-fnref-2)


# [You probably don’t factor in engineering time when calculating cost per hire. Here’s why you really should.](https://interviewing.io/blog/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should)

By Aline Lerner | Published: April 23, 2019; Last updated: September 17, 2023

Whether you’re a recruiter yourself or an engineer who’s involved in hiring, you’ve probably heard of the following two recruiting-related metrics: time to hire and cost per hire. Indeed, these are THE two metrics that any self-respecting recruiting team will track. Time to hire is important because it lets you plan — if a given role has historically taken 3 months to fill, you’re going to act differently when you need to fill it again than if it takes 2 weeks. And, traditionally, cost per hire has been a planning tool as well — if you’re setting recruiting budgets for next year and have a headcount in mind, seeing what recruiting spent last year is super helpful.

But, with cost per hire (or CPH, as I’ll refer to it from now on in this post) in particular, there’s a problem. CPH is typically blended across ALL your hiring channels and is confined to recruiting spend alone. Computing one holistic CPH and confining it to just the recruiting team’s spend hides problems with your funnel and doesn’t help compare the quality of all your various candidate sources. And, most importantly, it completely overlooks arguably the most important thing of all — how much time your team is actually spending on hiring. **Drilling down further, engineering time, specifically, despite being one of the most expensive resources, isn’t usually measured as part of the overall cost per hire.** Rather, it’s generally written off as part of the cost of doing business. **The irony, of course, is that a typical interview process puts the recruiter call at the very beginning of the process precisely to save eng time, but if we don’t measure eng time spent and quantify, then we can’t really save it.**

For what it’s worth, the Twitterverse (my followers are something like 50/50 engineers and recruiters) seems to agree. Here are the results (and some associated comments) of a poll I conducted on this very issue:

> Quick poll! Should "cost per hire" calculations include engineering time spent on interviewing candidates?
>
> — Aline Lerner (@alinelernerLLC) [March 17, 2019](https://twitter.com/alinelernerLLC/status/1107073450013650944?ref_src=twsrc%5Etfw)

And yet, most of us don’t do it. Why? Is it because it doesn’t measure the things recruiters care about? Or is it because it’s hard? Or is it because we can’t change anything, so why bother? After all, engineers need to do interviews, both phone screens and onsites, and we already try to shield them as much as possible by having candidates chat with recruiters or do coding challenges first, so what else can you do?

**If you’d like to skip straight to how to compute a better, more inclusive CPH, you can skip down to our handy spreadsheet. Otherwise read on!**

I’ve worked as both an engineer and an in-house recruiter before founding interviewing.io, so I have the good fortune of having seen the limitations of measuring CPH, from both sides of the table. As such, in this post, I’ll throw out two ways that we can make the cost per hire calculation more useful — by including eng time and by breaking it out by candidate source — and try to quantify exactly why these improvements are impactful… while building better rapport between recruiting and eng (where, real talk, relationships can be somewhat strained). But first, let’s talk about how CPH is typically calculated.

## How is CPH typically calculated, and why does it omit eng time?

As I called out above, the primary purpose of calculating cost per hire is to plan the recruiting department’s budget for the next cycle. With that in mind, below is the formula that you’ll find if you google how to calculate cost per hire (pulled from [Workable](https://resources.workable.com/tutorial/faq-recruitment-budget-metrics)):

![A sketch outline of the cost per hire equation](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Ffc5ff_screenshot_2019_04_22_14_42_10_cdf4827348.webp&w=1920&q=75 "The Cost per Hire equation")

To figure out your CPH, you add up all the external and internal costs incurred during a recruiting cycle and divide by the number of hires.

“External” refers to any money paid out to third parties. Examples include job boards, tools (e.g. sourcing, assessment, your ATS), agency fees, candidate travel and lodging, and recruiting events/career fairs.

“Internal” refers to any money you spend within your company: recruiting team salaries, as well as any employee referral bonuses paid out over the course of the last cycle.

Note that internal costs don’t include eng salaries, as engineering and recruiting teams typically draw from different budgets. Hiring stuff is the domain of the recruiting team, and they pay for it out of their pockets… and engineers pay for… engineering stuff.

What’s problematic is that, while being called “cost per hire” this metric actually tells us what recruiting spends rather than what’s actually being spent as a whole. While tracking recruiting spend makes sense for budget planning, this metric, because of its increasingly inaccurate name, often gets pulled into something it ironically wasn’t intended for: figuring out how much the company is actually spending to make hires.

## Why does factoring in engineering time matter?

As you saw above, not only is this the way we compute CPH inaccurate because it doesn’t factor in any time or resource expenditure outside the recruiting team (with eng being the biggest one). But, does engineering time really matter?

Yes, it matters a lot, for the following three reasons:

1. Way more eng time than recruiting time goes into hiring (as you’ll see in this post!)
2. Eng time is more expensive
3. Eng time expenditure can vary wildly by channel

To establish that these things are (probably) true, let’s look at a typical eng hiring funnel.[1](#user-content-fn-1) For the purposes of this exercise, we’ll start the funnel at the recruiter screen and assume that the costs of sourcing candidates are fixed.[2](#user-content-fn-2)

![Screenshot of the hiring funnels howing pass-through rates at each stage](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6c093_screenshot_2019_03_23_17_36_16_1_c18d98de26.webp&w=3840&q=75 "The hiring funnel")

The green arrows are conversion rates between each step (e.g. 50% of people who get offers accept and get hired). The small gray text at the bottom of each box is how long that step takes for an engineer or recruiter (or both, in the case of an onsite). And the black number is how many times that needs to happen to ultimately make 1 hire, based on the green-arrow conversion rates.

So, with that in mind, to make one hire, let’s see how much time both eng and recruiting need to spend to make 1 hire and how much that time costs. Note that I’m assuming $100/hour is a decent approximation for recruiting comp and $150/hour is a decent approximation for eng comp.

## Is eng time spent on recruiting really that costly?

Based on the funnel above, here’s the breakdown of time spent by both engineering and recruiting to make 1 hire. The parentheticals next to each line of time spent are based on how long that step takes times the number of times it needs to happen.

**RECRUITING – 15 total hours**  
**10** hours of recruiter screens (20 screens needed \* 30 min per screen)  
**4** hours of onsites (4 onsites needed \* 1 hour per onsite)  
**1** hour of offers (2 offer calls needed \* 30 min per offer call)

To make 1 hire, it takes 15 recruiting hours or $1500.

**ENGINEERING – 40 total hours**  
**16** hours of phone screens (16 screens needed \* 1 hour per screen)  
**24** hours of onsites (4 onsites needed \* 6 hours per onsite)

For 1 hire, that’s a total of 40 eng hours, and on the face of it, it’s $6,000 of engineering time, but there is one more subtle multiplier on eng time that doesn’t apply to recruiting time that we need to factor in. Every time you interrupt an engineer from their primary job, which is solving problems with code, it takes time to refocus and get back into it. If you’re an engineer, you know this deep in your bones. And if you’re not, interruptions are very likely something you’ve heard your engineering friends decry… because they’re so painful and detrimental to continued productivity. Back when I was writing code on a regular basis, it would take me 15 minutes of staring at my IDE (or, if I’m honest, occasionally reading Hacker News or Reddit) to let my brain ease back into doing work after coming back from an interview. And it would take me 15 minutes before an interview to read a candidate’s resume and get in the mindset of whatever coding or design question I was going to ask. I expect my time windows are pretty typical, so it basically ends up being a half hour of ramping up and back down for every hour spent interviewing.

Therefore, with ramp-up and ramp-down time in mind, it’s more like $9,000 in eng hours.[3](#user-content-fn-3)

**Ultimately, for one hire, we’re paying a total of $10,500, but eng incurs 6X the cost that recruiting does during the hiring process.**

## Why does breaking out cost per hire by source matter?

So, hopefully, I’ve convinced you that engineering time spent on hiring matters and that it’s the biggest cost you incur. But, if there’s nothing we can do to change it, and it’s just the cost of doing business, then why factor it in to CPH calculations? **It turns out that eng time spent IS a lever you can pull, and its impact becomes clear when you think about cost per hire by candidate source.**

To make that more concrete, let’s take a look at 2 examples. In both cases, we’ll pretend that one of our candidate sources has a different conversion rate than the overall rate at some step in the funnel. Then we’ll change up the conversion rate at one step in the funnel and try to guess that the financial implications of that are… and then actually calculate it. You might be surprised by the results.

### What happens when you increase TPS to onsite conversion to 50%?

As you can see in the funnel above, a decent TPS to onsite conversion rate is 25%. Let’s say one of your sources could double that to 50% (by doing more extensive top-of-funnel filtering, let’s say). What do you think this will do to cost per hire?

In this model, we’re spending a total of 10 recruiting hours (worth $1000) and 32 eng hours (worth $7200).[4](#user-content-fn-4) Unlike in the first example, we’re now paying a total of $8200 to make a hire.

In this case, you’ve reduced your recruiting time spent by 30% and your eng time spent by 20%, ultimately saving $2300 per hire. If one of your sources can get you this kind of efficiency gain, you probably want to invest more resources into it. And though doubling conversion from tech screen to onsite sounds great and perhaps something you would have known already about your source, without computing the cost per hire for this channel, it’s not intuitively clear just how much money a funnel improvement can save you, end to end.

### What happens when you cut your offer acceptance rate in half?

Another possibility is that one of your sources does pretty well when it comes to candidate quality all the way to offer, but for some reason, those candidates are twice as hard to close. In this scenario, you double both the eng and recruiting time expenditure and ultimately pay an extra $7500 per hire for this source (which you’ll likely want to deallocate resources from here on out).[5](#user-content-fn-5)

In either of the examples above, until you break out CPH by source and see exactly what each is costing you, it’s a lot harder to figure out how to optimize your spend.

## How to actually measure cost per hire (and include eng time of course!)

The usual way to calculate cost per hire is definitely useful for setting recruiting budget, as we discussed above, but if you want to figure out how much your whole company is actually spending on hiring, you need to factor in the most expensive piece — engineering time.

To do this, we propose a different metric, one that’s based on time spent by your team rather than overall salaries and fixed costs. Let’s call it “cost per hire prime” or *CPH prime*.

CPH prime doesn’t factor in fixed costs like salaries or events, which you can still do using the formula above… but it is going to be instrumental in helping you get a handle on what your spend actually looks like and will help you compare different channels.

**To make your life easier, we’ve created a [handy spreadsheet for you to copy and then fill in your numbers](https://docs.google.com/spreadsheets/d/1a3Oq1hkP9NfEbjU6pJiQEKCvLUzB9UTZOcqBxMiwDc0/edit#gid=0), like so**:

![A spreadsheet calcuation of cost savings per hire](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F90740_screenshot_2019_04_24_09_50_29_3f7f158b1f.webp&w=1200&q=75 "Calculating cost savings per hire")

=>As you can see, once you fill the highlighted cells with your own conversion numbers (and optionally your hourly wages if yours differ much from our guesses), we’ll compute CPH prime for you.

And because we’re a business and want you to hire through us, we’ve included the average savings for companies hiring through our platform. We provide two big value-adds: we can pretty drastically improve your TPS to onsite conversion — about 65% of our candidates pass the tech screen at companies on average. From there, they get offers and accept them at the same rate as you’d see in your regular funnel.

## Closing thoughts on building bridges between eng and recruiting

So, why does being cognizant of eng time in your CPH calculations matter? I’ve already kind of beaten it into the ground that it’s the biggest cost sink. However, there’s another, more noble reason, to care about eng time. In my career, having sat on all different sides of the table, I’ve noticed one unfortunate, inalienable truth: engineering and recruiting teams are simply not aligned.

Engineers tend to harbor some resentment toward recruiters because recruiters are the arbiters of how eng spends their time when it comes to hiring without a set of clear metrics or goals that help protect that time.

Recruiters often feel some amount of resentment toward engineers who tend to be resistant to interruptions, toward putting in the time to provide meaningful feedback about candidates so that recruiting can get better, and toward changes in the process.

In our humble opinion, much of the resentment on both sides could be cured by incorporating recruiting and engineering costs together in a specific, actionable way that will reduce the misalignment we’re seeing. Recruiters tend to hold the cards when it comes to hiring practices, so we’d love to see them take the lead to reach across the aisle by proactively factoring in eng time spent during hiring and ultimately incorporating recruiting and eng costs together in one metric that matters. Once that’s in place, recruiting can use the data they gather to make better decisions about how to use eng time, and in the process, rebuild much of the rapport and love that’s lost between the two departments.

### Table 1.

**RECRUITING – 15 total hours or $1500**

- **5** hours of recruiter screens (10 screens needed \* 30 min per screen)
- **4** hours of onsites (4 onsites needed \* 1 hour per onsite)
- **1** hour of offers (2 offer calls needed \* 30 min per offer call)  
  **ENGINEERING – 32 total hours or $7200**
- **8** hours of phone screens (8 screens needed \* 1 hour per screen)
- **24** hours of onsites (4 onsites needed \* 6 hours per onsite)

### Table 2.

**RECRUITING – 30 total hours or $3000**

- **20** hours of recruiter screens (40 screens needed \* 30 min per screen)
- **8** hours of onsites (8 onsites needed \* 1 hour per onsite)
- **2** hours of offers (4 offer calls needed \* 30 min per offer call)  
  **ENGINEERING – 80 total hours or $18,000**
- **32** hours of phone screens (32 screens needed \* 1 hour per screen)
- **48** hours of onsites (8 onsites needed \* 6 hours per onsite)

1. We’re basing these numbers on a mix of ATS reporting ([Lever’s recruiting metrics report](https://www.lever.co/resources/recruiting-metrics-for-startups-and-smbs-report/) in particular) and what we’ve heard from our customers. [↩](#user-content-fnref-1)
2. We’re assuming sourcing costs are fixed for purposes of simplicity and because this post is largely about the importance of eng time factored in to the funnel. Of course, if you have channels that reduce sourcing time significantly, you’ll want to weigh that when deciding its efficacy. [↩](#user-content-fnref-2)
3. Really though, the value of an hour of work for an engineer is intangible and much higher than an hourly wage. There ARE inefficiencies and overhead to having a larger staff, not every hour is effective, and most likely it’s your best people who are conducting interviews. The reality is that the money spent on salaries is probably only a fraction of the true cost to the company, particularly for engineers (as opposed to recruiters). [↩](#user-content-fnref-3)
4. See *Appendix: Table 1* for our work in figuring out how much recruiting and eng time it takes to make a hire when your TPS to onsite conversion rate is 50%. [↩](#user-content-fnref-4)
5. See *Appendix: Table 2* for our work in figuring out how much recruiting and eng time it takes to make a hire when you cut your offer acceptance rate in half. [↩](#user-content-fnref-5)


# [How do I know if I’m ready to interview at FAANG?](https://interviewing.io/blog/how-know-ready-interview-faang)

By Atomic Artichoke | Published: December 2, 2020; Last updated: June 18, 2024

Recently, someone asked us how you know you’re ready to succeed in a [FAANG interview](https://interviewing.io/guides/hiring-process) (an interview at Facebook/Amazon/Apple/Netflix/Google)..

![Screenshot of a question posted about interviewing at a FAANG: what metric is crucial for tracking preparedness](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fef35d_2020_11_30_faang_question_c_0b58d67526.webp&w=750&q=75 "Data-driven interview prep")

It’s an interesting question, and one I’m sure many of you job seekers out there are wondering.

Internally, we have our own beliefs, but we wanted to see if we could answer this question more objectively. So we set off on a journey to acquire data to try answering it.

## Methodology

interviewing.io helps prospective job candidates practice mock interviews with actual interviewers from the major tech companies like the aforementioned FAANG companies, as well as others like Dropbox, Uber, LinkedIn, and Slack.

After each mock interview, candidates are measured on a 1-4 scale against three criteria (technical ability, problem solving, and communication) and are also given an overall hire/no hire rating.

![Screenshot of Interviewing.io interview feeback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F5a595_screenshot_2017_11_29_09_13_30_70d1167200.webp&w=1080&q=75 "Interviewing.io interview feedback form")

Feedback form filled out by interviewers after an interviewing.io mock interview

In our analysis, we used these scores to estimate a candidate’s overall skill level, which we would expect to be positively associated with “readiness”.

Additionally, we needed to know whether a job candidate’s skill level and preparation ultimately resulted in the candidate getting the job. Since most real world interviews happen outside of interviewing.io, we surveyed our users to learn how far through the hiring funnel they progressed at three popular tech companies: Google, Facebook, and Amazon. For each of these companies, at least 150 respondents reported participating in the company’s hiring processes. These were the hiring stages we asked for in the survey:

- Applied
- Recruiter call
- Technical phone interview
- Take-home assignment
- Onsite
- Offer
- Hire

We also needed to consider variables that could affect a person’s chance of progressing through the hiring funnel, but weren’t necessarily related to “readiness”. In addition to the mock interview feedback ratings mentioned above, we grabbed the following data about users that we collect at interviewing.io:

- How many mock interviews completed on interviewing.io
- Self-reported career experience level (e.g. junior, intermediate, experienced)

Finally, we asked survey respondents to share data that our platform doesn’t collect but could be associated with a higher chance of landing a job. Here were those other attributes:

- Gender
- Possessed a computer science degree?
- How many real tech interviews they’ve attended as interviewee (excludes mock interviews)
- How the person learned to code
- How long ago did they last go through a job search

## Relationship of respondents’ technical scores to interview success

Intuitively, you would think that better technical ability would be associated with a higher chance of succeeding at interviews. To see if this intuition holds true, let’s look at the frequency of people passing a phone interview, bucketed by a person’s average technical rating on interviewing.io, and see if a positive correlation exists.

Based on 158 respondents who reported to have progressed at least as far as Google’s phone interview phase, we observed a positive-looking relationship, but maybe not as obvious a relationship as one might expect.

![Chart showing the percentage of respondents Who Passed Google Phone Screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_passed_google_phone_tech_e66b4c760b.webp&w=1200&q=75 "Percentage of respondents Who Passed Google Phone Screen")

Looking at respondents who went through the Facebook and Amazon hiring processes, the relationship seemed even less obvious:

![Chart showing percentage of respondents who passed Facebook phone screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_facebook_passed_phone_tech_3906569daf.webp&w=1200&q=75 "Percentage of Respondents Who Passed Facebook Phone Screen")

![Chart showing percentage of respondents who passed Amazon phone screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_28_amazon_pass_phone_tech_ad681a5e2e.webp&w=1200&q=75 "Percentage of Respondents Who Passed Amazon Phone Screen")

While it would be simple to conclude there exists no relationship between technical ability and passing a phone interview, it seems more likely that we were experiencing selection bias. After all, 60-80% of people progressing past Google/Amazon/Facebook phone screens seems a bit high relative to what one might expect.

Compared to the population of all people on interviewing.io, survey respondents with high average technical scores between 3 and 4 were over-represented relative to those who scored between 1.5 and 2.75.[1](#user-content-fn-1) Maybe non-respondents with lower scores and who failed Google/Facebook/Amazon phone interviews happened to be less likely to respond to our survey.

So that’s pretty limiting. But let’s keep in mind that all models are wrong, yet some are useful, so perhaps it’s still possible to learn other stuff from the data. If the survey respondents’ technical abilities weren’t obviously associated with interview success, what about other factors?

## More experience with real technical interviews is associated with future interview success

When digging further, the factor that stood out most was how many real technical interviews the candidate had done in the past. Across all three companies, people who had completed 5 or more real technical interviews tended to have higher rates of passing a phone interview than those who had less real interview experience.

![Chart showing probability of passing Amazon phone screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_amazon_phone_familiar_ac8c45e063.webp&w=1080&q=75 "Probability of Passing Amazon Phone Screen")

![Chart showing probability of passing Google phone screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_google_phone_familiar_1_f346104f0c.webp&w=1080&q=75 "Probability of Passing Google Phone Screen")

![Chart showing probability of passing Facebook phone screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F2020_10_30_facebook_phone_familiar_1_18d0557905.webp%3Fupdated_at%3D2022-11-22T13%3A41%3A23.440Z&w=1080&q=75 "Probability of Passing Facebook Phone Screen")

It’s possible this large effect could have been confounded by other factors, so I iterated on the model to account for these factors, to see whether the effect of prior interview experience on interview success would continue to shine through.

Here were the factors considered:

- Average technical rating
- Average problem solving rating
- Average communication rating
- Number of mock interviews completed on interviewing.io
- Experience level
- Gender
- Did the person have a computer science degree?

For 171 respondents who went through the Amazon hiring process, a statistically significant association continued to exist between prior technical interview experience and succeeding in a phone interview. Below were the predicted probabilities of passing a phone interview for a typical respondent in this survey (males with a computer science degree and 4+ years of experience, with average technical, communication, and problem solving ability), conditional on their prior technical interview experience.

| **Prior Experience** | **Predicted Probability of Passing Amazon Phone Screen** |
| --- | --- |
| 1-4 technical interviews | 65% |
| 5+ technical interviews | 81% |

For Facebook we observed a similar effect:

| **Prior Experience** | **Predicted Probability of Passing Facebook Phone Screen** |
| --- | --- |
| 1-4 technical interviews | 40% |
| 5+ technical interviews | 71% |

Finally, for Google applicants we observed that the number of interviewing.io mock interviews completed had the clearest association with interview success, not prior technical interview experience. While mock interviews aren’t exactly the same as real-world technical interviews, they are conducted by the same kinds of people asking similar questions and using similar assessment criteria. Because of this, it seems possible that the effects of mock interviews on interview success could be similar to the effect of real technical interview experience.

Below was the estimated chance of passing the Google phone interview for the typical survey respondent, conditional on the number of interviewing.io practice interviews attended.

| **Mock interviews on interviewing.io** | **Predicted Probability of Passing Google Phone Screen** |
| --- | --- |
| 2 | 69% |
| 3 | 76% |

Looking at the other variables we included in the model, we observed no other statistically significant relationships with success in phone interviews. It’s possible those relationships actually exist, but based on this sample, we did not observe enough evidence to reject the null hypothesis that no relationship exists.

So even after accounting for other factors, more technical interview experience was still associated with greater success in phone interviews.

We also wondered whether similar effects might exist when you get further down the hiring funnel. For example, could prior technical interview experience also improve your chances of receiving an offer after an onsite? We performed a similar analysis to predict the chance of receiving an offer conditional upon attending an onsite interview, but we found no noticeable relationships between the predictors listed above and the probability of receiving an offer. Sample sizes are naturally smaller when looking this far down the funnel, and perhaps the differences between candidates are smaller, which could be harder to detect.

Based on an analysis of this particular group of people, it seems we have a pretty clear answer to the original question of “What metric should be used to know you’re prepared to succeed in a FAANG interview?”

The answer is to **experience five or more real technical interviews**. Simple, right?

## Why more experience with technical interviews helps you succeed in future technical interviews

I don’t know about you, but I find that answer unsatisfying. That conclusion seems *really* questionable and self-serving, especially when the recommendation comes from a company that offers mock technical interviews as a product.

I don’t blame you, and quite frankly, this result wasn’t what we expected either. The metric of technical interviewing experience is probably just a proxy for some other phenomenon that does correlate with interview success in some explainable way. After all, it’s unlikely that just showing up to five interviews will magically bestow upon you new talents.

So what is it about having technical interview experience that might be associated with success in future technical interviews? To dig even deeper, we followed up with survey respondents who successfully received an offer with Google, Facebook, or Amazon, and asked what factors they felt contributed to their success.

As you’d expect, nearly all respondents said practicing technical problems was the foundation for their success. So yes, technical competency matters, and no, past interview experience doesn’t appear to be a substitute for it.

Beyond technical ability, respondents shared anecdotes that hint at why such a relationship might exist, as well as some possible underlying factors that could explain what technical interview experience might proxy for.

### Direct feedback from other people helps you improve quicker

It’s one thing to get a question wrong in a practice environment, and it’s another thing to get a question wrong when you have an interviewer looking over your shoulder. A few respondents shared instances when they performed poorly in an interview, and explained how those instances influenced their future behavior. These negative experiences clearly highlighted areas in their skillset or presentation that companies tended to rate less favorably.

> *I bombed multiple phone interviews with both Google and Facebook where the questions were about graphs or trees, and the questions were actually trivial. I didn’t have a formal CS background, and I knew that I was weak in those areas.*

Once identified, respondents addressed those weaknesses through study and repetition, helping them allocate their preparation time more effectively.

For one job hunter, a past [Google interview](https://interviewing.io/guides/hiring-process/google#google) failure not only helped shore up a specific technical weakness, but also helped the person learn how to maintain a more focused mindset in general, which proved valuable in a future interview with Facebook.

> *Because my main weak spot in my Google onsite was being rusty in data structures and algorithms, I studied key data structures in CLRS, such as heaps and red-black trees. The Facebook interviews did not actually ask me to implement any data structures, but studying data structures helped keep my mind “on the game”.*

Obviously, there are many other ways to receive feedback other than getting it directly from another person. For example, respondents also made extensive use of LeetCode and Cracking the Coding Interview-style exercises, and I’m sure many of you out there do too.

However, receiving direct feedback from another person seems different for some reason. Whether it’s about social acceptance or proving oneself or something else, feedback from another person seems to be internalized more, which can act as an efficient catalyst for personal growth as long as you keep your mind open to suggestions.

Respondents found direct personal feedback to be very useful, not only from feedback received in real interviews, but also from direct feedback in non-interview settings.

> *Whether it’s interviewing.io or friends asking each other questions or any generic peer interview platform, practice is different than practicing yourself. I found interviewers to be a resource rather than just someone evaluating you which is something you don’t get when practicing yourself.*

While real interviews give you unambiguous feedback about your overall performance (i.e. you did or didn’t get the job), you don’t always receive specific feedback about problems you answered well or skills that you exhibited effectively. Simulated interview environments can be uniquely beneficial because they allow job seekers to engage in a two-way dialogue with the interviewer, which can yield more information than a simple “hire” or “no hire” decision.

> *For practice interviews, I worked with a friend who was also interviewing for Google and other FAANG companies. I heavily leaned on interviewing.io interviews from FAANG interviewers during the last stage of my preparation, to make sure I was ready for Google, Amazon and FAANG interviews in particular.*

> *Knowing I received solid (and specific) feedback from FAANG interviewers was a huge help and signal to me that I was ready for Google and Amazon interviews.*

So maybe the reason why prior technical interview experience correlates with interview success is because interviews happen to be the most common avenue for receiving direct, honest feedback from other people about how you and your skills are perceived.

### You learn how to communicate in an interview setting, which is different than how you communicate in everyday work

Technical interviews can also require different communication patterns than what you might normally use in a typical workplace or academic environment.

For example, some respondents believe that it’s not enough to solve a problem correctly. Additionally, you are expected to narrate your thought process as you solve it.

> *Another thing study materials remind people is the need to constantly communicate your thought processes during the coding interview. Having done many interviews (and many more pair programming sessions), this is second nature to me, but I know it’s not second nature to everyone and bears repeating.*

This implicit expectation can catch some off-guard, particularly if their preparation focused solely on programming exercises.

> *Solving Leetcode on your own is quite different from having to explain your thinking process to someone else.*

These perspectives echo the advice given by our [interviewer Ian Douglas in his guest blog post](https://interviewing.io/blog/ive-conducted-over-600-technical-interviews-on-interviewing-io-here-are-5-common-problem-areas-ive-seen). All five of Ian’s tips help you improve how you communicate with your interviewer while you’re in the middle of the stressful interview environment. At the end of the day, a debugger won’t be making the final assessment on you, a bunch of human beings will, and the things interviewers look for encompass a lot more than the correctness of the code you write.

> *It is okay to get some guidance from the interviewer. You actually can feel that the interviewers are evaluating more than just your problem solving skills (your communication, how you work as a team, will you be a good person to collaborate with, etc).*

By doing more technical interviews, you gain a better understanding of the unique interpersonal dynamics that exist during interviews. Those dynamics impact how interviewers assess you, and failing to adapt to those dynamics could obfuscate your true abilities. But once you’ve gained that understanding, you are able to hone specialized interviewing skills like the ones Ian suggests and apply them in future interviews.

One respondent took this concept to an extreme, re-learning a particular programming language from his or her past to be used primarily for interviewing.

> *Python is much more succinct and expressive than [C++](https://interviewing.io/cplusplus-interview-questions), [Java,](https://interviewing.io/java-interview-questions) or [C#](https://interviewing.io/csharp-interview-questions), which I had used earlier in my career. By not having to write all the braces and semicolons, I free my hands and mind to dig deeper into the problem and better engage with the interviewer. I haven’t made my way up Paul Graham’s succinctness = power hierarchy, but in an interview situation, communication is the most important thing, and Python allows me to communicate with the interviewer better than C++ does.[2](#user-content-fn-2)*

This tactic probably won’t work for everyone. But there probably does exist a tactic that works for you. The more technical interviews you experience, the more chances you’ll have to discover those tactics.

## You learn a lot about how to interview effectively when you communicate directly with other people

Going back to the original question, we said that the metric you should track is how many technical interviews you’ve experienced, because that is what the analysis of this particular set of people outputted.

But that shouldn’t be your main takeaway. The real learning is to acquire another person’s opinion about your interview performance, because you’ll learn a lot of different things from that person’s feedback.

We can help you accomplish that here at interviewing.io, with options to receive direct feedback from currently-employed Google or Facebook interviewers, and even help assess specific skills like systems design or front-end development. But as our survey respondents mentioned, there are other ways of receiving that direct feedback.

As I’ve written before, [interviewing isn’t all that objective](https://interviewing.io/blog/the-eng-hiring-bar-what-the-hell-is-it) because [people aren’t always objective](https://interviewing.io/blog/interview-bias-pseudonyms). Given that interviews are still going to be conducted by other people for the foreseeable future, gaining direct feedback from others appears to be an effective tool for succeeding within the existing system, so you might as well take advantage of it.

1. As an incentive for responding to the survey, we gave a reward of either a $30 credit toward a professional interview or a free peer interview on our platform. If there were any bias, we’d have thought people who scored lower would have been comparatively more likely to respond, rather than less. [↩](#user-content-fnref-1)
2. The idea that Python might be a better programming language for interviews isn’t totally crazy. Empirically, interviewing.io mock interviews conducted in Python have the second-highest success rate among the most popular programming languages. The highest? C++. So if you’re blindly going by the numbers, the respondent should have stuck to C++. However, chances are that this data also suffers from selection bias: people who know C++ might be different in many ways than people who know Python. Key point: use whatever works for you. For this person, Python happened to work better than C++. [↩](#user-content-fnref-2)

[Why resume writing is snake oil](/blog/why-resume-writing-is-snake-oil)


# [Why giving feedback (whether it’s good or bad) will help you hire](https://interviewing.io/blog/why-giving-feedback-good-or-bad-will-help-you-hire)

By Aline Lerner | Published: February 12, 2023; Last updated: May 1, 2023

*Note: This post originally appeared in [TechCrunch](https://techcrunch.com/2023/02/06/to-improve-close-rates-for-technical-interviews-give-applicants-feedback-good-or-bad/) on February 6, 2023.*

One of the things that sucks most about technical interviews is that they’re a black box — candidates (usually) get told whether they made it to the next round, but they’re rarely told why they got the outcome that they did.

Lack of feedback isn’t just frustrating to candidates. It’s bad for business. We did a [whole study](https://interviewing.io/blog/people-cant-gauge-their-own-interview-performance-and-that-makes-them-harder-to-hire) on this. It turns out that 43% of all candidates consistently underrate their technical interview performance, and 25% of all candidates consistently think they failed when they actually passed.

What’s particularly important is that there’s a statistically significant relationship between whether people think they did well in an interview and whether they’d want to work with you. In other words, in every interview cycle, some portion of interviewees are losing interest in joining your company just because they don’t think they did well, even when they actually did.

Practically speaking, giving instant feedback to successful candidates can do wonders for increasing your close rate.

Giving feedback will not only make candidates you want today more likely to join your team, but it’s also crucial to hiring the candidates you might want down the road. Technical interview outcomes are erratic, and according to our data, [only about 25% of candidates perform consistently from interview to interview](https://interviewing.io/blog/after-a-lot-more-data-technical-interview-performance-really-is-kind-of-arbitrary). This means that the same candidate you reject today might be someone you want to hire in 6 months. It’s in your interest to forge a good relationship with them now.

## But won't we get sued?

I surveyed founders, hiring managers, recruiters, and labor lawyers to understand why anyone who’s ever gone through interviewer training has been told in no uncertain terms to not give feedback.

The main reason: companies are scared of getting sued.

As it turns out, literally zero companies (at least in the US) have ever been sued by an engineer who received constructive post-interview feedback. As some of my lawyer contacts pointed out, a lot of cases get settled out of court, and that data is much harder to get, but given what we know, the odds of getting sued after giving useful feedback are extremely low.

## What about candidates getting defensive?

For every interviewer on our platform, we track two key metrics: the candidate experience score and the interviewer calibration score.

For our purposes here, all you need to know is that the candidate experience is a measure of how likely candidates are likely to come back after interviewing with a given interviewer, and the interviewer calibration score tell us whether a given interviewer is too strict or too lenient, based on how their candidates do in subsequent, REAL interviews (which we host on our platform as well). If an interviewer continually gives good scores to candidates who fail real interviews, they’re too lenient, and vice versa.

When you put the candidate experience score and the interviewer calibration score together, you can reason about the value of delivering honest feedback! To wit, below is a graph of the average candidate experience score as a function of interviewer accuracy, representing data from over 1,000 distinct interviewers (comprising ~100K interviews).

As you can see, the candidate experience score peaks right at the point where interviewers are neither too strict or too lenient but are, in Goldilocks terms, just right. It drops off pretty dramatically on either side after that.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/The_best_calibrated_interviewers_are_also_the_best_rated_n_1000_cfd3ee6c28.png)

In short, based on our data, we’re confident that, if you do it right, candidates won’t get defensive and that the benefits of delivering honest feedback greatly outweigh the risks.

## The playbook for how to deliver honest (and sometimes harsh) feedback

The first and most important thing is to NOT focus on the outcome but rather to get specific right away — this will keep your candidate from getting defensive and will set them up to actually hear and internalize your feedback.

In other words, whether they did well or poorly, don’t tell them right away. Instead, dive into a constructive, detailed assessment of their performance. Reframing feedback in this way takes some practice, but your candidates won’t push you to give them the outcome. Instead, their attention will be redirected to the details, which will make the pass/fail part much more of an afterthought (and, in some cases, entirely moot). After all, why do people get defensive? It’s not because they failed! Rather, it’s because they don’t understand why and feel powerless.

To help start the conversation, here are some leading questions for you to consider:

- Did they ask enough questions about constraints before getting into coding or before starting to design a system?
- Go over specific code snippets or portions of their solution, and talk about what they could have done better.
- Could their solution have been more efficient?
- Did they discuss and reason about tradeoffs?
- Did they make mistakes when discussing time or space complexity? What were those specific mistakes?
- Did they make any mistakes when trying to use their programming language of choice idiomatically (e.g., iterating in Python or JavaScript)?
- For systems design questions, did they jump to suggesting a specific database, load balancer, tool, etc. without reasoning through why that tool is the right choice for the job?

Note that to answer these questions well and to give specific, constructive feedback, it’s critical to take notes, ideally timestamped ones, during the interview. Then you can always go back to your notes and say, “Hey, you jumped into coding just 5 minutes into the interview. Typically, you’ll want to spend a few minutes asking questions.”

And, of course, specific feedback really does mean being specific. One of the kindest, albeit most labor-intensive, things you can do is walk through their code with them, point out places where they went astray, and note what they could have done better.

One other useful pattern for giving feedback is to share objective benchmarks for a given interview question, both with respect to times and number of hints given. If you’re a great interviewer, you probably do something called layering of complexity, where after a candidate successfully solves a question, you change up the constraints in real time. You may even do this 3-4 times during the interview if a candidate is blowing through your questions quickly.

This means that you know exactly how many constraint changes you’ll be able to go through with a low-performing candidate vs. a mediocre one vs. one who’s truly exceptional.

Your candidates don’t know this, though! In fact, candidates commonly overestimate their performance in interviews because they don’t realize how many layers of complexity a question has. In this scenario, a candidate will finish, say, the first layer successfully right before time is called. They walk away thinking they did well, when in reality, the interviewer is benchmarking them against people who can complete 3 layers in that amount of time.

How do you put all of this info to practical use? Let your candidates know what the benchmarks are for a top-performing candidate at the end of the interview. For instance, you could say something like, “In the 45 minutes we spent working this problem, the strongest performers usually complete the brute-force solution in about 20 minutes, optimize it until it runs in linear time (which takes another 10 minutes), and then, in the last 15 minutes, successfully complete an enhancement where, instead of an array, your input is a stream of integers.”

Also, let them know exactly how many hints are par for the course. Just like with how much time should elapse for different parts of the interview, candidates have no idea what “normal” is when it comes to the number and detail level of hints. For instance, if a candidate needed a hint about which data structure to use, followed by a hint about what time complexity is associated with that data structure, followed by a hint about a common off-by-one error that comes up, you may want to tell them that the strongest performers usually need a hint about one of those things, but not all three.

The key to communicating these benchmarks constructively is, of course, to be as specific as possible with runtimes or space constraints or whatever success metric you’re using.

One final technique some of our interviewers employ is to ask their candidate to perform a detailed self-assessment at the end of the interview before giving feedback. This is an advanced maneuver, and if you’re completely new to giving synchronous feedback, I wouldn’t do it in your first few interviews. However, once you get comfortable, this approach can be a great way to zero in on the areas that the candidate needs the most help on immediately.

If you do end up going the self-assessment route, it’s good to ask your candidate some leading questions. For instance, for algorithmic interviews, you can ask:

- How well do you think you did at solving the problem and arriving at an optimized solution?
- How clean was your code?
- Where are some places that you struggled?

While the candidate is responding, take notes (perhaps even in your shared editor!), and then go through their points together, and speak to each point in detail. For instance, if a candidate rates themselves well on code quality but poorly on their ability to solve the problem, you can agree or disagree and give them benchmarks (as discussed above) for both.

Here’s the summary playbook:

- Take detailed notes during the interview, ideally with timestamps, that you can refer to later.
- DON’T lead with whether they passed or failed. Instead, get specific and constructive right away. This will divert the candidate’s attention away from the outcome and put them in the right headspace to receive feedback.
- As much as possible, give objective benchmarks for performance. For instance, tell candidates that the strongest performers are usually able to finish part 1 within 20 minutes, part 2 within 10 minutes, and part 3 within 15 minutes, with at most 1 hint.
- Once you get comfortable with giving feedback, you can try asking candidates to do a self-assessment and then use it as a rubric that you can go down, point by point.

[A founder’s guide to making your first recruiting hire](/blog/a-founders-guide-to-making-your-first-recruiting-hire)

[How to write (actually) good job descriptions](/blog/how-to-write-good-job-descriptions)


# [Building interviewing.io's collaborative & replayable whiteboard, or making systems design interviews not suck](https://interviewing.io/blog/building-interviewing-ios-collaborative-replayable-whiteboard)

By Shehbaj Dhillon | Published: September 5, 2022; Last updated: May 1, 2023

*Read the original article [here](https://medium.com/@shehbaj/building-interviewing-ios-collaborative-replayable-whiteboard-5efda69db35).*

![Screenshot of the new interviewing.io whiteboard](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6_FAF_87_A6_127_D_4_A71_992_B_6_A94594520_B7_a1af73cd08.webp&w=2048&q=75 "The new interviewing.io whiteboard")

During the spring of 2022, I went from being a user of interviewing.io to being one of the engineers on the team.

I discovered interviewing.io in 2021 while preparing for my internship interviews, little did I know that I would end up interviewing for interviewing.io via an interview conducted on interviewing.io to receive an internship opportunity at interviewing.io upon passing the said interview. Yes.

During my 11 weeks, I solved an important business problem, quadrupled my problem-solving skills, and collaborated with the fantastic folks who built the product made for engineers, by engineers.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/98_DAE_405_8387_4_A56_B342_0_E91_B1_EE_4_EE_4_cdb8f54a4c.webp)

Winning all categories of the interviewing.io code golf competition

## Virtual & collaborative whiteboarding is hard

While solutions such as CoderPad have allowed companies including interviewing.io to conduct virtual interviews easily, CoderPad has one significant shortcoming: Drawing Mode.

And companies know this too.

Text works fine for normal coding questions but it falls flat on its face for systems design interviews which typically involve drawing shapes and lines which might be hard to depict using text in an IDE and CoderPad’s attempt at solving this problem using their Drawing Mode is lackluster. This is why companies have resorted to using Excalidraw for systems design interviews.

Excalidraw is an open-source whiteboard tool and is feature-rich compared to CoderPad’s drawing mode, some of its valuable features include importing user-made shapes, dark mode (yes), support for real-time user collaboration, and adaptability to different screen sizes.

You can go to excalidraw.com right now and jump into multiplayer mode and start collaborating with your peers.

So, if the solution to the CoderPad’s drawing mode is just as simple as dropping a link to an Excalidraw board, then what’s left, Shehbaj?

## Neither CoderPad’s drawing mode nor Excalidraw’s self-hosted solution is replayable

One of the ways interviewing.io provides the most value to its customers is by allowing them to watch their mock interview replays, and with the rest of the world by sharing these mock interviews.

Example of a systems design interview conducted via text. Imagine how many more high-quality replays we could have gotten if the Drawing Mode worked?

So by allowing customers to use CoderPad’s Drawing Mode or Excalidraw’s self-hosted solution, not only does interviewing.io rob their customers of a core product feature, but it also prevents interviewing.io from sharing amazing replays of systems design interviews conducted with these tools with the rest of the world.

So, to deal with all this, we made our own whiteboard! 🚀

## System overview

![Diagram showing how drawing data flows from one participant to the other](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FB9_A3_ACB_7_1_AEC_4951_B89_A_C74015_B02692_000b042a79.webp&w=2048&q=75 "Whiteboard data flow")

Excalidraw lets us embed their whiteboard in our React frontend through their npm package. With that, we set up the wiring to connect everything in the full stack together. We use WebSockets to transfer changes between the client and the server and store changes in a MongoDB document.

## Features

Our whiteboard supports real-time user collaboration, mobile screens, and most importantly, the ability to watch replays.

### Real-time user collaboration

Wasn’t that the whole point of making this thing???

![Screencast animation showing two users collaborating on a drawing](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FColorful_Unpleasant_Iguanodon_size_restricted_41d2eff4f4.gif&w=1920&q=75 "Real-time user collaboration in action")

### Mobile friendly

Coding on a mobile screen is hard, but making beautiful drawings isn’t. Now, our users can join from their tablets and other touch screen devices that will allow them to prep for their systems design interviews in style.

![Screencast animation showing the whiteboard on a smaller mobile screen](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompassionate_Distant_Arieltoucan_size_restricted_ae0011c06c.gif%3Fupdated_at%3D2022-12-14T13%3A43%3A34.018Z&w=1080&q=75 "Whiteboard on a smaller mobile screen")

### Replay support

To get the replay to work, we store all the elements which are added, updated, or deleted based on the timestamp of the event.

```
const whiteboardSchema = new Schema({
  // ...
  
  elements: [{
    createdAt: { type: Date, default: Date.now, required: true },
    addedElements: { type: [ExcalidrawElement], required: true }
  }]
  
  // ...
})

Pseudocode of a Mongoose Schema for storing an Excalidraw Scene in MongoDB

Then on the frontend, we fetch all the scene data for replay and sync the current scene with the current timestamp in the replay.

```
const replayElements = fetch(...); // Fetch all the sceneData from the server
const excalidrawCanvas = useRef(...);

const syncReplay = (timestamp: number) => {
  
  // ...
  
  const currentSceneElements: Map<string, ExcalidrawElement>() = new Map();
  
  replayElements.forEach(replayData => {
    if (replayData.createdAt <= timestamp) {
      replayData.changedElements.forEach(element => {
        currentSceneElements[element.id] = element;
      }
    };
  });
  
  excalidrawCanvas.updateScene({ elements : currentSceneElements });
  
  // ...

};

Pseudocode for syncing the Excalidraw scene on the frontend for replay

To try out the whiteboard in production and test out the replay functionality, I did a mock systems design interview with a Senior Software Engineer (SDE III) from Amazon. We discussed how to build a feed system similar to Facebook. Here are some clips of the replay.

![Screencast animation showing the whiteboard in replay mode and the use of drawing and text tools](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FUnequaled_Teeming_Brontosaurus_size_restricted_66b722e964.gif&w=1200&q=75 "Replay mode with drawing and text tools")

Screenshot #1 - Replay mode with drawing and text tools

![Screencast animation showing the whiteboard in replay mode zoomed out](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FSilver_Wellworn_Agouti_size_restricted_8a367241f7.gif&w=1200&q=75 "Replay mode zoomed out")

Screenshot #2 - Replay mode zoomed out

Not bad for a college student doing a systems design interview for the first time 🥱🥱🥱🥱🥱🥱

![Screenshot showing interviewer feedback on an interview where the whiteboard was put to use](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FC8938307_45_BD_4886_B7_FD_C85_B77_B47_E45_6a3d8c204e.webp&w=1080&q=75 "Interview feedback after using the whiteboard")

The whiteboard is now live, so go check it out and let us know if you have any feedback!!!


# [We analyzed thousands of technical interviews on everything from language to code style. Here's what we found.](https://interviewing.io/blog/we-analyzed-thousands-of-technical-interviews-on-everything-from-language-to-code-style-here-s-what-we-found)

By Aline Lerner | Published: June 12, 2017; Last updated: July 14, 2023

*Note: Though I wrote most of the words in this post, the legendary [Dave Holtz](https://twitter.com/daveholtz) did the heavy lifting on the data side. See more of his work on [his blog](https://www.daveholtz.net/).*

If you’re reading this post, there’s a decent chance that you’re about to re-enter the crazy and scary world of technical interviewing. Maybe you’re a college student or fresh grad who is going through the interviewing process for the first time. Maybe you’re an experienced software engineer who hasn’t even thought about interviews for a few years. Either way, the first step in the interviewing process is usually to read a bunch of online interview guides (especially if they’re written by companies you’re interested in) and to chat with friends about their experiences with the interviewing process (both as an interviewer and interviewee). More likely than not, what you read and learn in this first, “exploratory” phase of the interview process will inform how you choose to prepare moving forward.

There are a few issues with this typical approach to interview preparation:

- Most interview guides are written from the perspective of one company. While Company A may really value efficient code, Company B may place more of an emphasis on high-level problem-solving skills. Unless your heart is set on Company A, you probably don’t want to give too much weight to what they value.
- People lie sometimes, even if they don’t mean to. In writing, companies may say they’re language agnostic, or that it’s worthwhile to explain your thought process, even if the answer isn’t quite right. However, it’s not clear if this is actually how they act! We’re not saying that tech companies are nefarious liars who are trying to mislead their applicant pool. We’re just saying that sometimes implicit biases sneak in and people aren’t even aware of them.
- A lot of the “folk knowledge” that you hear from friends and acquaintances may not be based in fact at all. A lot of people assume that short interviews spell doom. Similarly, everyone can recall one long interview after which they’ve thought to themselves, “I really hit it off with that interviewer, I’ll definitely get passed onto the next stage.” In the past, [we’ve seen that people are really bad at gauging how they did in interviews](https://interviewing.io/blog/own-interview-performance). This time, we wanted to look directly at indicators like interview length and see if those actually matter.

**Here at interviewing.io, we are uniquely positioned to approach technical interviews and their outcomes in a data-driven way. This time, we’ve opted for a quick (if not dirty) and quantitative analysis. In other words, rather than digging deep into individual interviews, we focused on easily measurable attributes that many interviews share, like duration and language choice.** In upcoming posts, we’ll be delving deeper into the interview content itself. If you’re new to our blog and want to get some context about how interviewing.io works and what interview data we collect, please take a look at the section called “The setup” below. Otherwise, please skip over that and head straight for the results!

[interviewing.io](https://interviewing.io/) is a platform where people can practice technical interviewing anonymously, and if things go well, unlock the ability to interview anonymously, whenever they’d like, with top companies like Uber, Lyft, and Twitch. The cool thing is that both practice interviews and real interviews with companies take place within the interviewing.io ecosystem. As a result, we’re able to collect quite a bit of interview data and analyze it to better understand technical interviews, the signal they carry, what works and what doesn’t, and which aspects of an interview might actually matter for the outcome.

Each interview, whether it’s practice or real, starts with the interviewer and interviewee meeting in a collaborative coding environment with voice, text chat, and a whiteboard, at which point they jump right into a technical question. Interview questions tend to fall into the category of what you’d encounter in a phone screen for a back-end software engineering role. **During these interviews, we collect everything that happens, including audio transcripts, data and metadata describing the code that the interviewee wrote and tried to run, and detailed feedback from both the interviewer and interviewee about how they think the interview went and what they thought of each other.**

If you’re curious, you can see what the feedback forms for interviewers and interviewees look like below — in addition to one direct yes/no question, we also ask about a few different aspects of interview performance using a 1-4 scale. We also ask interviewees some extra questions that we don’t share with their interviewers, and one of the things we ask is whether an interviewee has previously seen the question they just worked on.

![Screenshot of Interviewing.io interview feedback form for interviewers](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6ba25_new_interviewer_feedback_8fa7f982c6.webp&w=1200&q=75 "Feedback form for interviewers")

![Screenshot of Interviewing.io interview feedback form for interviewees](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2F6702c_new_interviewee_feedback_3ae44bf112.webp&w=1200&q=75 "Feedback form for interviewees")

Before getting into the thick of it, it’s worth noting that the conclusions below are based on observational data, which means we can’t make strong causal claims… but we can still share surprising relationships we’ve observed and explain what we found so you can draw your own conclusions.

### Having seen the interview question before

*“We’re talking about practice!”* -Allen Iverson

First thing’s first. It doesn’t take a rocket scientist to suggest that one of the best ways to do better in interviews is to… practice interviewing. There are a number of resources out there to help you practice, ours among them. One of the main benefits of working through practice problems is that you reduce the likelihood of being asked to solve something you’ve never seen before. Balancing that binary search tree will be much less intimidating if you’ve already done it once or twice.

We looked at a sample of ~3000 interviews and compared the outcome to whether the interviewee had seen the interview question before. You can see the results in the plot below.

**Unsurprisingly, interviewees who had seen the question were 16.6% more likely to be considered hirable by their interviewer.** This difference is statistically significant (p < 0.001).[1](#user-content-fn-1)

### Does it matter what language you code in?

*“Whoever does not love the language of his birth is lower than a beast and a foul smelling fish.”* -Jose Rizal

You might imagine that different languages lead to better interviews. For instance, maybe the readability of Python gives you a leg up in interviews. Or perhaps the fact that certain languages handle data structures in a particularly clean way makes common interview questions easier. We wanted to see whether or not there were statistically significant differences in interview performance across different interview languages.

To investigate, we grouped interviews on our platform by interview language and filtered out any languages that were used in fewer than 5 interviews (this only threw out a handful of interviews). After doing this, we were able to look at interview outcome and how it varied as a function of interview language.

The results of that analysis are in the chart below. Any non-overlapping confidence intervals represent a statistically significant difference in how likely an interviewee is to ‘pass’ an interview, as a function of interview language. Although we don’t do a pairwise comparison for every possible pair of languages, the data below suggest that generally speaking, **there aren’t statistically significant differences between the success rate when interviews are conducted in different languages**.[2](#user-content-fn-2)

That said, one of the most common mistakes we’ve observed qualitatively is people choosing languages they’re not comfortable in and then messing up basic stuff like array length lookup, iterating over an array, instantiating a hash table, and so on. This is especially mortifying when interviewees purposely pick a fancy-sounding language to impress their interviewer. Trust us, wielding your language of choice comfortably beats out showing off in a fancy-sounding language you don’t know well, every time.

### Even if language doesn’t matter… is it advantageous to code in the company’s language of choice?

*“God help me, I’ve gone native.”* -Margaret Blaine

It’s all well and good that, in general, interview language doesn’t seem particularly correlated with performance. However, you might imagine that there could be an effect depending on the language that a given company uses. You could imagine a Ruby shop saying “we only hire Ruby developers, if you interview in Python we’re less likely to hire you.” On the flip side, you could imagine that a company that writes all of their code in Python is going to be much more critical of an interviewee in Python – they know the ins and outs of the language, and might judge the candidate for doing all sorts of “non-pythonic” things during their interview.

The chart below is similar to the chart which showed differences in interview success rate (as measured by interviewers being willing to hire the interviewee) for [C++,](https://interviewing.io/cplusplus-interview-questions) [Java](https://interviewing.io/java-interview-questions), and [Python](https://interviewing.io/python-interview-questions). However, this chart also breaks out performance by whether or not the interview language is in the company’s stack. We restrict this analysis to C++, Java and Python because these are the three languages where we had a good mixture of interviews where the company did and did not use that language. **The results here are mixed. When the interview language is Python or C++, there’s no statistically significant difference between the success rates for interviews where the interview language is or is not a language in the company’s stack. However, interviewers who interviewed in Java were more likely to succeed when interviewing with a Java shop (p=0.037).**

So, why is it that coding in the company’s language seems to be helpful when it’s Java, but *not* when it’s Python or C++? One possible explanation is that the communities that exist around certain programming languages (such as Java) place a higher premium on previous experience with the language. Along these lines, it’s also possible that interviewers from companies that use Java are more likely to ask questions that favor those with a pre-existing knowledge of Java’s idiosyncrasies.

### What about the relationship between what language you program in and how good of a communicator you’re perceived to be?

*“To handle a language skillfully is to practice a kind of evocative sorcery.”* -Charles Baudelaire

Even if language choice doesn’t matter that much for overall performance (Java-wielding companies notwithstanding), we were curious whether different language choices led to different outcomes in other interview dimensions. For instance, an extremely readable language, like Python, may lead to interview candidates who are assessed to have communicated better. On the other hand, a low-level language like C++ might lead to higher scores for technical ability. Furthermore, very readable or low-level languages might lead to correlations between these two scores (for instance, maybe they’re a C++ interview candidate who can’t explain at all what he or she is doing but who writes very efficient code). The chart below suggests that there isn’t really any observable difference between how candidates’ technical and communication abilities are perceived, across a variety of programming languages.

![Heatmaps showing communication ability vs. coding skill for different programming languages](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fcodistribution_1_72711c2cab.webp&w=1920&q=75 "Communication ability vs. coding skill for different programming languages")

**Furthermore, no matter what, poor technical ability seems highly correlated with poor communication ability – regardless of language, it’s relatively rare for candidates to perform well technically but not effectively communicate what they’re doing (or vice versa)**, largely (and fortunately) debunking the myth of the incoherent, fast-talking, awkward engineer.[3](#user-content-fn-3)

### Interview duration

*“It’s fine when you careen off disasters and terrifyingly bad reviews and rejection and all that stuff when you’re young; your resilience is just terrific.”* -Harold Prince

We’ve all had the experience of leaving an interview and just feeling like it went poorly. Often, that feeling of certain underperformance is motivated by rules of thumb that we’ve either come up with ourselves or heard repeated over and over again. You might find yourself thinking, “the interview didn’t last long? That’s probably a bad sign… ” or “I barely wrote anything in that interview! I’m definitely not going to pass.” Using our data, we wanted to see whether these rules of thumb for evaluating your interview performance had any merit.

First, we looked at the length of the interview. Does a shorter interviewer mean you were such a trainwreck that the interviewer just had to stop the interview early? Or was it maybe the case that the interviewer had less time than normal, or had seen in just a short amount of time that you were an awesome candidate? The plot below shows the distributions of interview length (measured in minutes) for both successful and unsuccessful candidates. **A quick look at this chart suggests that there is no difference in the distribution of interview lengths between interviews that go well and interviews that don’t — the average length of interviews where the interviewer wanted to hire the candidate was 51.00 minutes, whereas the average length of interviews where the interviewer did not was 49.95 minutes. This difference is not statistically significant.**[4](#user-content-fn-4)

### Amount of code written

*“Brevity is the soul of wit.”* -William Shakespeare

You may have experienced an interview where you were totally stumped. The interviewer asks you a question you barely understand, you repeat back to him or her “binary search what?”, and you basically write no code during your interview. You might hope that you could still pass an interview like this through sheer wit, charm, and high-level problem-solving skills. In order to assess whether or not this was true, we looked at the final character length of code written by the interviewee. The plot below shows the distributions of character length for both successful and unsuccessful. A quick look at this chart suggests that there is a difference between the two — interviews that don’t go well tend to have less code. There are two phenomena that may contribute to this. First, unsuccessful interviewers may write less code to begin with. Additionally, they may be more prone to delete large swathes of code they’ve written that either don’t run or don’t return the expected result.

**On average, successful interviews had final interview code that was on average 2045 characters long, whereas unsuccessful ones were, on average, 1760 characters long.** That’s a big difference! This finding is statistically significant and probably not very surprising.

### Code modularity

*“The mark of a mature programmer is willingness to throw out code you spent time on when you realize it’s pointless.”* -Bram Cohen

In addition to just look at *how much* code you write, we can also think about the type of code you write. Conventional wisdom suggests that good programmers don’t recycle code – they write modular code that can be reused over and over again. We wanted to know if that type of behavior was actually rewarded during the interview process. In order to do so, we looked at interviews conducted in Python[5](#user-content-fn-5) and counted how many function definitions appeared in the final version of the interview. We wanted to know if successful interviewees defined more functions — while having more function handlers is not the definition of modularity, in our experience, it’s a pretty strong signal of it. As always, it’s impossible to make strong causal claims about this – it might be the case that certain interviewers (who are more or less lenient) ask interview questions that lend themselves to more or fewer functions. Nonetheless, it is an interesting trend to investigate!

The plot below shows the distribution of the number of Python functions defined for both candidates who the interviewer said they would hire and candidates who the interviewer said they would not hire. A quick look at this chart suggests that there *is* a difference in the distribution of function definitions between interviews that go well and interviews that don’t. Successful interviewees seem to define *more* functions.

**On average, successful candidates interviewing in Python define 3.29 functions, whereas unsuccessful candidates define 2.71 functions. This finding is statistically significant. The upshot here is that interviewers really do reward the kind of code they say they want you to write.**

### Does it matter if your code runs?

*“Move fast and break things. Unless you are breaking stuff, you are not moving fast enough.”* -Mark Zuckerberg  
*“The most effective debugging tool is still careful thought, coupled with judiciously placed print statements.”* -Brian Kernighan

A common refrain in technical interviews is that interviewers don’t actually care if your code runs – what they care about is problem-solving skills. Since we collect data on the code interviewees run and whether or not that code compiles, we wanted to see if there was evidence for this in our data. Is there any difference between the percentage of code that compiles error-free in successful interviews versus unsuccessful interviews? Furthermore, can interviewees actually still get hired, even if they make tons of syntax errors?

In order to get at this question, we looked at the data. We restricted our dataset to interviews longer than 10 minutes with more than 5 unique instances of code being executed. This helped filter out interviews where interviewers didn’t actually want the interviewee to run code, or where the interview was cut short for some reason. We then measured the percent of code runs that resulted in errors.[6](#user-content-fn-6) Of course, there are some limitations to this approach – for instance, candidates could execute code that does compile but gives a slightly incorrect answer. They could also get the right answer and write it to stderr! Nonetheless, this should give us a directional sense of whether or not there’s a difference.

The chart below gives a summary of this data. The x-axis shows the percentage of code executions that were error-free in a given interview. So an interview with 3 code executions and 1 error message would count towards the “30%-40%” bucket. The y-axis indicates the percentage of all interviews that fall in that bucket, for both successful and unsuccessful interviews. Just eyeballing the chart below, one gets the sense that on average, successful candidates run more code that goes off without an error. But is this difference statistically significant?

On average, successful candidates’ code ran successfully (didn’t result in errors) 64% of the time, whereas unsuccessful candidates’ attempts to compile code ran successfully 60% of the time, and this difference was indeed significant. **Again, while we can’t make any causal claims, the main takeaway is that successful candidates do usually write code that runs better, despite what interviewers may tell you at the outset of an interview.**

### Should you wait and gather your thoughts before writing code?

*“Never forget the power of silence, that massively disconcerting pause which goes on and on and may at last induce an opponent to babble and backtrack nervously.”* -Lance Morrow

We were also curious whether or not successful interviewees tended to take their time in the interview. Interview questions are often complex! After being presented with a question, there might be some benefit to taking a step back and coming up with a plan, rather than jumping right into things. In order to get a sense of whether or not this was true, we measured how far into a given interview candidates first executed code. Below is a histogram showing how far into interviews both successful and unsuccessful interviewees first ran code. Looking quickly at the histogram, you can tell that successful candidates do in fact wait a bit longer to start running code, although the magnitude of the effect isn’t huge.

More specifically, **on average, candidates with successful interviews first run code 27% of the way through the interview, whereas candidates with unsuccessful interviews first run code 23.9% of the way into the interview, and this difference is significant**. Of course, there are alternate explanations for what’s happening here. For instance, perhaps successful candidates are better at taking the time to sweet-talk their interviewer. Furthermore, the usual caveat that we can’t make causal claims applies – if you just sit in an interview for an extra 5 minutes in complete silence, it won’t help your chances. Nonetheless, there does seem to be a difference between the two cohorts.

## Conclusions

All in all, this post was our first attempt to understand what does and does not typically lead to an interviewer saying “you know what, I’d really like to hire this person.” Because all of our data are observational, its hard to make causal claims about what we see. While successful interviewees may exhibit certain behaviors, adopting those behaviors doesn’t guarantee success. Nonetheless, it does allow us to support (or call bullshit on) a lot of the advice you’ll read on the internet about how to be a successful interviewee.

That said, there is much still to be done. This was a first, quantitative pass over our data (which is, in many ways, a treasure trove of interview secrets), but we’re excited to do a deeper, qualitative dive and actually start to categorize different questions to see which carry the most signal as well as really get our head around 2nd order behaviors that you can’t measure easily by running a regex over a code sample or measuring how long an interview took. If you want to help us with this and are excited to listen to a bunch of technical interviews, drop me a line (at [aline@interviewing.io](mailto:aline@interviewing.io))!

1. All error bars in this post represent a 95% confidence interval. [↩](#user-content-fnref-1)
2. There were more languages than these on our platform, but the more obscure the language, the less data points we have. For instance, all interviews in [Brainfuck](https://en.wikipedia.org/wiki/Brainfuck) were clearly successful. Kidding. [↩](#user-content-fnref-2)
3. The best engineers I’ve met have also been legendarily good at breaking down complex concepts and explaining them to laypeople. Why the infuriating myth of the socially awkward, incoherent tech nerd continues to exist, I have absolutely no idea. [↩](#user-content-fnref-3)
4. For every comparison of distributions in this post, we use both a Fisher-Pitman permutation test to compare the difference in the means of the distributions. [↩](#user-content-fnref-4)
5. We limit this analysis to interviews in Python because it lends itself particularly well to the identification of function definitions with a simple parsing script. [↩](#user-content-fnref-5)
6. We calculate this by looking at what percentage of the time the interviewee executed code that resulted in either an error or non-error output contained the term “error” or “traceback.” [↩](#user-content-fnref-6)


# [Why we’re pausing our Pay Later Program](https://interviewing.io/blog/why-were-pausing-our-pay-later-program)

By Aline Lerner | Published: September 28, 2022; Last updated: May 1, 2023

If you’ve been following the history of interviewing.io, then you’ll know we took a bunch of sharp turns during our journey. I’ll spare you the long version (you can find that in our [announcement that we’re out of beta](https://interviewing.io/blog/interviewing-io-is-out-of-beta-anonymous-technical-interview-practice-for-all), as well as our [Series A announcement](https://interviewing.io/blog/weve-raised-our-series-a), but the TL;DR is that we used to make money from employers, and mock interviews were free for engineers. Then hiring basically froze when COVID-19 happened, and to survive, we started charging engineers. We didn’t feel great about it, but the company would have shut down otherwise. The silver lining was that this allowed us to open up interviewing.io to engineers of all experience levels and in more locations and also meant that we no longer needed to throttle how many mock interviews people could do.

When I announced that we were starting to charge our users, I promised 2 things: (1) We would start a Fellowship program for engineers from non-traditional backgrounds, where they could receive mentorship and practice for free, and (2) We would create a program where you didn’t have to pay for practice till you got a job.

We launched our [Technical Interview Practice Fellowship](https://interviewing.io/blog/announcing-the-interviewing-io-technical-interview-practice-fellowship) in 2020 and have completed 2 successful cohorts so far. We also launched our beta deferred payment program in September 2020 and ran it for a year to see what portion of users would get jobs and pay us back. When that was successful, we followed it in January 2021 with a much bigger, fully productized version called the Pay Later Program.

Unfortunately, we recently made the very difficult decision to pause our Pay Later Program. In this post, we’ll talk about why we made that call and what we’ll be doing instead to ensure that engineers who can’t afford to pay for practice will still be able to get it. We’ll also explain some things we’ve learned along the way about funnel optimization, some mistakes we made while iterating on this program, and what we’ll do differently when we hopefully unpause it in the future.

But first, some history.

## Our beta deferral program

Ever since we started charging, we wanted to give users a way to de-risk buying interview practice. **However, we didn’t want to subject our users to lengthy and invasive loan applications.** I had seen what some Income Share Agreement (ISA) application processes looked like, and they turned my stomach – many demanded bank account info so they could periodically check on your purchases. In one demo we saw, one of these firms bragged that if a student was spending too much money at Starbucks, it could be a sign that it was time to collect.

We didn’t want any part of that. We also wanted to distance ourselves from ISAs in general – mock interviews don’t cost tens of thousands of dollars, so with us, you wouldn’t have to give us a portion of your income. You’d just defer paying us for some number of mock interviews, which would usually run you somewhere between $500-$1000 depending on the number of interviews. So, we decided we’d ideally backstop this program ourselves, and as a precursor, we ran the beta deferral program for a year to see what kinds of payback rates we’d get.

Below are the terms of the original deferral program. While this program was in beta, most of our enrollment flow was in Typeform. It looked like this:

![Deferral program application - page 1 - about the program](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdeferral_program_application_page_1_a107f6db8f.png&w=1920&q=75 "Deferral program application - page 1")

Then we’d ask a few questions, and the final step of the form was to sign a contract, like so:

![Deferral program application - page 2 - agreement terms](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fdeferral_program_application_page_2_2222461614.png&w=1920&q=75 "Deferral program application - page 2")

**After 4 months, we saw a 70% payback rate, and after 6 months, it was around 90%.** Between September 2020 and September 2021, over 500 beta users enrolled in this program, and feedback was overwhelmingly positive.

We felt great about this; however, we knew that the execution of the beta was still clunky. Typeform was our whole UI, and our back-end included a bunch of manual work held together with chewing gum and a massive spreadsheet, where we tracked enrollments, statuses, and paybacks.

When we saw that the deferral program was clearly working, we decided to go all in, remove the manual work, and productize it properly.

## The shiny new Pay Later Program

The new Pay Later Program would be a fully automated, shiny version of what we had cobbled together in the beta. The terms would be streamlined, enrollment would be easier, check-in emails would be automatic, and everything would just… *work.* As such, we took a critical look at the deferral program flow and all of the operational pain points and decided to streamline them as much as possible.

Because so many people in the old deferral program reached out asking to upgrade their algorithmic interviews to something more specialized (e.g., “I want to practice machine learning instead,” “I want an interviewer specifically from Google/Amazon/Facebook”), we decided to make the next version easier for everyone by just giving users interviewing.io credits.

When you enrolled in the program, you’d start with $512 or $1024 in credits (your choice) that you could spend on any mock interview or mentorship session that you liked.

**We launched our Pay Later Program at the end of December 2021. As we usually do with new features, we A/B tested a few different variants, with the intent of increasing enrollment as much as possible each time.**

Our first iteration of Pay Later was almost identical to its predecessor, the deferral program, except that instead of having to fill out a Typeform, the enrollment flow was in our app. Here’s how it looked:

![Screenshot of the Deferral Program enrollment form - personal details and credit amount](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FScreen_Shot_2022_09_28_at_5_41_04_PM_efb0b88942.png&w=828&q=75 "Deferral Program enrollment form - personal details and credit amount")

The only out-of-app part of the experience was having to sign a contract in HelloSign. We also had a very wordy explanation of the terms. It was a wall of text, much like the first page of the old Typeform.

**Over time, as we continued to A/B test, we pruned more and more steps, tried different UIs, and even changed up the terms. We were ultimately able to more than *double* enrollment in the Pay Later Program.** The 3 biggest and most impactful changes we made were:

- Removing the contract-signing step entirely. Instead, we just asked users to check a box that said, “I agree to the terms of the Pay Later Program,” where clicking into the terms opened them in a new tab (see the screenshot below). *This was by far the most impactful change.*
- Making our terms more lax. Instead of charging you 4 months later, if you stayed at your job, we amended the terms to make it so you could get as many extensions as you needed, as long as you were still looking for a job (in good faith).
- Condensing the terms summary from the wordy bullets in the original Typeform to the blue box below:

![Screenshot of the Deferral Program enrollment form - provide a valid credit card](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2Fpay_later_program_page_1_55f906a475.png&w=1200&q=75 "Deferral Program enrollment form - provide a valid credit card")

## The dangers of funnel hacking

Our new Pay Later Program was objectively better for users than our old deferral program for 2 reasons. First, it was more flexible – users got credits they could spend however they wished, instead of 5 algorithmic interviews that they’d have to pay to upgrade. Second, the terms were more generous and user-friendly – you didn’t have to pay us back if you stayed at your old job, as long as you promised us you were still looking, whereas in the old deferral program if you stayed at your current job, you’d have to pay back the full amount after 4 months.

**Despite the offering being objectively better, user feedback was objectively worse.**

In our quest to increase funnel throughput, we made enrollment too frictionless. **By no longer requiring people to sign a contract and by basically creating 2-click enrollment with an abbreviated TL;DR that glossed over the mechanics of payback** (we’ll send you an email, we’ll charge if we don’t hear back from you, you’re on the hook for the full amount and any unused credits are yours to keep in perpetuity), **we failed to sufficiently explain the program to users, many of whom were justifiably shocked when they got billed.**

Reading a contract and signing it forced users to think through whether they were willing to be on the hook for a non-trivial amount of money. When we made enrollment as easy as clicking a few buttons, many users (understandably) no longer read the terms and made assumptions about how the program worked. These assumptions varied from “I thought these interviews were free” to “I thought I only had to pay if I got a job specifically at Google” to “I never signed up for this.”

With that in mind, of course it made sense that some portion of our users were irate and called us a scam when we emailed to ask if they were ready to pay us back. What sucked is that, even though we created this program with the best of intentions and modified the terms to make them more permissive, because of how we packaged and presented it, we experienced a very different response than from the original deferral program.

Though the majority of users still paid us back within the 4 month period, there was now an angry and disappointed vocal minority, and our ops team began to dread the end of the month, when we’d invariably receive a number of irate emails stating that interviewing.io was a scam.

**I don’t feel great that I got caught up in the funnel hacking and in seeing enrollment numbers increase.** If you’ve read my writing over the years, you’ll know that I take pride in transparency, candor, and clarity. You may also know that our one core value as a company is putting our users (i.e., engineers practicing for interviews) first. I didn’t live up to that this time, and I am sorry.

**Practically speaking, I’ve learned that creating some amount of friction is necessary when you’re asking people to promise to pay you ~$1000 in the future. Removing that friction can create short-term wins but may hurt you (and disappoint your users) in the long run.**

Now, you might expect that if we just revert the flow and bring back the friction, everything will be great again. Unfortunately not.

## Why we’re pausing

In the current economic climate, we’ve had to take a hard look at our business and, like many other companies, cut our burn and tighten our belts. When we launched the program and committed to backstopping it ourselves, it was with the expectation that hiring would continue at a healthy clip. We anticipated a hiring slowdown in the recession, but multiple FAANGs freezing hiring was beyond what we had predicted – all of our models and assumptions had been based on COVID-era economics. This was something new.

The reality is that because the payback period is at least 4 months, regardless, as long as we’re running it, we are losing 4 months of runway. In this climate that’s significant.

Despite that, we knew that for some portion of our users, the existence of this program would be the difference between being able to practice and not, so we looked into financing it through a third party. Here’s how we did the math:

1. Estimate our enrollment rate in the Pay Later Program if we go back to the old UI/UX and terms.
2. Estimate our payback rate for the Pay Later Program moving forward
3. Cross-reference (1) and (2) with the interest rates we’d be paying to finance the program

**Unfortunately, no matter how we ran the numbers, we couldn’t find a way to make this program ROI positive in the near term, especially with the reduction in enrollment we’d expect upon adding friction back into our enrollment process**, which we decided would be a hard requirement for bringing back the program – we learned to be careful with growth hacks through the mistakes we made with this program, and we will not repeat them.

In summary, the old flow had a lot of friction, and in the current climate, enrollment rates would be too low to justify its existence. The new flow had significantly more enrollment, but it made it too easy for users to enroll without grokking the terms, and this generated enough negative feedback from users to make us want to kill it, especially given our company’s core value of putting our engineering community first. Finally, even if we kept tweaking the old flow to find the perfect balance, payback rates within a reasonable time window would be too low to be ROI positive when we factor in the cost of financing this program by a 3rd party, which we’d need to do to conserve cash.

It’s possible we’ll bring this program back in the future, but if we do, it’ll look substantially different from what we released in 2022.

## What about users who are already enrolled in the Pay Later Program?

If you’re already enrolled, you’re good! Your credits will continue to work in perpetuity, just like we promised, and we’ll honor the terms: You get extensions as long as you haven’t found a job and are still actively looking.

## What we’re doing instead to help those who can’t afford to pay for practice

We realize that many of you were depending on this program to be able to practice, especially during a recession. Even though we’ve paused our Pay Later Program, until we hopefully bring it back in the future, we have a few other ways that we can make it easier for you to get the practice you need.

- We still have peer-to-peer practice interviews, where you get matched with other interviewing.io users and work problems together. We introduced these right after COVID-19, and they’re not going anywhere. You get 1 interview with a peer when you join the platform, and after that, you can unlock more by conducting interviews yourself. We’ll be looking at this program to see how we can make it more accessible.
- We’ll be starting the next cohort of our Fellowship for engineers from non-traditional backgrounds soon.
- We’re experimenting with a few other ideas that may, together, fill the void created by the Pay Later Program. One of the more promising items is implementing Affirm, which will let our users defer payment for practice or pay it in smaller chunks.

This was a hard post to write, and I welcome feedback. You can always reach me at [aline@interviewing.io](mailto:aline@interviewing.io).


# [Our business depends on having the best interviewers, so we built an interviewer rating system. And you can too.](https://interviewing.io/blog/our-business-depends-on-having-the-best-interviewers-so-we-built-an-interviewer-rating-system-and-you-can-too)

By Aline Lerner | Published: January 17, 2023; Last updated: May 1, 2023

We make money in two ways: engineers pay us for mock interviews, and employers pay us for access to the best performers. Companies connect with our engineers through anonymous technical interviews — this is really important to us because we don’t want employers to be biased by where people went to school or where they previously worked. When employers use interviewing.io, they get great candidates, on the condition that they’re willing to talk to them anonymously. And it really is anonymous – they get zero information about the candidates ahead of time, and they agree to devote precious engineering time to interviewing people who are, from their perspective, basically randos off the internet.

To keep our engineer customers happy, we have to make sure that our interviewers deliver value to them by conducting realistic mock interviews and giving useful, actionable feedback afterwards.

To keep our employer customers happy, we have to make sure that the engineers we send them are way better than the ones they’re getting without us[1](#user-content-fn-1). Otherwise, it’s just not worth it for them.

**This means that we live and die by the quality of our interviewers, in a way that no single employer does, no matter how much they say they care about people analytics or interviewer metrics or training. If we don’t have really well-calibrated interviewers, who also create great candidate experience, we don’t get paid.**

Fortunately, we have two pieces of data that no single employer can collect: 1) honest candidate feedback and 2) real outcomes for candidates, from multiple interviews at multiple companies.

The former helps us figure out how effective our interviewers are at engaging candidates and creating great candidate experience. The latter helps us figure out how well-calibrated our interviewers are, with a high degree of confidence.

Over time, as we’ve used these metrics, our interviewer quality has gone way up.

In this post, we’ll explain exactly how we compute and use these metrics to get the best work out of our interviewers. In a [follow-up post](https://interviewing.io/blog/we-have-the-best-technical-interviewers-heres-how-we-do-it), we’ll talk about how to bring some of these approaches and metrics to your own interview process.

Before we get into all that, I’d like to provide some context about why these metrics are important, and why you can’t assume that just because an interviewer comes from a great company, they’ll be great at conducting interviews.

## Interviewer quality generally sucks and isn’t something companies really care about

We host thousands of interviews a month, and that means we need a lot of interviewers. But, as you saw above, we live and die by interviewer quality, so we need those interviewers to be world-class.

When I first started interviewing.io, I assumed that if we were very picky about whom we let be an interviewer, then all of our interviewers would be great. These are the criteria we came up with and still use today:

- Engineers from FAANG and FAANG-adjacent companies (e.g., Uber, Stripe, Dropbox)
- At least 4 years of experience (though today the average on our platform is 8)
- Conducted at least 20 interviews at FAANG or a FAANG-adjacent company

As we quickly learned, however, even these stringent criteria weren’t enough. We found ourselves facing two problems. First, our users would regularly tell us that their interviewer didn’t seem like they were paying attention, was a jerk, or didn’t teach them anything useful. Second, over time, as interviewing.io became more popular and gained more users, we saw that our candidates weren’t performing as well in real interviews as they previously had… which meant that our interviewers weren’t as well calibrated as we thought.

I could speculate at length about why just choosing interviewers from top companies isn’t good enough, but my suspicion is that interviewer training is not created equal.

Some companies invest considerable time and effort into teaching their interviewers how to be present during an interview, how to give hints, how to ask good follow-up questions, and how to make the candidate walk away feeling like they spent an hour working with a smart friend on a cool problem.

Others just throw new interviewers into the fray.

Some companies, like Opendoor (whose [eng manager shared with us the notion of “superforecasters”](https://interviewing.io/blog/technical-phone-screen-superforecasters) keep detailed metrics on which interviewers have the best eye for talent. But other companies shoot blind.

Even with great training, some interviewers won’t put in the work to stay present and engaged during interviews. Interviewing people is polarizing — some love it and some hate it — and if you don’t love it, it’ll show.

With this said, while our criteria might sound like a pretty high barrier to entry, the reality is that interviewer quality, before we instituted some formal metrics, was all over the place, both with respect to candidate experience and calibration.

Enter the money-making metrics!

## The need for two metrics

As you saw, we care about two separate things: how good of a candidate experience our interviewers create and how well-calibrated they are.

**Candidate experience matters for retention.** We want engineers to complete as many mock interviews as they need on our platform. Some users will be ready after one mock, but others will need more practice. What we don’t want is for someone to leave us before they’re ready simply because they had a negative experience with their first (or second) interviewer.

We also care about retention on a multi-year timeline. We’re a hiring marketplace, so we know that — like a dating site — if we do our job well, our users will leave us. Unlike a dating site, though, people today aren’t married to their jobs. The average tenure of a software engineer in the US is 2.4 years. We hope that once our users are considering their next move, they’ll come back to us to practice and de-rust.[2](#user-content-fn-2)

**Calibration matters just as much as candidate experience.** I already talked about how the employers who hire through us are taking a leap of faith because they’re agreeing to spend eng time on our candidates, sight unseen. Calibration and candidate experience are also inextricably linked — feedback that’s too lenient might boost a candidate’s confidence in the short term, but it will also hurt their chances of doing well in interviews, which ultimately reflects poorly on us.

Candidate experience and calibration are related, but they’re different enough to warrant tracking separately. We ultimately decided to create a “candidate experience” score and a “calibration” score for each interviewer and track them over time, both individually and in aggregate across the whole platform.

First, how did we find the interviewers who drove customer success?

### Metric 1: The candidate experience score

After each interview on interviewing.io, we collect feedback from both candidates and interviewers. As you may recall, interviews are fully anonymous, and you don’t see the other person’s feedback until you submit yours. This means that on our platform, candidates are incentivized to tell us honestly how things went.

This is what our candidate feedback form looks like:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/candidate_feedback_form_5e72f023da.png)

Though we’d been qualitatively tracking candidate feedback for years, we recently quantified it and created a per-interviewer candidate experience score. Here’s what we did.

Ranking how well people engage customers is a wide-ranging task: similar questions could be asked about Uber drivers, Instacart shoppers, sales reps, or even schoolteachers.

A core issue in customer-facing platforms is that customers are too… nice. There’s grade inflation in the feedback. For example, Uber driver ratings are compressed at the top, with most drivers clustered around 4.8 or so. In absolute terms, a 4.8 seems not that different from a 4.7, but seasoned riders have learned the difference. Despite interviews being anonymous and feedback being mutually incentivized, it’s true in our data as well: a seemingly high average in absolute terms could mask a chronic underperformer, so it’s important to use relative rankings and concrete outcomes (e.g., customer churn) in addition to customer ratings.

What did our score need to do? We saw three overarching attributes that needed to be balanced:

- **Predictive**: Scores should capture something persistent about the interviewer’s ability. Someone’s score today should tell you something about their performance tomorrow.
- **Fair**: Scores shouldn’t feel arbitrary; they should be based on meaningful metrics and interviewers should be given space to improve.
- **Legible**: Scores should be interpretable to both the interviewing.io ops team and to the interviewers themselves.

Our first task was to check whether an interviewer-based metric was **predictive**. This wasn’t guaranteed: most of the variation in the customer experience could be driven by the attitudes of the customers or extraneous factors like the weather that day. If this were the case, any ranking of interviewers would be entirely based on luck.

Instead, we found evidence that interviewers had stable skill levels — someone’s history of customer ratings was highly predictive of what the next customer would say about them. These results extended beyond just survey responses: customers who got paired with a lower-ranked interviewer were more likely to leave the platform entirely. A bad interviewer increased churn by almost 10 percentage points.

The graph below shows the result. If interviewers had no effect on churn, we would expect the fitted line to be flat. Instead, once we rank interviewers based on their past sessions, we can predict how likely a customer is to leave the platform after an interview with them. Around 30% of customers leave if they get an interviewer from the bottom fifth. But only 20% of customers leave if they get an interviewer from above the 80th percentile.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/The_tight_link_between_interviewer_quality_and_candidate_attrition_9c2a88fccb.png)

So, interviewers had stable traits which, in the worst cases, could actually scare customers off the platform. Our next step was to condense this valuable information into a single score. Machine learning methods have one suggestion for this: use the interviewer identities as features in a model predicting a customer outcome like attrition or dissatisfaction. The coefficients could then be used as a measure of quality.

While we did estimate these, a ranking based on these numbers wouldn’t be very **legible**. At best, we could tell the interviewers that their quality scores were based on a well-calibrated model.

It turned out that averages of the more readily understood dimensions of customer feedback (how excited they are to work with them and the quality of the interview questions, hints, and communication) were closely correlated with the machine learning predictions. So to reduce the complexity in the score, we used the linear combination of the customer feedback factors that were most correlated with the output from the more complicated model.

Thus, we boiled down our optimal prediction into a simpler linear model that the interviewers could even calculate themselves, giving them insight into which components were holding them back. This would also help our ops team provide focused nudges to the interviewers.

Was it **fair**? In general, the model’s out-of-sample predictions were more accurate when it used all past interviewers in the data to train. This is unsurprising, but it meant that veteran interviewers who had shown recent improvement might have to wait a year for this to be reflected in their score. This low-score purgatory would be too demoralizing.

Another fairness concern was that interviewers in hot water with objectively high averages could complain that they’d received only “Excellent” and “Great” ratings, much like a 4.6-star Uber driver. **We emphasized to interviewers that these scores put them in the bottom 5-10% of interviewers, with real business consequences.**

One early challenge we faced was setting the right observation window. Originally, we decided to look at interviewers who had conducted at least 5 interviews in a rolling 90-day period. But because some interviewers had 5 in that 90-day period and others had 150, this created very different sample sizes and made it possible for a single interview to derail someone’s score.

After measuring the predictiveness of different look-back periods, we eventually decided to limit it to the most recent 30 interviews. We saw that, while more data was always better, the curve flattened around 30. Sessions older than that would be ignored so that interviewers on an upward trajectory would have this reflected in their scores.

**Our final result was a simple linear function of someone’s average customer feedback, with different weights according to how predictive that factor was of the machine learning estimates, and with the limited look-back period.**
Below, you can see our weighted (by number of interviews) aggregate candidate experience score across all the mock interviews on the platform. We track this as an internal health metric, and it’s a remarkably useful metric that shows us how happy our users are, at a glance (and correlates very directly with NPS).

### Metric 2: The accuracy score

Outside of delivering great candidate experience, it’s critical to our business that our interviewers are well-calibrated. After all, if our candidates aren’t amazing, the companies who hire through us will stop giving precious eng time to interviewing our users. They’ll just go back to using resumes.

So how much better do our candidates have to be?

In a good funnel at a company with a high engineering bar, candidates pass the technical screen about 20-25% of the time. Over time and after a bunch of trial and error, we realized that we needed our candidates to pass interviews about 70% of the time.[3](#user-content-fn-3) That’s about 3X what our customers see in their funnels, and it’s the threshold where working with us started to feel like magic. After all, when companies work with us, we’re removing their ability to choose who they talk to. They just have to trust us… and that makes the psychological burden of proof higher than you might expect. 50% pass rate, for instance, wasn’t enough to feel like magic.

In a nutshell, we had to ensure that our interviewers were well-calibrated enough to achieve a 70% passthrough rate for our candidates in real interviews.

Fortunately, we had the data to make this happen.

On interviewing.io, engineers do mock interviews, and top performers do real ones. Both sets of interviews have an identical feedback form that the interviewer completes after each interview:

![](https://strapi-iio.s3.us-west-2.amazonaws.com/interviewer_feedback_form_bd43d2e5c3.png)

To figure out how “accurate” our mock interviewers were — a measure of whether their judgments tended to be too strict, too lenient, or well-calibrated, relative to the hiring decisions of companies — we compared how they rated their candidates to how those candidates performed later, in real interviews.

In other words, this was based on a similar principle as the interviewer quality score, although based on less subjective measures than customer feedback.

Using just the interviewers’ own ratings and decisions of hiring companies, we asked whether some interviewers regularly under- or overestimated their candidates compared to candidates’ subsequent interviews with real companies. The answer was yes, and this result persisted in out-of-sample testing.

Much like the customer feedback, our interviewers had a bigger problem with positivity: the overly positive interviewers indicated they would hire their candidates 75% of the time, while their candidates received a yes just 60% of the time in real interviews — a considerable gap. But a sizable portion of well-calibrated interviewers closely matched the judgments of companies, and we told them so.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_accuracy_and_their_disagreement_with_real_job_interviews_df13562638.png)

Our accuracy calculations were a continuous score, but we wanted to condense this. We converged on a -4 to 4 scale, a small number of values that we could connect to a distinct message. People with a 0 are among the best-calibrated interviewers. People with 3s and 4s are definitely too positive: they’re almost twice as likely to want to hire someone compared to the company. People with negative values are similarly too negative.

## How these scores play together, or why brutal honesty makes for great candidate experience

Once we built out both sets of scores, we could finally answer a question that interviewers new to our platform pose regularly: Is there a tension between delivering great candidate experience and being brutally honest with candidates about their performance?

After all, doesn’t candidate experience depend, in part, on NOT giving harsh, unvarnished feedback? And doesn’t failing people create a bad experience that makes them not want to return?

Fortunately, for hard questions like these, we have the data! Below is a graph of average candidate experience score as a function of interviewer accuracy, representing data from over 1,000 interviewers. As you can see, the candidate experience score peaks right at the point where interviewers are neither too strict or too lenient but are, in Goldilocks terms, just right. It drops off pretty dramatically on either side after that.[4](#user-content-fn-4)

## How we used these scores to significantly improve candidate experience over time

In the graph at the bottom of the “candidate experience score” section, you can see that starting in Q1 of 2022, our aggregate candidate experience score improved dramatically. This was intentional. Once we realized how well this metric predicted both candidate experience and attrition, we started to actively drive it up. Here’s what we did:

- Systematically, ruthlessly paused interviewers who fell below the bar
- Created full transparency and visibility around both scores
- Built automatic throttling
- Started running monthly onboarding meetings to explain our business model to interviewers and share how both candidate experience and accurate vetting are critical to interviewing.io’s success

### Ruthlessly pausing interviewers below the bar

Once we built both scores, we started to use them to filter out underperforming interviewers.

For candidate experience, we chose a line below which candidate attrition was unacceptable. If an interviewer toed that line, we would reach out and encourage them to improve their performance. If an interviewer consistently fell below that line, then we paused their ability to do interviews.

We took a similar approach to accuracy. If an interviewer was too strict or too lenient (below a -1 or above a 1), we issued a warning and ultimately paused. In a pinch, we’d keep the strict interviewers and just let go of the lenient ones.

It’s hard to frame letting people go in a really positive light, but it’s important to distinguish between the two abilities in question here. As often seen in sports, the best players are not necessarily the best coaches. Similarly, we were forced to cut some really stellar *engineers* who, regrettably, didn’t have the disposition or interpersonal skills to be great mentors.

As we started to roll out our new, ruthless policy of letting go underperformers, we realized that we had made a mistake. Though interviewers’ scores were visible to us, they were not visible to interviewers, so when we started issuing warnings, they were often blindsided and not in a place where they could take the news well.

### Transparency and visibility

To fix the problem and not blindside our interviewers, we made a dashboard where interviewers could see both their “candidate experience” and “accuracy” scores in near-real time (for accuracy, we bucketed the score into “too strict,” “OK,” and “too lenient”). We also shared how we calculate both scores.

![](https://strapi-iio.s3.us-west-2.amazonaws.com/image_15dafbf507.png)

Exposing the scores, as well as the logic behind them, helped our interviewer community buy into the scoring system and to see firsthand that it was built on data and not our subjective opinions. It also made it so no one was shocked when we reached out about their performance. Over time, we started encouraging interviewers to come to us if they saw their score slipping… before we had the chance to reach out to them.

### Automatic throttling

Having our ops team manually review interviewer scores every week and manually pause underperforming interviewers obviously didn’t scale, so we decided to build a throttling system that would put our best interviewers in front of the most customers. That meant adjusting our interview scheduling algorithm to prioritize interviewers who maintained higher ratings. Those rated lower still got assignments but only if higher-rated interviewers weren’t available for the same type of interview or time slot. The net result is the top 20% of our interviewers administering 80% of practice interviews.

New interviewers still get the chance to prove their mettle, of course.

### Monthly onboarding

As we spoke to more and more interviewers, it became clear that many of them didn’t actually know what interviewing.io did or why candidate experience and accuracy mattered so much to our bottom line.

When I first started the company, I’d spend 30-60 minutes on the phone with every new interviewer, explaining how hiring was broken and how they played a critical part in fixing it. Then, at some point, we started growing fast, and I stopped.

It was clearly time to start again. Though it was no longer sustainable for me to do 1:1 calls, I now run a 1.25-hour monthly onboarding session along with our ops team, and every interviewer who’s joined our community since the last session has to attend.

In these sessions, we discuss a bunch of the stuff covered in this post. We talk about the mission of the company, why the two metrics matter, how to deliver harsh feedback, how important it is not to sugarcoat things, and what it means to be a good interviewer.

Explaining what we do, why we do it, and how high our expectations are made a step function difference in the quality of our interviewer community. It also reaffirmed my faith in humanity. If you want people to do a great job, tell them what a great job looks like and why it matters. And if you’re fortunate enough to have talented people working for you who are committed to your mission, they will knock it out of the park.

In the next post (coming soon), we’ll talk about how to bring some of these approaches and metrics to your own interview process. Because you are a single company, not a dedicated interview platform, you may not be able to replicate what we did exactly, but you can get pretty close with just a little bit of work.

*Big thank you to Maxim Massenkoff, Liz Graves, and Richard Graves for their contributions to and help with this post.*

1. Thankfully, our candidates are better. On average, interviewing.io top performers perform 3X better in technical interviews than candidates from other sources (70% pass rate compared to 20-25%, the latter being the industry standard at companies with a high bar). [↩](#user-content-fnref-1)
2. We’re proud to say that 70% of our users return to interviewing.io for their next job search. [↩](#user-content-fnref-2)
3. While we could go higher than 70%, we didn’t want to have too many false negatives, which aren’t great for business or for candidate experience. [↩](#user-content-fnref-3)
4. There are some notable peculiarities in the data as well. For instance, why do interviewers with an accuracy score of -3 get rated so much worse than interviewers who are even more strict? And why does candidate experience keep improving ever so slightly, as interviewers get more and more lenient, only to drop off again at the most lenient (accuracy = 4)? I expect these are functions of needing more data and that if we had, say, 10K interviewers in our ecosystem, the curve would be a lot more smooth. [↩](#user-content-fnref-4)


# [We analyzed 100K technical interviews to see where the best performers work. Here are the results.](https://interviewing.io/blog/we-analyzed-100k-technical-interviews-to-see-where-the-best-performers-work-here-are-the-results)

By Aline Lerner | Published: March 30, 2022; Last updated: March 28, 2024

At interviewing.io, we’ve hosted over 100K technical interviews, split between mock interviews and real ones.

As it happens, we know where many of our users currently work – they tell us that when they sign up to avoid getting matched for mock interviews with people they work with.

Given that we have this data AND given that we know how well people do in their interviews, we thought it would be interesting to see which companies’ engineers are especially good at technical interviews. We looked at which companies have engineers with the best overall performance, as well as those that shine specifically on technical ability, problem solving ability, and communication skills.

The resulting top ten lists are below!

On interviewing.io, engineers can practice technical interviewing anonymously. If things go well, they skip right to the technical interview at real companies (which is also fully anonymous). We started interviewing.io because resumes suck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.

When an interviewer and an interviewee match on our platform, they join a collaborative coding environment with voice, text chat, and a whiteboard, and jump right into a technical interview. After each interview, both parties leave feedback, and once they’ve both submitted, each one can see what the other person said and how they were rated.

![Screenshot of the Interviewing.io feedback form](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FScreen_Shot_2022_03_30_at_5_06_30_PM_1511096ff8.png&w=750&q=75 "Interviewing.io feedback form")

To arrive at the top ten rankings in this post, we took post-interview feedback data from our users and grouped them by the company where they work.

About 60% of our users work at FAANG and FAANG-adjacent companies (e.g. Dropbox, Lyft, Uber, Square, etc). The rest work at other large tech companies, startups of all stages and sizes, digital agencies. A small portion are students, some are entrepreneurs, some are career changers, and some are unemployed (as you’ll see in a spicy tidbit below). For context, our average (and median) active user has about 7 years of experience.

For this analysis, we only included companies where we had at least 50 employees on interviewing.io (which meant that several companies were notably missing because they were just shy of that mark, including Quora, Asana, Stripe, and a few others).

## A few words about statistical significance

In all the sections below, we’ve listed the top 10 companies in each category. But how different is the 1st from the 10th, really? As it happens:

1. Membership in the top 10 is indeed highly statistically significant. The companies that made it into our top ten lists do indeed have significantly better interviewees than the ones that did not.

2. Most comparisons *within* the top 10 are largely insignificant. Though it varies a bit from list to list, in general, the top company is significantly different from the 8-10th ranked companies, except with the communication score, which is very noisy. In other words, a lot of the intermediate ranks could have gone either way.

That being said, here are the lists!

## Best overall performance

We looked to see which 10 companies had the highest % of people passing interviews on interviewing.io. The average across all of our users is 54%.

![Chart showing companies whose engineers had the highest interview pass rates](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_highest_interview_pass_rates_1_64103d0dbb.png&w=2048&q=75 "Companies whose engineers had the highest interview pass rates")

## Best technical

We also looked to see which 10 companies had the highest average technical scores in interviews on our platform. The average across all of our users is 2.85 out of 4.

![Chart showing companies whose engineers had the best technical scores](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_technical_scores_1_102f54f16e.png&w=2048&q=75 "Companies whose engineers had the best technical scores")

## Best problem solving

We also looked to see which 10 companies had the highest average problem solving scores in interviews on our platform. The average across all of our users is 2.79 out of 4.

![Chart showing companies whose engineers had the best problem solving scores](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_problem_solving_scores_1_711241b216.png&w=2048&q=75 "Companies whose engineers had the best problem solving scores")

## Best communication

Finally, we looked to see which 10 companies had the highest average communication scores in interviews on our platform. The average across all of our users is 3.22 out of 4. It was surprising to see how high “Unemployed” ranked (that is exactly what it sounds like – these are users who indicated they didn’t have a job when they joined interviewing.io).

![Chart showing companies whose engineers had the best communication scores](https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FCompanies_whose_engineers_had_the_best_communication_scores_1_03760e4995.png&w=2048&q=75 "Company engineeers' average communication score")

## Limitations and closing thoughts

It bears mentioning that there are some limitations to our data. As you saw at the beginning, where people work is self-reported, and though we’ve done enough spot checking over the years to be confident that, *for the most part*, our users are honest about where they work, it’s not a perfect system. People also don’t always update their employer when they switch jobs.

There’s also the issue of selection bias. Maybe we’re just getting people who really feel like they need practice and aren’t an indicative slice of engineers at that company. After having talked to our users for years and after seeing how they perform in real interviews later on, I’m not too sure that’s true, but hey, it’s totally possible.

Finally, there’s the obvious quantity issue. As I mentioned earlier, we’re only including companies where at least 50 of their engineers practiced on interviewing.io. When we lowered that limit, the list started to look different… but we weren’t confident enough in those results (yet) to publish them.

Limitations aside, what does this all mean? I’m tempted to speculate about why Dropbox dominated these rankings and what about their engineers is distinct from many of the other companies with great brands that our users hail from. Dropbox does also hire on interviewing.io, and from what we’ve seen, they have an extremely high bar – many users who’ve done well with other companies on our platform have failed their Dropbox interview. However, I will refrain from speculating beyond that, and I will also refrain from speculating about why certain companies do well on one score but not the others – looking from the outside, we simply don’t know enough about these companies’ inner workings or the nuances of the types of engineers they attract to come up with a credible hypothesis. However, I hope to hear from you, dear reader, if you have visibility into these things and can comment on them.

You might be wondering how this list of companies and scores resolves with interviewing.io’s ongoing refrain about how resumes and pedigree don’t tell you very much about whether someone is a good engineer. We’ve seen over and over that [where you go to school doesn’t matter](https://interviewing.io/blog/we-looked-at-how-a-thousand-college-students-performed-in-technical-interviews-to-see-if-where-they-went-to-school-mattered-it-didnt) (and in fact, interview performance among students from elite schools doesn’t meaningfully differ from that of students in state schools), but we have seen consistently that where you’ve worked in the past does have some bearing on how you do in interviews.

So, yes, where you work matters. The good news, though, is that it’s not the whole story. We’ve seen in the past that [people who take a bunch of Coursera and Udacity classes](https://interviewing.io/blog/lessons-from-3000-technical-interviews) on topics related to algorithms and data structures tend to perform better than people from top companies who have not. And we’ve seen that after 5 mock interviews on interviewing.io, regardless of where you started or where you work, your [chances of passing real interviews will (on average) double](https://interviewing.io/blog/how-know-ready-interview-faang).

Of course, the really interesting question in all of this is the holy grail of technical recruiting: *Does performance in interviews reliably predict on-the-job performance?* While we diligently gather data on the subject, I’d love to hear from you. If you’ve hired engineers from some of the companies in this post, have they performed better than others? Are there any patterns or anti-patterns that you have noticed?

### [Longest Increasing Path in a Matrix](/questions/longest-increasing-path-in-a-matrix)

[Given an m x n integers matrix, return the length of the longest increasing path in the matrix. You may only move up, down, left, or right.](/questions/longest-increasing-path-in-a-matrix)


